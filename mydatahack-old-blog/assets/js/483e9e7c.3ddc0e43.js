"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[6595],{85940:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>c,toc:()=>l});var o=t(74848),r=t(28453);const a={sidebar_position:9},s="Bulk Loading Postgres with Node.js",c={id:"data-ingestion/bulk-load-pg-node",title:"Bulk Loading Postgres with Node.js",description:"The fastest way to bulk load data into Postgres is to call Copy, which is a SQL command to load data into a table from a flat file. To connect to Postgres with Node.js, we can use the node-postgres module (pg). To use the copy function, we can use the pg-copy-streams module, which enables you to execute the copy function from a file readable stream.",source:"@site/docs/data-ingestion/9.bulk-load-pg-node.md",sourceDirName:"data-ingestion",slug:"/data-ingestion/bulk-load-pg-node",permalink:"/mydatahack-old-blog/docs/data-ingestion/bulk-load-pg-node",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:9,frontMatter:{sidebar_position:9},sidebar:"tutorialSidebar",previous:{title:"Converting CSV to JSON and Loading it to Postgres with Node.js",permalink:"/mydatahack-old-blog/docs/data-ingestion/csv-to-json-pg-node"},next:{title:"A Comprehensive Guide for Reading and Writing JSON with Python",permalink:"/mydatahack-old-blog/docs/data-ingestion/json-with-python"}},i={},l=[];function d(e){const n={code:"code",h1:"h1",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"bulk-loading-postgres-with-nodejs",children:"Bulk Loading Postgres with Node.js"}),"\n",(0,o.jsx)(n.p,{children:"The fastest way to bulk load data into Postgres is to call Copy, which is a SQL command to load data into a table from a flat file. To connect to Postgres with Node.js, we can use the node-postgres module (pg). To use the copy function, we can use the pg-copy-streams module, which enables you to execute the copy function from a file readable stream."}),"\n",(0,o.jsx)(n.p,{children:"We will first check out how to load the table and then create the code that does truncate & load."}),"\n",(0,o.jsx)(n.p,{children:"Setup"}),"\n",(0,o.jsx)(n.p,{children:"Let\u2019s set it up by creating a project and installing required modules."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"mkdir node-pg-test\ncd node-pg-test\nnpm init\nnpm i pg -ES\nnpm i pg-copy-streams -ES\n"})}),"\n",(0,o.jsx)(n.p,{children:"config.json"}),"\n",(0,o.jsx)(n.p,{children:"For the connection details, let\u2019s use config.json and import the connection from the config file."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'{\n  "host": "your host",\n  "user": "your username",\n  "pw": "your password",\n  "db": "your database name",\n  "port": "port, 5432 is default"\n}\n'})}),"\n",(0,o.jsx)(n.p,{children:"Import modules and define global variables"}),"\n",(0,o.jsx)(n.p,{children:"We import the required modules. Then, we define the input file path and table name. We can import the connection details from config.json file located in the same folder."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-js",children:"// Import required modules\nconst fs = require('fs')\nconst path = require('path')\nconst { Pool, Client} = require('pg')\nconst copyFrom = require('pg-copy-streams').from\nconst config = require('./config.json')\n\n// inputfile & target table\nvar inputFile = path.join(__dirname, '/data/customer.csv')\nvar table = 'usermanaged.customers'\n\n// Getting connectin parameters from config.json\nconst host = config.host\nconst user = config.user\nconst pw = config.pw\nconst db = config.db\nconst port = config.port\nconst conString = `postgres://${user}:${pw}@${host}:${port}/${db}`\nLoad Table\n\nLet\u2019s create the readable stream and pipe the copy function.\n\n// Connecting to Database\nconst client = new Client({\n    connectionString: conString,\n  })\n  client.connect()\n  // Execute Copy Function\nvar stream = client.query(copyFrom(`COPY ${targetTable} FROM CSV HEADER STDIN`))\nvar fileStream = fs.createReadStream(inputFile)\n\nfileStream.on('error', (error) =>{\n    console.log(`Error in reading file: ${error}`)\n})\nstream.on('error', (error) => {\n    console.log(`Error in copy command: ${error}`)\n})\nstream.on('end', () => {\n    console.log(`Completed loading data into ${targetTable}`)\n    client.end()\n})\nfileStream.pipe(stream);\nTruncate Table\n\nBefore doing the classic truncate and load ingestion pattern, let\u2019s review the code for truncating table.\n\n// Connecting to Database\nconst client = new Client({\n    connectionString: conString,\n  })\nclient.connect()\n\n// Execute Truncate Table\nclient.query(`Truncate ${targetTable}`, (err) => {\n    if (err) {\n      client.end()\n      // return console.log(err.stack)\n      return console.log(`Error in truncate table ${err}`)\n      process.exit(1)\n    } else {\n      console.log(`Truncated ${targetTable}`)\n    }\n  })\n"})}),"\n",(0,o.jsx)(n.p,{children:"Truncate & Load"}),"\n",(0,o.jsx)(n.p,{children:"Let\u2019s put them all together to do truncate & load. Remember that the functions we are using are asynchronous. To ensure sequential execution of the code, we are using callback. With callback, we can make sure that the copy function only gets executed after the truncate statement was successfully executed."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-js",children:'// Connecting to Database\nconst client = new Client({\n  connectionString: conString,\n});\n\nclient.connect();\n\nconst executeQuery = (targetTable) => {\n  const execute = (target, callback) => {\n    client.query(`Truncate ${target}`, (err) => {\n      if (err) {\n        client.end();\n        callback(err);\n        // return console.log(err.stack)\n      } else {\n        console.log(`Truncated ${target}`);\n        callback(null, target);\n      }\n    });\n  };\n  execute(targetTable, (err) => {\n    if (err) return console.log(`Error in Truncate Table: ${err}`);\n    var stream = client.query(copyFrom(`COPY ${targetTable} FROM STDIN`));\n    var fileStream = fs.createReadStream(inputFile);\n\n    fileStream.on("error", (error) => {\n      console.log(`Error in creating read stream ${error}`);\n    });\n    stream.on("error", (error) => {\n      console.log(`Error in creating stream ${error}`);\n    });\n    stream.on("end", () => {\n      console.log(`Completed loading data into ${targetTable}`);\n      client.end();\n    });\n    fileStream.pipe(stream);\n  });\n};\n// Execute the function\nexecuteQuery(table);\n'})}),"\n",(0,o.jsx)(n.p,{children:"(2018-04-16)"})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>c});var o=t(96540);const r={},a=o.createContext(r);function s(e){const n=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);