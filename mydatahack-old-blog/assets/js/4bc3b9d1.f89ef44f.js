"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[8845],{82160:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>r,contentTitle:()=>i,default:()=>h,frontMatter:()=>s,metadata:()=>d,toc:()=>c});var a=n(74848),o=n(28453);const s={sidebar_position:13},i="How to Bulk Load Data with ODBC and Python",d={id:"data-ingestion/odbc-python",title:"How to Bulk Load Data with ODBC and Python",description:"I think Hello World of Data Engineering to make an one-to-one copy of a table from the source to the target database by bulk-loading data. The fastest way to achieve this is exporting a table into a CSV file from the source database and importing a CSV file to a table in the target database. With any database, importing data from a flat file is faster than using insert or update statements.",source:"@site/docs/data-ingestion/13.odbc-python.md",sourceDirName:"data-ingestion",slug:"/data-ingestion/odbc-python",permalink:"/mydatahack-old-blog/mydatahack-old-blog/data-ingestion/odbc-python",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:13,frontMatter:{sidebar_position:13},sidebar:"tutorialSidebar",previous:{title:"How to Bulk Load Data with JDBC and Python",permalink:"/mydatahack-old-blog/mydatahack-old-blog/data-ingestion/jdbc-python"},next:{title:"How to Bulk Load Data into MySQL with Python",permalink:"/mydatahack-old-blog/mydatahack-old-blog/data-ingestion/mysql-python"}},r={},c=[];function l(t){const e={code:"code",h1:"h1",p:"p",pre:"pre",...(0,o.R)(),...t.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"how-to-bulk-load-data-with-odbc-and-python",children:"How to Bulk Load Data with ODBC and Python"}),"\n",(0,a.jsx)(e.p,{children:"I think Hello World of Data Engineering to make an one-to-one copy of a table from the source to the target database by bulk-loading data. The fastest way to achieve this is exporting a table into a CSV file from the source database and importing a CSV file to a table in the target database. With any database, importing data from a flat file is faster than using insert or update statements."}),"\n",(0,a.jsx)(e.p,{children:"To connect ODBC data source with Python, you first need to install the pyodbc module. Obviously, you need to install and configure ODBC for the database you are trying to connect."}),"\n",(0,a.jsx)(e.p,{children:"Let\u2019s load the required modules for this exercise. The code here works for both Python 2.7 and 3."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import pyodbc\nimport sys\nimport pandas as pd\n"})}),"\n",(0,a.jsx)(e.p,{children:"Exporting table to CSV"}),"\n",(0,a.jsx)(e.p,{children:"The function below takes a select query, file path for exported file and connection details. The best practice is to turn on autocommit. Some ODBC will give you an error if this parameter is not there."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"def table_to_csv(sql, file_path, dsn, uid, pwd):\n    '''\n    This function creates csv file from the query result with ODBC driver\n    '''\n    try:\n        cnxn = pyodbc.connect('DSN={};UID={};PWD={}'\\\n        .format(dsn, uid, pwd), autocommit=True)\n        print('Connected to {}'.format(dns))\n        # Get data into pandas dataframe\n        df = pd.read_sql(sql, cnxn)\n        # Write to csv file\n        df.to_csv(file_path, encoding='utf-8', header = True,\\\n         doublequote = True, sep=',', index=False)\n        print(\"CSV File has been created\")\n        cnxn.close()\n\n    except Exception as e:\n        print(\"Error: {}\".format(str(e)))\n        sys.exit(1)\n"})}),"\n",(0,a.jsx)(e.p,{children:"The execution example is exporting the city table to a csv file from MySQL. ODBC is set up with MySQL_ODBC as DSN."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"sql = 'Select * From world.city'\nfile_path = '/tmp/city.csv'\ndsn = 'dsn name such as MySQL_ODBC'\nuid = 'username'\npwd = 'password'\ntable_to_csv(sql, file_path, dsn, uid, pwd)\n"})}),"\n",(0,a.jsx)(e.p,{children:"Load Table from CSV"}),"\n",(0,a.jsx)(e.p,{children:"The function below takes a csv upload query and connection details to import CSV to a table. Autocommit should be turned on. The local_infile parameter helps MySQL\u2019s LOAD DATA INFILE commands. It may not relevant for other databases. If the parameter is not relevant in the connection for the specific database, it will ignore it. So, you can keep the local_infile parameter for other databases."}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"def load_csv(load_sql, dns, uid, pwd):\n    '''\n    This function will load table from csv file according to\n    the load SQL statement through ODBC\n    '''\n    try:\n        cnxn = pyodbc.connect('DSN={};UID={};PWD={}'\\\n        .format(dns, uid, pwd), autocommit=True, local_infile=1)\n        print('Connected to {}'.format(dns))\n        cursor = cnxn.cursor()\n        # Execute SQL Load Statement\n        cursor.execute(load_sql)\n        print('Loading table completed successfully.')\n        cnxn.close()\n\n    except Exception as e:\n        print(\"Error: {}\".format(str(e)))\n        sys.exit(1)\n"})}),"\n",(0,a.jsx)(e.p,{children:"Let\u2019s load the data exported with the first function into both MySQL and PostgreSQL databases. Each database has SQL syntax for this and you need to pass the statement to the function. MySQL uses the LOAD DATA INFILE command while Postgres uses the copy command."}),"\n",(0,a.jsx)(e.h1,{id:"1-execution-example-with-mysql-odbc",children:"(1) Execution Example with MySQL ODBC"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"load_sql = \"LOAD DATA LOCAL INFILE '/tmp/city.csv' INTO TABLE usermanaged.city \\\nFIELDS TERMINATED BY ',' ENCLOSED BY '\\\"' IGNORE 1 LINES;\"\ndsn = 'MySQL_ODBC'\nuid = 'username'\npwd = 'password'\nload_csv(load_sql, dns, uid, pwd)\n\n# (2) Execution Example with PostgreSQL ODBC\nload_sql = \"COPY usermanaged.city FROM '/tmp/city.csv' CSV HEADER\"\ndns = 'PG_ODBC'\nuid = 'username'\npwd = 'password'\nload_csv(load_sql, dns, uid, pwd)\n"})}),"\n",(0,a.jsx)(e.p,{children:"(2018-03-30)"})]})}function h(t={}){const{wrapper:e}={...(0,o.R)(),...t.components};return e?(0,a.jsx)(e,{...t,children:(0,a.jsx)(l,{...t})}):l(t)}},28453:(t,e,n)=>{n.d(e,{R:()=>i,x:()=>d});var a=n(96540);const o={},s=a.createContext(o);function i(t){const e=a.useContext(s);return a.useMemo((function(){return"function"==typeof t?t(e):{...e,...t}}),[e,t])}function d(t){let e;return e=t.disableParentContext?"function"==typeof t.components?t.components(o):t.components||o:i(t.components),a.createElement(s.Provider,{value:e},t.children)}}}]);