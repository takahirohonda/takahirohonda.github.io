"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[2712],{67623:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});var i=t(74848),a=t(28453);const s={slug:"data-science/deep-learning//introduction-to-dense-net-with-tensorflow",title:"Introduction to Dense Layers for Deep Learning with TensorFlow",tags:["Data Science","Deep Learning","Dense Net","Iris","TensorFlow"]},r=void 0,o={permalink:"/data-science/data-science/deep-learning/introduction-to-dense-net-with-tensorflow",source:"@site/data-science/deep-learning/2018-05-07-dense-layers-tensorflow.md",title:"Introduction to Dense Layers for Deep Learning with TensorFlow",description:"TensorFlow offers both high- and low-level APIs for Deep Learning. Coding in TensorFlow is slightly different from other machine learning frameworks.",date:"2018-05-07T00:00:00.000Z",tags:[{label:"Data Science",permalink:"/data-science/tags/data-science"},{label:"Deep Learning",permalink:"/data-science/tags/deep-learning"},{label:"Dense Net",permalink:"/data-science/tags/dense-net"},{label:"Iris",permalink:"/data-science/tags/iris"},{label:"TensorFlow",permalink:"/data-science/tags/tensor-flow"}],readingTime:3.51,hasTruncateMarker:!0,authors:[],frontMatter:{slug:"data-science/deep-learning//introduction-to-dense-net-with-tensorflow",title:"Introduction to Dense Layers for Deep Learning with TensorFlow",tags:["Data Science","Deep Learning","Dense Net","Iris","TensorFlow"]},unlisted:!1,prevItem:{title:"Introduction to Dense Layers for Deep Learning with Keras",permalink:"/data-science/data-science/deep-learning/introduction-to-dense-net-with-keras/"},nextItem:{title:"How To Create Your Own Personal Data Science Computing Environment In AWS",permalink:"/data-science/data-science/infra/how-to-create-your-personal-data-science-computing-environment-in-aws"}},c={authorsImageUrls:[]},l=[];function d(e){const n={code:"code",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"TensorFlow offers both high- and low-level APIs for Deep Learning. Coding in TensorFlow is slightly different from other machine learning frameworks."}),"\n",(0,i.jsx)(n.p,{children:"You first need to define the variables and architectures. This is because the entire code is executed outside of Python with C++ and the python code itself is just a bunch of definitions."}),"\n",(0,i.jsx)(n.p,{children:"The aim of this post is to replicate the previous Keras code into TensorFlow. Before writing code in TensorFlow, it is better to use high-level APIs like Keras to build the model (read Introduction to Dense Net with Keras for a preparation)."}),"\n",(0,i.jsx)(n.p,{children:"Steps"}),"\n",(0,i.jsx)(n.p,{children:"(1) Import Modules"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import tensorflow as tf\ntf.set_random_seed(42)\ntf.reset_default_graph()\nimport pandas as pd\nimport numpy as np\nnp.random.seed(42)\nfrom sklearn.model_selection import train_test_split\nimport keras\n"})}),"\n",(0,i.jsx)(n.p,{children:"(2) Data Preparation"}),"\n",(0,i.jsx)(n.p,{children:"As in the previous post, we are importing the Iris dataset from a csv file. This step is the same as before."}),"\n",(0,i.jsx)(n.p,{children:"You can also check the official TensorFlow documents about deep learning on Iris dataset here. The way the dataset is preprocessed is quite different from what I did and will be an interesting read."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"iris = pd.read_csv('/tmp/iris.csv')\nprint(iris.head(5))\nprint(iris.Species.unique())\niris['Species'] = np.where(iris['Species']=='setosa', 0,\n               np.where(iris['Species']=='versicolor', 1,\n                       np.where(iris['Species']=='virginica', 2, 3)))\nprint(iris[0:3])\nprint(iris.Species.unique())\n\n# convert df to features and targets\nfeature_cols = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\nfeatures = iris[feature_cols].values\ntarget = iris.Species.values\n\n# Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\n\nprint(X_train[3])\nprint(y_train[3])\n\n# Convert y into one hot encoded variables\nn_classes = 3\ny_train = keras.utils.to_categorical(y_train, n_classes)\ny_test = keras.utils.to_categorical(y_test, n_classes)\n\nprint(y_train[0:3])\n"})}),"\n",(0,i.jsx)(n.p,{children:"(3) Defining Variables and Models"}),"\n",(0,i.jsx)(n.p,{children:"Before running the code, we need to define variables and models. The model is the same as the one defined in the previous post with Keras."}),"\n",(0,i.jsx)(n.p,{children:"Even for more complicated models (e.g. with added convolutional layers), you can use the same steps."}),"\n",(0,i.jsx)(n.p,{children:"Set hyperparameters\nSet layers\nDefine placeholders\nDefine layers\nDefine architecture\nDefine variable dictionary\nBuild Model\nDefine loss & optimizer\nDefine evaluation metrics\nHere is the code from the steps above."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# (1) Set hyperparameters\nlr = 0.01\nepochs = 1000\nbatch_size = 20\nweight_initializer = tf.contrib.layers.xavier_initializer()\n\n# (2) Set layers\nn_input = 4\nn_dense = 10\nn_classes = 3\n\n# (3) Define placeholders\nx = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\n\n# (4) Define layers\ndef dense(x, w, b):\n    z = tf.add(tf.matmul(x, w), b)\n    a = tf.nn.relu(z)\n    return a\n\n# (5) Define architecture\ndef network(x, weights, biases):\n    dense1 = dense(x, weights['w1'], biases['b'])\n    out_layer_z = tf.add(tf.matmul(dense1, weights['w_out']), biases['b_out'])\n    return out_layer_z\n\n# (6) Define variable dictionary\nbias_dict = {\n    'b': tf.Variable(tf.zeros([n_dense])),\n    'b_out': tf.Variable(tf.zeros([n_classes]))\n}\n\nweight_dict = {\n    'w1': tf.get_variable('w1', [n_input, n_dense], initializer = weight_initializer),\n    'w_out': tf.get_variable('w_out', [n_dense, n_classes], initializer = weight_initializer)\n}\n\n# (7) Build Model\npredictions = network(x, weights=weight_dict, biases=bias_dict)\nprint(predictions)\n\n# (8) Define loss & optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)\n\n# (9) Define evaluation metrics\ncorrect_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\naccuracy_pct = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\n"})}),"\n",(0,i.jsx)(n.p,{children:"(4) Initialise and Run"}),"\n",(0,i.jsx)(n.p,{children:"Once everything is set up, initialise variables and execute the code in session! After 1000 epochs, you will see the test accuracy of 96%."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# (10) Initialization\ninitializer_op = tf.global_variables_initializer()\n\n# (11) Train the network in a session\nwith tf.Session() as session:\n    session.run(initializer_op)\n\n    print("Training for", epochs, "epochs.")\n\n    # Go through epochs\n    for epoch in range(epochs):\n        # monitoring each epochs on training cost and accuracy\n        avg_cost = 0.0\n        avg_acc = 0.0\n\n        # loop over all batches of the epoch:\n        n_batches = int(120 / batch_size)\n        for i in range(n_batches):\n\n            # Get the random int for batch\n            random_indices = np.random.randint(120, size=batch_size) # 120 is the no of training set records\n\n            feed = {\n                x: X_train[random_indices],\n                y: y_train[random_indices]\n            }\n\n            # feed batch data to run optimization and fetching cost and accuracy:\n            _, batch_cost, batch_acc = session.run([optimizer, cost, accuracy_pct],\n                                                   feed_dict=feed)\n\n            # accumulate mean loss and accuracy over epoch:\n            avg_cost += batch_cost / n_batches\n            avg_acc += batch_acc / n_batches\n\n        # Training cost and accuracy at end of each epoch of training:\n        print("Epoch ", \'%03d\' % (epoch+1),\n              ": cost = ", \'{:.3f}\'.format(avg_cost),\n              ", accuracy = ", \'{:.2f}\'.format(avg_acc), "%",\n              sep=\'\')\n\n    print("Training Complete. Testing Model.\\n")\n\n    test_cost = cost.eval({x: X_test, y: y_test})\n    test_acc = accuracy_pct.eval({x: X_test, y: y_test})\n\n    print("Test Cost:", \'{:.3f}\'.format(test_cost))\n    print("Test Accuracy: ", \'{:.2f}\'.format(test_acc), "%", sep=\'\')\n'})})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(96540);const a={},s=i.createContext(a);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);