"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[7731],{34897:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>d});var r=t(74848),a=t(28453);const s={sidebar_position:25},o="How To Get Survey Response Data From Qualtrics With Python",i={id:"data-ingestion/qualtrics-python",title:"How To Get Survey Response Data From Qualtrics With Python",description:"In the previous post, we had a look at Python code examples of basic data engineering with AWS infrastructure. By using Qualtrics API, I would like to present a coding example of API data ingestion into S3 and Redshift. This code can be scheduled hourly, daily or weekly in a server or AWS Data Pipeline.",source:"@site/docs/data-ingestion/26.qualtrics-python.md",sourceDirName:"data-ingestion",slug:"/data-ingestion/qualtrics-python",permalink:"/docs/data-ingestion/qualtrics-python",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:25,frontMatter:{sidebar_position:25},sidebar:"tutorialSidebar",previous:{title:"How To Get Data From SharePoint With Python",permalink:"/docs/data-ingestion/sharepoint-python"},next:{title:"Data Engineering in S3 and Redshift with Python",permalink:"/docs/data-ingestion/s3-redshift-py"}},l={},d=[];function c(e){const n={code:"code",h1:"h1",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"how-to-get-survey-response-data-from-qualtrics-with-python",children:"How To Get Survey Response Data From Qualtrics With Python"}),"\n",(0,r.jsx)(n.p,{children:"In the previous post, we had a look at Python code examples of basic data engineering with AWS infrastructure. By using Qualtrics API, I would like to present a coding example of API data ingestion into S3 and Redshift. This code can be scheduled hourly, daily or weekly in a server or AWS Data Pipeline."}),"\n",(0,r.jsx)(n.p,{children:"Qualtrics is an online survey software which allows you to send surveys via email or SMS, receive responses and generate reports. The aim of the ingestion is to get the survey response data into Redshift."}),"\n",(0,r.jsx)(n.p,{children:"API Reference"}),"\n",(0,r.jsx)(n.p,{children:"Qualtrics API is a simple REST-based API. Once you generate an API token, you are pretty much ready to go. They have comprehensive API documentation. In a nutshell, you can use the requests module to make a POST requests with the token in the header to get the data as a csv file. Further API references are here."}),"\n",(0,r.jsx)(n.p,{children:"I found Qualtrics API was unreliable hard way. My code initially failed randomly because I didn\u2019t have the for loop to keep the request repeating until it connects. In the code example, I set the maximum to 200 (see the bulk_export method). It usually works within 20 times."}),"\n",(0,r.jsx)(n.p,{children:"Key Points"}),"\n",(0,r.jsx)(n.p,{children:"In this example, we are using truncate & load because the data comes in one csv file with all the responses. We cannot obtain data incrementally. But, this is ok. We can leverage the power of Redshift copy command from S3, which is extremely fast. Truncate & load should be fine unless you have massive volume or other business requirements. If you want to do an incremental load, you can pick the record in the exported file according to the last updated time for insertion, which can be done relatively easily."}),"\n",(0,r.jsx)(n.p,{children:"The program exports response data in a csv format into a local directory, push it to a specified S3 bucket, and execute copy command after truncating the table. It can ingest responses from multiple surveys. The argument for survey project names has to be concatenated by \u2018,\u2019."}),"\n",(0,r.jsx)(n.p,{children:"The get_project_id method will return a list of survey ids based on the survey project name, which in turn uses to get the survey-specific response data."}),"\n",(0,r.jsx)(n.p,{children:"The format_colnames method takes care of formatting the column as the exported file has multiple rows for the column names. I am using the pandas package to do the data manipulation."}),"\n",(0,r.jsx)(n.p,{children:"Note that you also need AWS Access Key Id & Secret Access Key for the copy command."}),"\n",(0,r.jsx)(n.p,{children:"OK, here comes the code."}),"\n",(0,r.jsx)(n.p,{children:"Enjoy!"}),"\n",(0,r.jsx)(n.p,{children:"Code"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import requests\nimport zipfile\nimport json\nimport urllib2\nimport sys\nimport os\nimport pandas as pd\nimport shutil\nimport psycopg2\nimport boto3\nimport time\n'''\nThe character formatting below is for Python 2.7.\nGet rid of them for Python 3 and use encoding ='utf-8' when you open file.abs\nThe rest should work for both versions.\n'''\nreload(sys)\nsys.setdefaultencoding('utf8')\n\n# List of Argument taken from the command line\n\nproject_names = sys.argv[1]\n\nlst = []\nfor i in project_names.split(','):\n    lst.append(i)\n\ntarget_date = sys.argv[2]\nbucket_name = sys.argv[3]\ndbname = sys.argv[4]\nhost = sys.argv[5]\nport = sys.argv[6]\nuser = sys.argv[7]\npassword = sys.argv[8]\naws_access_key_id = sys.argv[9]\naws_secret_access_key = sys.argv[10]\ntarget_schema = sys.argv[11]\nbase_url = sys.argv[12]\nsecret_token = sys.argv[13]\n\n\n# (1) Getting a list of ID\n\ndef get_project_id(names):\n    '''The function takes a list of project names and return a list of IDs\n    '''\n    report_dic = {}\n    url = base_url + '/surveys/'\n    header = {'X-API-TOKEN': secret_token}\n\n    # (1) generating the request object\n    req = urllib2.Request(url,None,header)\n\n    # (2) Make request\n    response = urllib2.urlopen(req)\n    data = json.load(response)\n    # (3) Find Id for each project\n    for name in names:\n\n        # This is necessary because project names sometimes contain 2 spaces instead of 1\n        target_name = name.replace(\" \", \"\").lower()\n\n        # It is better to create a table name list separately\n        table_key = name.replace(\" \", \"_\").replace(\"-\", \"_\").lower()\n\n        # print target_name\n        for i in data['result']['elements']:\n            if i['name'].replace(\" \", \"\").lower() == target_name:\n                report_dic[table_key] = i['id']\n\n    return report_dic\n\n# (2) Get Metadata\n\ndef get_survey_metadata(report_dic, target_dir):\n    '''Takes survey ID and create json file in a specified directory'''\n\n    for k, v in report_dic.items():\n\n        url = base_url + '/surveys/' + v\n        header = {'X-API-TOKEN': secret_token}\n\n        req = urllib2.Request(url,None,header)\n        response = urllib2.urlopen(req)\n\n        data = json.load(response)\n        pretty = json.dumps(data, sort_keys=False, indent=4)\n        file = open('./' + target_dir + '/' + k + '_meta.json', 'w')\n        file.write(pretty)\n        print('Metadata File for %s Generated!' % (k))\n\n# (3) Exporting reports\n\nfileFormat = \"csv\"\nbaseUrl = base_url + \"/responseexports/\"\nheaders = {\"content-type\": \"application/json\",\"x-api-token\": secret_token}\n\ndef bulk_exports(report_dic):\n    '''This function takes a list of ids and create data export'''\n\n    if os.path.exists('./Exported'):\n        shutil.rmtree('./Exported')\n\n    for key, val in report_dic.items():\n        # Step 1: Creating Data Export\n        print(key, val)\n\n        downloadRequestUrl = baseUrl\n        downloadRequestPayload = '{\"format\":\"' + fileFormat + '\",\"surveyId\":\"' + val + '\"}'\n        downloadRequestResponse = requests.request(\"POST\", downloadRequestUrl, \\\n        data=downloadRequestPayload, headers=headers)\n        progressId = downloadRequestResponse.json()[\"result\"][\"id\"]\n\n        # Step 3: Downloading file\n        requestDownloadUrl = baseUrl + progressId + '/file'\n        requestDownload = requests.request(\"GET\", requestDownloadUrl, headers=headers, stream=True)\n        for i in range(0, 200):\n            print(str(requestDownload))\n            if str(requestDownload) == '<Response [200]>':\n                # Step 4: Unziping file\n                with open(\"RequestFile.zip\", \"wb\") as f:\n                    for chunk in requestDownload.iter_content(chunk_size=1024):\n                        f.write(chunk)\n                    f.close()\n                zipfile.ZipFile(\"RequestFile.zip\").extractall('Exported')\n                print('Completed Export for {}'.format(key))\n                os.remove(\"./RequestFile.zip\")\n                break\n            else:\n                time.sleep(10)\n                requestDownload = requests.request(\"GET\", requestDownloadUrl, headers=headers, stream=True)\n\n    for filename in os.listdir(\"Exported\"):\n        print(filename)\n        os.rename('./Exported/'+filename, './Exported/'+filename.replace(\" \", \"_\").replace(\"-\", \"_\").lower())\n        # os.rename('./'+filename, './'+filename.replace(\" \", \"_\").replace(\"-\", \"_\").lower())\n\n# (4) Create the folder before moving to S3\n\ndef create_dir(target_date):\n    direc = \"./\" + target_date\n\n    if not os.path.exists(direc):\n        os.makedirs(direc)\n        print('New directory %s has been created' % (target_date))\n    else:\n        shutil.rmtree(direc)\n        os.makedirs(direc)\n        print('New directory %s has been created' % (target_date))\n\n# (5) Reformat csv file and put into the right local folder created in (4)\n\ndef format_colnames(output_dir):\n    '''This function takes the file and rename its columns with the right format,\n    and generate csv file with the right column names'''\n\n    for filename in os.listdir(\"./Exported\"):\n        # (1) Read csv file\n        df = pd.read_csv(\"./Exported/\" + filename, skiprows=[0,1], low_memory=False)\n\n        columns = df.columns\n        new_cols = []\n\n        # (2) Reformat the column names\n        for name in columns:\n            new_name = name.replace('{', '').replace('}', '').split(':')[1].replace('\\'', '').\\\n            replace('-', '_').replace(' ', '')\n            new_cols.append(new_name)\n\n        # print new_cols\n        df.columns = new_cols\n\n        # (3) Create CSV file into the output directory\n        df.to_csv('./' + output_dir + '/' + filename, doublequote=True, sep='|', index=False)\n        print('Reformateed and moved %s' % (filename))\n\n# (6) Uploading to S3\n\ndef upload_files(local_path, s3_path, bucket_name):\n    '''Search all the files from specified directory and push to S3'''\n    s3 = boto3.resource('s3')\n\n    for (root, dirs, files) in os.walk(local_path):\n        for filename in files:\n            print(\"File: {}\".format(filename))\n            s3_filename = s3_path + filename\n            print('Uploading to %s...' % (s3_filename))\n            s3.meta.client.upload_file(local_path + filename, bucket_name, s3_filename)\n            print('Done')\n\n# (7) Truncate & Load to Redshift\n\ndef truncate_load_tables(report_dict):\n    con = psycopg2.connect(dbname=dbname, host=host, port=port, user=user, password=password)\n    print(\"Connection to Redshift Successful!\")\n    cur = con.cursor()\n    for k, v in report_dict.items():\n        target_table = target_schema + '.' + k\n        file_name = 's3://' + bucket_name + '/Qualtrics/data_export/' + k + '.csv'\n\n        sql = \"\"\"\n        Truncate %s;Commit;\n        copy %s from '%s'  dateformat 'auto' credentials\n        'aws_access_key_id=%s;aws_secret_access_key=%s' CSV QUOTE '\"' DELIMITER '|'\n        ACCEPTINVCHARS EMPTYASNULL COMPUPDATE OFF IGNOREHEADER 1;\n        Commit;\n        \"\"\" % (target_table, target_table, file_name, aws_access_key_id, aws_secret_access_key)\n\n        print(sql)\n        cur.execute(sql)\n        print(\"Copy Command executed successfully for %s\" % (target_table))\n    con.close()\n\n# Execution #\n\n# (1) get the dictionary with report name and report id to export\nreports = get_project_id(lst)\n\n# (2) Create a directory for S3 transfer\ncreate_dir(target_date)\n\n# (3) Do Bulk Export\nbulk_exports(reports)\n\n# (4) Get metadata and prep for S3 transfer\nget_survey_metadata(reports, target_date)\n\n# (5) Transfer\nformat_colnames(target_date)\n\n# (6) Move to S3\nupload_files('./' + target_date + '/', 'Qualtrics/data_export/', bucket_name)\n\n# (7) Load Table\ntruncate_load_tables(reports)\n"})}),"\n",(0,r.jsx)(n.p,{children:"(2017-11-12)"})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>i});var r=t(96540);const a={},s=r.createContext(a);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);