"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[1587],{54026:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>c});var a=r(74848),n=r(28453);const i={slug:"data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python",title:"Predict Internet Popularity By Optimising Neural Networks With Python",tags:["Data Science","Machine Learning","caret","Hyper Parameter Search","Neural Networks","nnet","Python"]},s=void 0,o={permalink:"/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python",source:"@site/data-science/machine-learning/2018-01-12-internet-popularity-python.md",title:"Predict Internet Popularity By Optimising Neural Networks With Python",description:"In the previous post, we used grid search to find the best hyper parameter for the neural network model with R\u2019s caret package. Here, let\u2019s use Python and scikit-learn package to optimise a neural network model.",date:"2018-01-12T00:00:00.000Z",tags:[{label:"Data Science",permalink:"/data-science/tags/data-science"},{label:"Machine Learning",permalink:"/data-science/tags/machine-learning"},{label:"caret",permalink:"/data-science/tags/caret"},{label:"Hyper Parameter Search",permalink:"/data-science/tags/hyper-parameter-search"},{label:"Neural Networks",permalink:"/data-science/tags/neural-networks"},{label:"nnet",permalink:"/data-science/tags/nnet"},{label:"Python",permalink:"/data-science/tags/python"}],readingTime:7.225,hasTruncateMarker:!1,authors:[],frontMatter:{slug:"data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python",title:"Predict Internet Popularity By Optimising Neural Networks With Python",tags:["Data Science","Machine Learning","caret","Hyper Parameter Search","Neural Networks","nnet","Python"]},unlisted:!1,prevItem:{title:"How To Create Your Own Personal Data Science Computing Environment In AWS",permalink:"/data-science/data-science/infra/how-to-create-your-personal-data-science-computing-environment-in-aws"},nextItem:{title:"Predict Internet Popularity By Optimising Neural Networks With R",permalink:"/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r"}},l={authorsImageUrls:[]},c=[];function p(e){const t={code:"code",img:"img",p:"p",pre:"pre",...(0,n.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"In the previous post, we used grid search to find the best hyper parameter for the neural network model with R\u2019s caret package. Here, let\u2019s use Python and scikit-learn package to optimise a neural network model."}),"\n",(0,a.jsx)(t.p,{children:"Just like the caret package, scikit-learn has a pre-built function for hyper parameter search. As for dataset, we will use Online News Popularity Data Set from the UCI Machine Learning repository, which is the same dataset used in the previous post. It was originally used in this publication."}),"\n",(0,a.jsx)(t.p,{children:"The nnet package from previous post is a single hidden layer back-propagation network. Therefore, there is only one size parameter for the layer to tune. Here, we will use MLPClassifier that implements a multi-layer perceptron algorithm. Having a multiple hidden layers enables us to specify the number of layers as well as the number of neurons for each layer."}),"\n",(0,a.jsx)(t.p,{children:"As for parameter optimisation, we will use GridSearchCV with 10-fold cross validation. Optimising multi-layer neural networks can be lengthy because you can try different layer numbers and neuron size in each layer. Here, let\u2019s use 2 layers and try to optimise the neuron size per layer for simplicity. Generally speaking, stating small works fine with neural networks in terms of both layer and neuron sizes."}),"\n",(0,a.jsx)(t.p,{children:"We will also search for the best alpha parameter for L2 regularisation (you can read more about regularisation here). In short, regularisation prevents over-fitting by penalising it. With no regularisation term, you will get a great accuracy on training set, but not so great on test. Neural networks are often called as a black box method, which makes the method fancy and magical. In realisty, they are a variation of nonlinear statistical models. So, regularisation terms work just like logistic regression. Scikit-learn has a great documentation on MPL here for more detail."}),"\n",(0,a.jsx)(t.p,{children:"As in the previous post, we are dealing with the prediction of binary classifier (\u2018popular\u2019 or \u2018unpopular\u2019) based on the attributes comes with online news paper articles (see details here)."}),"\n",(0,a.jsx)(t.p,{children:"OK, let\u2019s code!"}),"\n",(0,a.jsx)(t.p,{children:"Summary Steps"}),"\n",(0,a.jsx)(t.p,{children:"Get data and prep it (by selecting the right columns, splitting them to training and test and normalising the data).\nDo hyperparameter search.\nModel & predict with the best hyper parameter.\nCalculate performance metrics and draw ROC curve.\nCode"}),"\n",(0,a.jsx)(t.p,{children:"(1) First of all, we have to load the data (after downloading data from here) and do data prep. I normalised it after splitting into training and test set, but you can do it prior to the split. Note that you have to use the scaler function fitted with training data for the test data set to keep consistency if you are doing this way."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# (1) Get data\nshares = pd.read_csv(\"/tmp/OnlineNewsPopularity.csv\")\n\n# (2) Check data\nshares.head(5)\nshares.shape\n\n# (3) Prepare data (train_test split and normalise)\nX = shares.iloc[:,2:59]\ny = shares.iloc[:,60]\n\ny = np.where(y >= 1400, 'Popular', 'Unpopular')\n\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=42)\n\nscaler = StandardScaler()\n# Fit only to the training data\nscaler.fit(train_x)\n# Now apply the transformations to the data:\ntrain_x = scaler.transform(train_x)\ntest_x = scaler.transform(test_x)\n"})}),"\n",(0,a.jsx)(t.p,{children:"(2) I created a grid_search function so that I can reuse it for other models. It takes a scikit-learn model, feature matrix, label array and parameter grids and return the best parameters."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from sklearn.model_selection import GridSearchCV\n\ndef grid_search(estimator, X, y, param_grid):\n    '''Takes a sklearn model, feature matrix, label array and parameter grid dictionary.\n    Returns the dictionary of best parameters'''\n    clf = GridSearchCV(estimator, param_grid = param_grid, n_jobs=-1, cv=10, scoring='f1_weighted')\n    clf.fit(X, y)\n    print('Best F1 Score is: {}'.format(clf.best_score_))\n    print('Best Parameter is: {}'.format(clf.best_params_))\n    return clf.best_params_\n"})}),"\n",(0,a.jsx)(t.p,{children:"(3) See the range of the alpha and neuron size for each layer that I choose. For some reason, neuron size (5, 2) appear in many places (including the official documentation). So, I chose my parameter grid range around them. The bigger the neuron size is, the more complex the models become. Generally speaking, a smaller neuron size works for most of the time. But, of course, you have to experiment it a lot to find the best size. You can read this post for more details. It is a good starting point to know about layer and neuron size."}),"\n",(0,a.jsx)(t.p,{children:"I picked lbfgs for a solver. You can try to use different ones. Again, you have to experiment it to see which one works best for your use case."}),"\n",(0,a.jsx)(t.p,{children:"As I did with R code here, you can do parallel computing by adding the n_jobs=-1. As the grid search is an iterative CPU intense process, it is always faster to do parallel."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from sklearn.neural_network import MLPClassifier\n\nalpha = [1e-5,3e-5,1e-4,3e-4,1e-3,3e-3, 1e-2,3e-2]\nhidden_layer_sizes = [(3,1), (5,2), (9,4)]\nparam_grid = {'alpha':alpha, 'hidden_layer_sizes':hidden_layer_sizes}\nestimator = MLPClassifier(solver='lbfgs',random_state=1)\nbest_param = grid_search(estimator, train_x, train_y, param_grid)\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Best F1 Score is: ",(0,a.jsx)(t.code,{children:"0.6631095339771439"}),"\nBest Parameter is: ",(0,a.jsx)(t.code,{children:"{\u2018alpha\u2019: 3e-05, \u2018hidden_layer_sizes\u2019: (5, 2)}"})]}),"\n",(0,a.jsx)(t.p,{children:"(4) Now, we got the best hyperparameter set, let\u2019s model the neural net and do prediction."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"clf = MLPClassifier(solver='lbfgs', alpha=best_param['alpha'],\\\n        hidden_layer_sizes=best_param['hidden_layer_sizes'], random_state=1)\nclf.fit(train_x, train_y)\npred_train = clf.predict(train_x)\npred_test = clf.predict(test_x)\n\npred_train_prob = clf.predict_proba(train_x)[:, 1]\npred_test_prob = clf.predict_proba(test_x)[:, 1]\n"})}),"\n",(0,a.jsx)(t.p,{children:"(5) I created two functions to get all the evaluation metrics. The calculate_auc function also produces ROC. I used pandas magic to create a data frame for the easy summary of performance metrics. These metrics are the same one use in the publication."}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc, roc_curve\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef evaluation(train_y, pred_train, test_y, pred_test, model_name):\n\n    train_acc = accuracy_score(train_y, pred_train)\n    test_acc = accuracy_score(test_y, pred_test)\n\n    train_precision = precision_score(train_y, pred_train, pos_label='Unpopular')\n    test_precision = precision_score(test_y, pred_test, pos_label='Unpopular')\n\n    train_recall = recall_score(train_y, pred_train, pos_label='Unpopular')\n    test_recall = recall_score(test_y, pred_test, pos_label='Unpopular')\n\n    train_f1 = f1_score(train_y, pred_train, pos_label='Unpopular')\n    test_f1 = f1_score(test_y, pred_test, pos_label='Unpopular')\n\n    train_df = pd.DataFrame({'Accuracy':[train_acc], 'Precision':[train_precision],\\\n                             'Recall':[train_recall], 'F1':[train_f1]})\n    test_df = pd.DataFrame({'Accuracy':[test_acc], 'Precision':[test_precision],\\\n                             'Recall':[test_recall], 'F1':[test_f1]})\n    return train_df, test_df\n\ntraindf1, testdf1 = evaluation(train_y, pred_train, test_y, pred_test, 'Neural Network')\n\ndef calculate_auc(train_y, pred_train_prob, test_y, pred_test_prob, model_name):\n    fpr_train, tpr_train, thresholds_train = roc_curve(train_y, pred_train_prob, pos_label='Unpopular')\n    fpr_test, tpr_test, thresholds_test = roc_curve(test_y, pred_test_prob, pos_label='Unpopular')\n    train_auc = auc(fpr_train, tpr_train)\n    test_auc = auc(fpr_test, tpr_test)\n\n    # Draw ROC curve\n    plt.figure()\n    lw = 2\n    plt.plot(fpr_test, tpr_test, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % test_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Test Data ROC Curve With Neural Network')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    train_df = pd.DataFrame({'AUC':[train_auc]})\n    test_df = pd.DataFrame({'AUC':[test_auc]})\n    return train_df, test_df\n\ntraindf2, testdf2 = calculate_auc(train_y, pred_train_prob, test_y, pred_test_prob, 'Neural Network')\n\ntrain_df = pd.concat([traindf1, traindf2], axis=1)\ntest_df = pd.concat([testdf1, testdf2], axis=1)\n\nprint(\"Training Data Performance Metrics\\n\")\nprint(round(train_df, 2))\nprint(\"\\nTest Data Performance Metrics\\n\")\nprint(round(test_df, 2))\n"})}),"\n",(0,a.jsx)(t.p,{children:"Here is the output."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"ROC Curve",src:r(83604).A+"",width:"668",height:"669"})}),"\n",(0,a.jsx)(t.p,{children:"Comparing it With Default MLP Classifier Model"}),"\n",(0,a.jsx)(t.p,{children:"If you don\u2019t set any parameter, the default alpha is 0.0001 and hidden_layer_sizes is 100 neurons in a single layer. The default model is over-fitting, which happens a lot for neural networks. You can see the power of simple parameter tuning!"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"clf = MLPClassifier(solver='lbfgs', random_state=1) # default alpha=1e-5\nclf.fit(train_x, train_y)\npred_train = clf.predict(train_x)\npred_test = clf.predict(test_x)\ntrain_acc = accuracy_score(train_y, pred_train)\ntest_acc = accuracy_score(test_y, pred_test)\nprint('Neural Network Model Train Accuracy: {}'.format(train_acc))\nprint('Neural Network Model Test Accuracy: {}'.format(test_acc))\n"})}),"\n",(0,a.jsx)(t.p,{children:"Neural Network Model Train Accuracy: 0.77\nNeural Network Model Test Accuracy: 0.62"}),"\n",(0,a.jsx)(t.p,{children:"Let\u2019s Benchmark!"}),"\n",(0,a.jsx)(t.p,{children:"Below is the performance metric table from the paper (Fernandes et al. 2015). Our neural net model was comparable to the second best method (AdaBoost) in terms of accuracy and AUC. The model is still over-fitting slightly and has lower recall. So, I think there are more room for optimisation. All in all, pretty good outcome considering how simple the whole modelling process was."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Benchmark",src:r(48909).A+"",width:"1030",height:"243"})}),"\n",(0,a.jsx)(t.p,{children:"Your Turn"}),"\n",(0,a.jsxs)(t.p,{children:["In the publication, they also used grid search to find the best hyper parameter set in a few different methods. In the paper, SCV was optimised with C \u2208 ",4," , RF and AdaBoost was with number of trees \u2208 ",400," and KNN was number of neighbors \u2208 ",20,". Now that we have a custom grid_search function, you can try finding the best hyper parameters for all these methods in a streamlined manner."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"from sklearn.svm import SVC\nsvc_model = SVC(kernel='rbf', random_state=23)\nsvc_param = {'C':[2**0, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]}\nxx, yy, zz = grid_search(svc_model, train_x, train_y, svc_param)\n\nfrom sklearn.ensemble import RandomForestClassifier\n# n_estimator is the number of trees\nrf_param = {'n_estimators':[10, 20, 50, 100, 200, 400]}\nrf = RandomForestClassifier(n_jobs=-1, random_state=42)\nxx, yy, zz = grid_search(rf, train_x, train_y, rf_param)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_param = {'n_neighbors':[10, 20, 50, 100, 200, 400]}\nknn = KNeighborsClassifier(n_jobs=-1)\nxx, yy, zz = grid_search(knn, train_x, train_y, knn_param)\n"})})]})}function d(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},48909:(e,t,r)=>{r.d(t,{A:()=>a});const a=r.p+"assets/images/performance-benchmark-python-ea38c24d2c5f25bf7d3e11d4a48a41bd.webp"},83604:(e,t,r)=>{r.d(t,{A:()=>a});const a=r.p+"assets/images/roc-for-neural-net-with-python-4284f3565e35a7ff88e9f5a4d60ed35f.webp"},28453:(e,t,r)=>{r.d(t,{R:()=>s,x:()=>o});var a=r(96540);const n={},i=a.createContext(n);function s(e){const t=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),a.createElement(i.Provider,{value:t},e.children)}}}]);