"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[7179],{4692:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var a=t(74848),o=t(28453);const s={sidebar_position:29},i="How To Get Facebook Data With Python",r={id:"data-ingestion/facebook-py",title:"How To Get Facebook Data With Python",description:"By using Facebook Graph API, we can get the feed of posts and links published by the specific page, or by others on this page as well as likes and comments (feed api). I have written a python script to scrape the feed info in the JSON format and turn it into structured tables. Once the data is in the tabular format, we can load it in the relational database or use common analytical tools (like Excel) to do further analysis.",source:"@site/docs/data-ingestion/29.facebook-py.md",sourceDirName:"data-ingestion",slug:"/data-ingestion/facebook-py",permalink:"/mydatahack-old-blog/docs/data-ingestion/facebook-py",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:29,frontMatter:{sidebar_position:29},sidebar:"tutorialSidebar",previous:{title:"How To Get Data From Google Analytics With Python",permalink:"/mydatahack-old-blog/docs/data-ingestion/ga-py"},next:{title:"How To Get Twitter Data With Python",permalink:"/mydatahack-old-blog/docs/data-ingestion/twitter-py"}},c={},l=[];function d(e){const n={a:"a",code:"code",h1:"h1",img:"img",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"how-to-get-facebook-data-with-python",children:"How To Get Facebook Data With Python"}),"\n",(0,a.jsx)(n.p,{children:"By using Facebook Graph API, we can get the feed of posts and links published by the specific page, or by others on this page as well as likes and comments (feed api). I have written a python script to scrape the feed info in the JSON format and turn it into structured tables. Once the data is in the tabular format, we can load it in the relational database or use common analytical tools (like Excel) to do further analysis."}),"\n",(0,a.jsx)(n.p,{children:"Data Model"}),"\n",(0,a.jsx)(n.p,{children:"We can split feed data into 3 tables. Each post has one or many likes and comments. This data model nicely accommodates the one-to-many relationship. In the Feed table, Page_Name and Id are the composite keys. Likes and Comments can be joined to Feed by the Page_Name and Post_Id."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"img",src:t(23549).A+"",width:"1037",height:"749"})}),"\n",(0,a.jsx)(n.p,{children:"Facebook Graph API"}),"\n",(0,a.jsx)(n.p,{children:"Facebook offers different methods for authentication depending on which API function you want to use. In this example, all we need is App ID and App Secret. We can use this neat trick to create access token by concatenating App ID and App Secret with \u201c|\u201d."}),"\n",(0,a.jsx)(n.p,{children:"First of all, we need to create an app and generate API credentials."}),"\n",(0,a.jsxs)(n.p,{children:["Login to Facebook and go to ",(0,a.jsx)(n.a,{href:"https://developers.facebook.com/",children:"https://developers.facebook.com/"}),".\nSelect \u2018Add New App\u2019 from the top left corner.\nEnter Display Name and hit \u2018Create App ID\u2019.\nGet App ID and App Secret From the dashboard\nAccess Token = ",(0,a.jsx)(n.code,{children:"<App ID>|<App Secret>"}),"\nPython has Facebook SDK and it works fine. However, I am using the requests and json packages to make API calls and process data. In my opinion, the requests package is the best thing happened for creating REST applications with Python. To make a GET request, we can simply add url and access token as a parameter in the get() function. Then, we convert the response to a JOSN object for further processing."]}),"\n",(0,a.jsx)(n.p,{children:"Facebook Graph API"}),"\n",(0,a.jsx)(n.p,{children:"It takes 7 argumenst: Access Token, Page Name (e.g. CocaCola), Json File Name, Feed csv file path, Likes csv file path, Comments csv file path, Since data (from when to pull the data)."}),"\n",(0,a.jsx)(n.p,{children:"Example Call"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python facebookScrapeFeed.py &lt;Access Token&gt; CocaCola \\\nfeed.json feed.csv likes.csv comments.csv 2017-10-31\n"})}),"\n",(0,a.jsx)(n.p,{children:"Key Points"}),"\n",(0,a.jsx)(n.p,{children:"Since date has to be converted to a unix timestamp. I created the method to convert a regular date string to the unix timestamp, convert_to_epochtime()."}),"\n",(0,a.jsx)(n.p,{children:"The maximum number of feed records is 100. To obtain more than 100 records, we loop GET request by incrementing the offset parameter."}),"\n",(0,a.jsx)(n.p,{children:"The maximum records for Likes and Comments in the Feed json file are 25. If there are more than 25 records, we can use the url in the next node until there is no next url comes back in the data."}),"\n",(0,a.jsx)(n.p,{children:"The script works for both Python 2.7 and 3.x by changing the few lines to handle Unicode as instructed in the script. This is because each version handles Unicode differently."}),"\n",(0,a.jsx)(n.p,{children:"Now, here comes the code!"}),"\n",(0,a.jsx)(n.p,{children:"Code"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import requests\nimport json\nimport sys\nimport time\n\'\'\'\nreloading sys for utf8 encoding is for Python 2.7\nThis line should be removed for Python 3\nIn Python 3, we need to specify encoding when open a file\nf = open("file.csv", encoding=\'utf-8\')\n\'\'\'\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\n\nclass FacebookScraper:\n    \'\'\'\n    FacebookScraper class to scrape facebook info\n    \'\'\'\n\n    def __init__(self, token):\n        self.token = token\n\n    @staticmethod\n    def convert_to_epochtime(date_string):\n        \'\'\'Enter date_string in 2000-01-01 format and convert to epochtime\'\'\'\n        try:\n            epoch = int(time.mktime(time.strptime(date_string, \'%Y-%m-%d\')))\n            return epoch\n        except ValueError:\n            print(\'Invalid string format. Make sure to use %Y-%m-%d\')\n            quit()\n\n    def get_feed_data(self, target_page, offset, fields, json_path, date_string):\n        """\n        This method will get the feed data\n        """\n        url = "https://graph.facebook.com/v2.10/{}/feed".format(target_page)\n        param = dict()\n        param["access_token"] = self.token\n        param["limit"] = "100"\n        param["offset"] = offset\n        param["fields"] = fields\n        param["since"] = self.convert_to_epochtime(date_string)\n\n        r = requests.get(url, param)\n        data = json.loads(r.text)\n        f = open(json_path, "w")\n        f.write(json.dumps(data, indent=4))\n        print("json file has been generated")\n\n        f.close()\n\n        return data\n\n    def create_table(self, list_rows, file_path, page_name, table_name):\n        \'\'\'This method will create a table according to header and table name\'\'\'\n\n        if table_name == "feed" :\n            header = ["page_name", "id", "type", "created_time", "message", "name",\\\n            "description", "actions_link", "actions_name", "share_count",\\\n            "comment_count", "like_count"]\n        elif table_name == "likes":\n            header = ["page_name", "post_id", "user_id", "name"]\n        elif table_name == "comments":\n            header = ["page_name", "post_id", "created_time", "message",\\\n             "user_id", "name", "message_id"]\n        else:\n            print("Specified table name is not valid.")\n            quit()\n\n        file = open(file_path, \'w\')\n        file.write(\',\'.join(header) + \'\\n\')\n        for i in list_rows:\n            file.write(\'"\' + page_name + \'",\')\n            for j in range(len(i)):\n                row_string = \'\'\n                if j < len(i) -1 :\n                    row_string += \'"\' + str(i[j]).replace(\'"\', \'\').replace(\'\\n\', \'\') + \'"\' + \',\'\n                else:\n                    row_string += \'"\' + str(i[j]).replace(\'"\', \'\').replace(\'\\n\', \'\') + \'"\' + \'\\n\'\n                file.write(row_string)\n        file.close()\n        print("Generated {} table csv File for {}".format(table_name, page_name))\n\n    def convert_feed_data(self, response_json_list):\n        \'\'\'This method takes response json data and convert to csv\'\'\'\n        list_all = []\n        for response_json in response_json_list:\n            data = response_json["data"]\n\n            for i in range(len(data)):\n                list_row = []\n                row = data[i]\n                id = row["id"]\n                try:\n                    type = row["type"]\n                except KeyError:\n                    type = ""\n                try:\n                    created_time = row["created_time"]\n                except KeyError:\n                    created_time = ""\n                try:\n                    message = row["message"]\n                except KeyError:\n                    message = ""\n                try:\n                    name = row["name"]\n                except KeyError:\n                    name = ""\n                try:\n                    description = row["description"]\n                except KeyError:\n                    description = ""\n                try:\n                    actions_link = row["actions"][0]["link"]\n                except KeyError:\n                    actions_link = ""\n                try:\n                    actions_name = row["actions"][0]["name"]\n                except KeyError:\n                    actions_name = ""\n                try:\n                    share_count = row["shares"]["count"]\n                except KeyError:\n                    share_count = ""\n                try:\n                    comment_count = row["comments"]["summary"]["total_count"]\n                except KeyError:\n                    comment_count = ""\n                try:\n                    like_count = row["likes"]["summary"]["total_count"]\n                except KeyError:\n                    like_count = ""\n\n                list_row.extend((id, type, created_time, message, name, \\\n                description, actions_link, actions_name, share_count, comment_count, like_count))\n                list_all.append(list_row)\n\n        return list_all\n\n    def convert_likes_data(self, response_json_list):\n        \'\'\'This will get the list of people who liked post,\n        which can be joined to the feed table by post_id. \'\'\'\n        list_all = []\n        for response_json in response_json_list:\n            data = response_json["data"]\n            # like_list = []\n            for i in range(len(data)):\n                likes_count = 0\n                row = data[i]\n                post_id = row["id"]\n                try:\n                   like_count = row["likes"]["summary"]["total_count"]\n                except KeyError:\n                    like_count = 0\n                if like_count > 0:\n                    likes = row["likes"]["data"]\n                    for like in likes:\n                        row_list = []\n                        user_id = like["id"]\n                        name = like["name"]\n                        row_list.extend((post_id, user_id, name))\n                        list_all.append(row_list)\n                # Check if the next link exists\n                try:\n                    next_link = row["likes"]["paging"]["next"]\n                except KeyError:\n                    next_link = None\n                    continue\n\n                if next_link is not None:\n                    r = requests.get(next_link.replace("limit=25", "limit=100"))\n                    likes_data = json.loads(r.text)\n                    while True:\n                        for i in range(len(likes_data["data"])):\n                            row_list = []\n                            row = likes_data["data"][i]\n                            user_id = row["id"]\n                            name = row["name"].encode("latin1", "ignore")\n                            row_list.extend((post_id, user_id, name))\n                            list_all.append(row_list)\n                        try:\n                            next = likes_data["paging"]["next"]\n                            r = requests.get(next.replace("limit=25", "limit=100"))\n                            likes_data = json.loads(r.text)\n                        except KeyError:\n                            print("Likes for the post {} completed".format(post_id))\n                            break\n        return list_all\n\n    def convert_comments_data(self, response_json_list):\n        \'\'\'This will get the list of people who commented on the post,\n        which can be joined to the feed table by post_id. \'\'\'\n        list_all = []\n        for response_json in response_json_list:\n            data = response_json["data"]\n            # like_list = []\n            for i in range(len(data)):\n                likes_count = 0\n                row = data[i]\n                post_id = row["id"]\n                try:\n                   comment_count = row["comments"]["summary"]["total_count"]\n                except KeyError:\n                    comment_count = 0\n                if comment_count > 0:\n                    comments = row["comments"]["data"]\n                    for comment in comments:\n                        row_list = []\n                        created_time = comment["created_time"]\n                        message = comment["message"].encode(\'latin1\', \'ignore\')\n                        user_id = comment["from"]["id"]\n                        name = comment["from"]["name"].encode(\'latin1\', \'ignore\')\n                        message_id = comment["id"]\n                        row_list.extend((post_id, created_time, message,\\\n                        user_id, name, message_id))\n                        list_all.append(row_list)\n\n                # Check if the next link exists\n                try:\n                    next_link = row["comments"]["paging"]["next"]\n                except KeyError:\n                    next_link = None\n                    continue\n\n                if next_link is not None:\n                    r = requests.get(next_link.replace("limit=25", "limit=100"))\n                    comments_data = json.loads(r.text)\n                    while True:\n                        for i in range(len(comments_data["data"])):\n                            row_list = []\n                            comment = comments_data["data"][i]\n                            created_time = comment["created_time"]\n                            message = comment["message"].encode(\'latin1\', \'ignore\')\n                            user_id = comment["from"]["id"]\n                            name = comment["from"]["name"].encode(\'latin1\', \'ignore\')\n                            message_id = comment["id"]\n                            row_list.extend((post_id, created_time, message,\\\n                            user_id, name, message_id))\n                            list_all.append(row_list)\n                        try:\n                            next = comments_data["paging"]["next"]\n                            r = requests.get(next.replace("limit=25", "limit=100"))\n                            comments_data = json.loads(r.text)\n                        except KeyError:\n                            print("Comments for the post {} completed".format(post_id))\n                            break\n        return list_all\n\nif __name__ == "__main__":\n\n    token_input = sys.argv[1]\n    target_page_input = sys.argv[2]\n    json_path_input = sys.argv[3]\n    csv_feed_path_input = sys.argv[4]\n    csv_likes_path_input = sys.argv[5]\n    csv_comments_path_input = sys.argv[6]\n    date_since_input = sys.argv[7]\n    # Input check\n    print(token_input)\n    print(target_page_input)\n    field_input = \'id,created_time,name,message,comments.summary(true),\\\n    shares,type,published,link,likes.summary(true),actions,place,tags,\\\n    object_attachment,targeting,feed_targeting,scheduled_publish_time,\\\n    backdated_time,description\'\n\n    fb = FacebookScraper(token_input)\n\n    offset = 0\n    json_list = []\n    while True:\n        path = str(offset) + "_" + json_path_input\n        try:\n            data = fb.get_feed_data(target_page_input, str(offset), field_input, path, date_since_input)\n            check = data[\'data\']\n            if (len(check) >= 100):\n                json_list.append(data)\n                offset += 100\n            else:\n                json_list.append(data)\n                print("End of loop for obtaining more than 100 feed records.")\n                break\n        except KeyError:\n            print("Error with get request.")\n            quit()\n\n    feed_table_list = fb.convert_feed_data(json_list)\n    likes_table_list = fb.convert_likes_data(json_list)\n    comments_table_list = fb.convert_comments_data(json_list)\n    # Record check\n    print(feed_table_list[0])\n    print(likes_table_list[0])\n    print(comments_table_list[0])\n\n    fb.create_table(feed_table_list, csv_feed_path_input, target_page_input, "feed")\n    fb.create_table(likes_table_list, csv_likes_path_input, target_page_input, "likes")\n    fb.create_table(comments_table_list, csv_comments_path_input, target_page_input, "comments")\n'})}),"\n",(0,a.jsx)(n.p,{children:"(2017-11-06)"})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},23549:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/facebook-feed-data-model-142ca6688031842cf42750eaf4bd18f2.webp"},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var a=t(96540);const o={},s=a.createContext(o);function i(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);