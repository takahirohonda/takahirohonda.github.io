"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[44],{68783:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"data-science/deep-learning/building-alexnet-with-keras","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/building-alexnet-with-keras","source":"@site/data-science/deep-learning/2018-05-07-alexnet-keras.md","title":"Building AlexNet with Keras","description":"As the legend goes, the deep learning networks created by Alex Krizhevsky, Geoffrey Hinton and Ilya Sutskever (now largely know as AlexNet) blew everyone out of the water and won Image Classification Challenge (ILSVRC) in 2012. This heralded the new era of deep learning. AlexNet is the most influential modern deep learning networks in machine vision that use multiple convolutional and dense layers and distributed computing with GPU.","date":"2018-05-07T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Deep Learning","permalink":"/mydatahack-old-blog/data-science/tags/deep-learning"},{"label":"AlexNet","permalink":"/mydatahack-old-blog/data-science/tags/alex-net"},{"label":"Convolutional Neural Networks","permalink":"/mydatahack-old-blog/data-science/tags/convolutional-neural-networks"},{"label":"Image Classification","permalink":"/mydatahack-old-blog/data-science/tags/image-classification"},{"label":"Keras","permalink":"/mydatahack-old-blog/data-science/tags/keras"}],"readingTime":3.31,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/deep-learning/building-alexnet-with-keras","title":"Building AlexNet with Keras","tags":["Data Science","Deep Learning","AlexNet","Convolutional Neural Networks","Image Classification","Keras"]},"unlisted":false,"nextItem":{"title":"Building AlexNet with TensorFlow and Running it with AWS SageMaker","permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/building-alexnet-with-tensorflow-and-running-it-with-aws-sagemaker"}},"content":"As the legend goes, the deep learning networks created by Alex Krizhevsky, Geoffrey Hinton and Ilya Sutskever (now largely know as AlexNet) blew everyone out of the water and won Image Classification Challenge (ILSVRC) in 2012. This heralded the new era of deep learning. \x3c!--truncate--\x3eAlexNet is the most influential modern deep learning networks in machine vision that use multiple convolutional and dense layers and distributed computing with GPU.\\n\\nAlong with LeNet-5, AlexNet is one of the most important & influential neural network architectures that demonstrate the power of convolutional layers in machine vision. So, let\u2019s build AlexNet with Keras first, them move onto building it in .\\n\\nDataset\\n\\nWe are using OxfordFlower17 in the tflearn package. The dataset consists of 17 categories of flowers with 80 images for each class. It is a three dimensional data with RGB colour values per each pixel along with the width and height pixels.\\n\\nAlexNet Architecture\\n\\nAlexNet consist of 5 convolutional layers and 3 dense layers. The data gets split into to 2 GPU cores. The image below is from the first reference the AlexNet Wikipedia page here.\\n\\n![AlexNet Architecture](./img/alexnet-architecture.png)\\n\\nAlexNet with Keras\\n\\nI made a few changes in order to simplify a few things and further optimise the training outcome. First of all, I am using the sequential model and eliminating the parallelism for simplification. For example, the first convolutional layer has 2 layers with 48 neurons each. Instead, I am combining it to 98 neurons.\\n\\nThe original architecture did not have batch normalisation after every layer (although it had normalisation between a few layers) and dropouts. I am putting the batch normalisation before the input after every layer and dropouts between the fully-connected layers to reduce over-fitting.\\n\\nWhen to use batch normalisation is difficult. Everyone seems to have opinions or evidence that supports their opinions. Without going into too much details, I decided to normalise before the input as it seems to make sense statistically.\\n\\n**Code**\\n\\nHere is the code example. The input data is 3-dimensional and then you need to flatten the data before passing it into the dense layer. Using cross-entropy for the loss function, adam for optimiser and accuracy for performance metrics.\\n\\nAs the network is complex, it takes a long time to run. It took about 10 hours to run 250 epochs on my cheap laptop with CPU. The test dataset accuracy is not great. This is probably because we do not have enough datasets. I don\u2019t think 80 images each is enough for convolutional neural networks. But, it still runs.\\n\\nIt\u2019s pretty amazing that what was the-state-of-the-art in 2012 can be done with a very little programming and run on your $700 laptops!\\n\\n```python\\n# (1) Importing dependency\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, Dropout, Flatten,\\\\\\n Conv2D, MaxPooling2D\\nfrom keras.layers.normalization import BatchNormalization\\nimport numpy as np\\nnp.random.seed(1000)\\n\\n# (2) Get Data\\nimport tflearn.datasets.oxflower17 as oxflower17\\nx, y = oxflower17.load_data(one_hot=True)\\n\\n# (3) Create a sequential model\\nmodel = Sequential()\\n\\n# 1st Convolutional Layer\\nmodel.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),\\\\\\n strides=(4,4), padding=\'valid\'))\\nmodel.add(Activation(\'relu\'))\\n# Pooling\\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'valid\'))\\n# Batch Normalisation before passing it to the next layer\\nmodel.add(BatchNormalization())\\n\\n# 2nd Convolutional Layer\\nmodel.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding=\'valid\'))\\nmodel.add(Activation(\'relu\'))\\n# Pooling\\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'valid\'))\\n# Batch Normalisation\\nmodel.add(BatchNormalization())\\n\\n# 3rd Convolutional Layer\\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\'valid\'))\\nmodel.add(Activation(\'relu\'))\\n# Batch Normalisation\\nmodel.add(BatchNormalization())\\n\\n# 4th Convolutional Layer\\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\'valid\'))\\nmodel.add(Activation(\'relu\'))\\n# Batch Normalisation\\nmodel.add(BatchNormalization())\\n\\n# 5th Convolutional Layer\\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=\'valid\'))\\nmodel.add(Activation(\'relu\'))\\n# Pooling\\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'valid\'))\\n# Batch Normalisation\\nmodel.add(BatchNormalization())\\n\\n# Passing it to a dense layer\\nmodel.add(Flatten())\\n# 1st Dense Layer\\nmodel.add(Dense(4096, input_shape=(224*224*3,)))\\nmodel.add(Activation(\'relu\'))\\n# Add Dropout to prevent overfitting\\nmodel.add(Dropout(0.4))\\n# Batch Normalisation\\nmodel.add(BatchNormalization())\\n\\n# 2nd Dense Layer\\nmodel.add(Dense(4096))\\nmodel.add(Activation(\'relu\'))\\n# Add Dropout\\nmodel.add(Dropout(0.4))\\n# Batch Normalisation\\nmodel.add(BatchNormalization())\\n\\n# 3rd Dense Layer\\nmodel.add(Dense(1000))\\nmodel.add(Activation(\'relu\'))\\n# Add Dropout\\nmodel.add(Dropout(0.4))\\n# Batch Normalisation\\nmodel.add(BatchNormalization())\\n\\n# Output Layer\\nmodel.add(Dense(17))\\nmodel.add(Activation(\'softmax\'))\\n\\nmodel.summary()\\n\\n# (4) Compile\\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\',\\\\\\n metrics=[\'accuracy\'])\\n\\n# (5) Train\\nmodel.fit(x, y, batch_size=64, epochs=1, verbose=1, \\\\\\nvalidation_split=0.2, shuffle=True)\\n```"},{"id":"data-science/deep-learning//building-alexnet-with-tensorflow-and-running-it-with-aws-sagemaker","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/building-alexnet-with-tensorflow-and-running-it-with-aws-sagemaker","source":"@site/data-science/deep-learning/2018-05-07-alexnet-tensorflow.md","title":"Building AlexNet with TensorFlow and Running it with AWS SageMaker","description":"In the last post, we built AlexNet with Keras. This is the second part of AlexNet building. Let\u2019s rewrite the Keras code from the previous post (see Building AlexNet with Keras) with TensorFlow and run it in AWS SageMaker instead of the local machine.","date":"2018-05-07T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Deep Learning","permalink":"/mydatahack-old-blog/data-science/tags/deep-learning"},{"label":"AlexNet","permalink":"/mydatahack-old-blog/data-science/tags/alex-net"},{"label":"Convolutional Neural Networks","permalink":"/mydatahack-old-blog/data-science/tags/convolutional-neural-networks"},{"label":"Image Classification","permalink":"/mydatahack-old-blog/data-science/tags/image-classification"},{"label":"SageMaker","permalink":"/mydatahack-old-blog/data-science/tags/sage-maker"},{"label":"TensorFlow","permalink":"/mydatahack-old-blog/data-science/tags/tensor-flow"}],"readingTime":7.455,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/deep-learning//building-alexnet-with-tensorflow-and-running-it-with-aws-sagemaker","title":"Building AlexNet with TensorFlow and Running it with AWS SageMaker","tags":["Data Science","Deep Learning","AlexNet","Convolutional Neural Networks","Image Classification","SageMaker","TensorFlow"]},"unlisted":false,"prevItem":{"title":"Building AlexNet with Keras","permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/building-alexnet-with-keras"},"nextItem":{"title":"Introduction to Dense Layers for Deep Learning with Keras","permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-keras/"}},"content":"In the last post, we built AlexNet with Keras. This is the second part of AlexNet building. Let\u2019s rewrite the Keras code from the previous post (see Building AlexNet with Keras) with TensorFlow and run it in AWS SageMaker instead of the local machine.\\n\\n\x3c!--truncate--\x3e\\n\\nAlexNet is in fact too heavy for a regular commercial laptop to handle it. It only runs on a small dataset and takes for ages. By using the cloud service like AWS, we can access to much better computers without any hardware investment. AWS already has a series of deep learning specialised instances (P2 Instances). The smallest with one GPU (p2.xlarge) costs 90 cent per hour. It is much faster than CPU machines. You can experiment on computing capacities as you will be charged only by usage hours. If you are thinking about buying a more expensive GPU laptop for deep learning, the cloud services would be a better option.\\n\\nAWS recently released SageMaker, which enables you to develop and deploy deep learning code with no hustle. To run Tensorflow code in SageMaker, all you need is to create a notebook instance (check out the getting started video here).\\n\\n**Importing OxfordFlower17 Data**\\n\\nYou can creates a notebook instance with a chosen EC2 instance with SageMaker. Once the instance is created, you can access to the instance through Jupyter notebook for development and deployment. Many deep learning frameworks are already installed. Once you train the model, you can deploy it into the AWS environment without much hustle. The caveat is that you won\u2019t be able to install or update the preinstalled packages as you do not have access to the underlining instance. If you need to have special environmental requirements, you need to bring it in with a Docker container.\\n\\nIn fact, SageMaker does not have tflearn installed. As in the previous post, we are importing 17 category flower dataset (OxfordFlower17) from tflearn. If you try to import it in SageMaker, it will give you the module not found error.\\n\\nThe strategy I took here is to upload the dataset as numpy array files to S3 and retrieve them in SageMaker.\\n\\n(1) Create the numpy files and Upload to S3\\n\\nI first created npy files and uploaded to S3 bucket where SageMaker has the access policy.\\n\\n```python\\n# (1) Get dataset from s3 bucket\\nimport tflearn.datasets.oxflower17 as oxflower17\\nx, y = oxflower17.load_data(one_hot=True)\\n\\n# (2) save data as .npy files\\nx_path=\'/tmp/oxford_flower_17_x.npy\'\\ny_path=\'/tmp/oxford_flower_17_y.npy\'\\nimport numpy as np\\nnp.save(x_path, x)\\nnp.save(y_path, y)\\n\\n# (3) push file to s3\\nimport boto3\\nimport botocore\\n\\nclient = boto3.client(\'s3\')\\ntarget_bucket = \'sagemaker.mydh\'\\nx_key = \'data/oxford_flower_17_x.npy\'\\ny_key = \'data/oxford_flower_17_y.npy\'\\n\\n# Uploading\\nclient.upload_file(Filename=x_path, Bucket=target_bucket, Key=x_key)\\nprint(\'Completed Uploading {} to {}/{}\'.format(x_path, target_bucket, x_key))\\nclient.upload_file(Filename=y_path, Bucket=target_bucket, Key=y_key)\\nprint(\'Completed Uploading {} to {}/{}\'.format(y_path, target_bucket, y_key))\\n```\\n\\n(2) Import numpy files into the SageMaker instance.\\n\\nYou can get the file from S3 into the Notebook instance and simply load them as numpy objects.\\n\\n```python\\nimport boto3\\nimport botocore\\n\\ndef download_file_with_resource(bucket_name, key, local_path):\\n    s3 = boto3.resource(\'s3\')\\n    s3.Bucket(bucket_name).download_file(key, local_path)\\n    print(\'Downloaded {}\'.format(key))\\n\\nbucket_name = \'sagemaker.mydh\'\\nx_key=\'data/oxford_flower_17_x.npy\'\\ny_key=\'data/oxford_flower_17_y.npy\'\\nx_local_path = \'./data/oxford_flower_17_x.npy\'\\ny_local_path = \'./data/oxford_flower_17_y.npy\'\\n\\n# download_file_with_resource(bucket_name, \'data/check.csv\', \'./data/check.csv\')\\ndownload_file_with_resource(bucket_name, x_key, x_local_path)\\ndownload_file_with_resource(bucket_name, y_key, y_local_path)\\n\\n# (3) Load data\\nx_local_path = \'./data/oxford_flower_17_x.npy\'\\ny_local_path = \'./data/oxford_flower_17_y.npy\'\\nx = np.load(x_local_path)\\ny = np.load(y_local_path)\\nprint(\'Shape of features: \', x.shape, \'Type: \', type(x))\\nprint(\'Shape of classes: \', y.shape, \'Type: \', type(y))\\n```\\n\\nCode\\n\\nStrictly speaking, it is slightly different from the original AlexNet. The code is sequential and has no parallel computing components for simplicity. I am doing batch normalisation before every input and doing dropouts in the Dense layer. The network architecture is the same as the previous post.\\n\\nWith TensorFlow, you really need to be careful about the dimensions. The original dataset is 3-dimentional. After the convolution layers, the dimension is compressed from pooling. So, you need to specify the right dimension (7 x 7 in this case). Otherwise, the code will not run.\\n\\nIn the model, I purposely included the weights and biases with hard-coded values so that it is easy to follow. Apart from the model, the same code used in building Dense Net for Iris works. If you need to understand other part of the codes you should read the previous post (Introduction to Dense Net with TensorFlow).\\n\\n```python\\nimport tensorflow as tf\\ntf.set_random_seed(1000)\\nimport numpy as np\\nnp.random.seed(1000)\\nfrom sklearn.model_selection import train_test_split\\n\\n# (1) Create Training (80%), test (20%) and validation (20%) dataset\\n#     Datasets (x and y) are loaded as numpy object from the previous step\\nx_train, x_test_pre, y_train, y_test_pre = train_test_split(x, y, test_size=0.20, random_state=42)\\nx_test, x_validation, y_test, y_validation = train_test_split(x_test_pre, y_test_pre, test_size=0.1)\\n\\n# Check Shapes\\nprint(\'Shape: x_train={}, y_train={}\'.format(x_train.shape, y_train.shape))\\nprint(\'Shape: x_test={}, y_test={}\'.format(x_test.shape, y_test.shape))\\nprint(\'Shape: x_validation={}, y_validation={}\'.format(x_validation.shape, y_validation.shape))\\n\\n# (2) Define the placeholder tensors\\nx = tf.placeholder(tf.float32, [None, 224, 224, 3])\\ny = tf.placeholder(tf.float32, [None, 17]) # no of flower speces in the dataset\\n\\n# (3) Define Layers\\n# Convolutional Layer with Relu activation\\ndef conv2D(x, W, b, stride_size):\\n    xW = tf.nn.conv2d(x, W, strides=[1, stride_size, stride_size, 1],padding=\'SAME\')\\n    z = tf.nn.bias_add(xW, b)\\n    a = tf.nn.relu(z)\\n    return (a)\\n\\n# Max Pooling Layer\\ndef maxPooling2D(x, kernel_size, stride_size):\\n    return tf.nn.max_pool(x, ksize=[1, kernel_size, kernel_size, 1],\\n                         strides=[1, stride_size, stride_size, 1],padding=\'SAME\')\\n\\n# Dense Layer\\ndef dense(x, W, b):\\n    z = tf.add(tf.matmul(x, W), b)\\n    a = tf.nn.relu(z)\\n    return a\\n\\n# (4) Define AlexNet\\n# Setting some parameters\\nw_init = tf.contrib.layers.xavier_initializer()\\nbatch_size = 8\\nepochs = 1\\nprogress = 40\\nn_classes = 17\\n\\n# Function, x is the input features\\ndef alexNet(img_input):\\n\\n    # 1st Convolutional Layer\\n    w_c1 = tf.get_variable(\'w_c1\', [11, 11, 3, 96], initializer=w_init)\\n    b_c1 = tf.Variable(tf.zeros([96]))\\n    c1 = conv2D(img_input, w_c1, b_c1, stride_size=4)\\n    # Pooling\\n    p1 = maxPooling2D(c1, kernel_size=2, stride_size=2)\\n    # Batch Normalisation\\n    bn1 = tf.contrib.layers.batch_norm(p1)\\n\\n    # 2nd Convolutional layer\\n    w_c2 = tf.get_variable(\'w_c2\', [5, 5, 96, 256], initializer=w_init)\\n    b_c2 = tf.Variable(tf.zeros([256]))\\n    c2 = conv2D(bn1, w_c2, b_c2, stride_size=1)\\n    # Pooling\\n    p2 = maxPooling2D(c2, kernel_size=2, stride_size=2)\\n    # Batch Normalisation\\n    bn2 = tf.contrib.layers.batch_norm(p2)\\n\\n    # 3rd Convolutional Layer\\n    w_c3 = tf.get_variable(\'w_c3\', [3, 3, 256, 384], initializer=w_init)\\n    b_c3 = tf.Variable(tf.zeros([384]))\\n    c3 = conv2D(bn2, w_c3, b_c3, stride_size=1)\\n    # Batch Normalisation\\n    bn3 = tf.contrib.layers.batch_norm(c3)\\n\\n    # 4th Convolutional Layer\\n    w_c4 = tf.get_variable(\'w_c4\', [3, 3, 384, 384], initializer=w_init)\\n    b_c4 = tf.Variable(tf.zeros([384]))\\n    c4 = conv2D(bn3, w_c4, b_c4, stride_size=1)\\n    # Batch Normalisation\\n    bn4 = tf.contrib.layers.batch_norm(c4)\\n\\n    # 5th Convolutional Layer\\n    w_c5 = tf.get_variable(\'w_c5\', [3, 3, 384, 256], initializer=w_init)\\n    b_c5 = tf.Variable(tf.zeros([256]))\\n    c5 = conv2D(bn4, w_c5, b_c5, stride_size=1)\\n    # Pooling\\n    p3 = maxPooling2D(c5, kernel_size=2, stride_size=2)\\n    # Batch Normalisation\\n    bn5 = tf.contrib.layers.batch_norm(p3)\\n\\n    # Flatten the conv layer - features has been reduced by pooling 3 times: 224/2*2*2\\n    flattened = tf.reshape(bn5, [-1, 28*28*256])\\n\\n    # 1st Dense layer\\n    w_d1 = tf.get_variable(\'w_d1\', [28*28*256, 4096], initializer=w_init)\\n    b_d1 = tf.Variable(tf.zeros([4096]))\\n    d1 = dense(flattened, w_d1, b_d1)\\n    # Dropout\\n    dropout_d1 = tf.nn.dropout(d1, 0.6)\\n\\n    # 2nd Dense layer\\n    w_d2 = tf.get_variable(\'w_d2\', [4096, 4096], initializer=w_init)\\n    b_d2 = tf.Variable(tf.zeros([4096]))\\n    d2 = dense(dropout_d1, w_d2, b_d2)\\n    # Dropout\\n    dropout_d2 = tf.nn.dropout(d2, 0.6)\\n\\n    # 3rd Dense layer\\n    w_d3 = tf.get_variable(\'w_d3\', [4096, 1000], initializer=w_init)\\n    b_d3 = tf.Variable(tf.zeros([1000]))\\n    d3 = dense(dropout_d2, w_d3, b_d3)\\n    # Dropout\\n    dropout_d3 = tf.nn.dropout(d3, 0.6)\\n\\n    # Output layer\\n    w_out = tf.get_variable(\'w_out\', [1000, n_classes], initializer=w_init)\\n    b_out = tf.Variable(tf.zeros([n_classes]))\\n    out = tf.add(tf.matmul(dropout_d3, w_out), b_out)\\n\\n    return out\\n\\n# (5) Build model\\npredictions = alexNet(x)\\n\\n# (6) Define model\'s cost and optimizer\\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\\noptimizer = tf.train.AdamOptimizer().minimize(cost)\\n\\n# (7) Defining evaluation metrics\\ncorrect_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\\naccuracy_pct = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\\n\\n# (8) initialize\\ninitializer_op = tf.global_variables_initializer()\\n\\n# (9) Run\\nwith tf.Session() as session:\\n    session.run(initializer_op)\\n\\n    print(\\"Training for\\", epochs, \\"epochs.\\")\\n\\n    # looping over epochs:\\n    for epoch in range(epochs):\\n        # To monitor performance during training\\n        avg_cost = 0.0\\n        avg_acc_pct = 0.0\\n\\n        # loop over all batches of the epoch- 1088 records\\n        # batch_size = 128 is already defined\\n        n_batches = int(1088 / batch_size)\\n        counter = 1\\n        for i in range(n_batches):\\n\\n            # Get the random int for batch\\n            random_indices = np.random.randint(1088, size=batch_size) # 1088 is the no of training set records\\n\\n            feed = {\\n                x: x_train[random_indices],\\n                y: y_train[random_indices]\\n            }\\n\\n            # feed batch data to run optimization and fetching cost and accuracy:\\n            _, batch_cost, batch_acc = session.run([optimizer, cost, accuracy_pct],\\n                                                   feed_dict=feed)\\n            # Print batch cost to see the code is working (optional)\\n            # print(\'Batch no. {}: batch_cost: {}, batch_acc: {}\'.format(counter, batch_cost, batch_acc))\\n            # Get the average cost and accuracy for all batches:\\n            avg_cost += batch_cost / n_batches\\n            avg_acc_pct += batch_acc / n_batches\\n            counter += 1\\n\\n        # Get cost and accuracy after one iteration\\n        test_cost = cost.eval({x: x_test, y: y_test})\\n        test_acc_pct = accuracy_pct.eval({x: x_test, y: y_test})\\n        # output logs at end of each epoch of training:\\n        print(\\"Epoch {}: Training Cost = {:.3f}, Training Acc = {:.2f} -- Test Cost = {:.3f}, Test Acc = {:.2f}\\"\\\\\\n              .format(epoch + 1, avg_cost, avg_acc_pct, test_cost, test_acc_pct))\\n\\n    # Getting Final Test Evaluation\\n    print(\'\\\\n\')\\n    print(\\"Training Completed. Final Evaluation on Test Data Set.\\\\n\\")\\n    test_cost = cost.eval({x: x_test, y: y_test})\\n    test_accy_pct = accuracy_pct.eval({x: x_test, y: y_test})\\n    print(\\"Test Cost:\\", \'{:.3f}\'.format(test_cost))\\n    print(\\"Test Accuracy: \\", \'{:.2f}\'.format(test_accy_pct), \\"%\\", sep=\'\')\\n    print(\'\\\\n\')\\n\\n    # Getting accuracy on Validation set\\n    val_cost = cost.eval({x: x_validation, y: y_validation})\\n    val_acc_pct = accuracy_pct.eval({x: x_validation, y: y_validation})\\n    print(\\"Evaluation on Validation Data Set.\\\\n\\")\\n    print(\\"Evaluation Cost:\\", \'{:.3f}\'.format(val_cost))\\n    print(\\"Evaluation Accuracy: \\", \'{:.2f}\'.format(val_acc_pct), \\"%\\", sep=\'\')\\n```"},{"id":"data-science/deep-learning//introduction-to-dense-net-with-keras/","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-keras/","source":"@site/data-science/deep-learning/2018-05-07-dense-layer-keras.md","title":"Introduction to Dense Layers for Deep Learning with Keras","description":"The most basic neural network architecture in deep learning is the dense neural networks consisting of dense layers (a.k.a. fully-connected layers).","date":"2018-05-07T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Deep Learning","permalink":"/mydatahack-old-blog/data-science/tags/deep-learning"},{"label":"Dense Net","permalink":"/mydatahack-old-blog/data-science/tags/dense-net"},{"label":"Iris","permalink":"/mydatahack-old-blog/data-science/tags/iris"},{"label":"Keras","permalink":"/mydatahack-old-blog/data-science/tags/keras"}],"readingTime":4.355,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/deep-learning//introduction-to-dense-net-with-keras/","title":"Introduction to Dense Layers for Deep Learning with Keras","tags":["Data Science","Deep Learning","Dense Net","Iris","Keras"]},"unlisted":false,"prevItem":{"title":"Building AlexNet with TensorFlow and Running it with AWS SageMaker","permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/building-alexnet-with-tensorflow-and-running-it-with-aws-sagemaker"},"nextItem":{"title":"Introduction to Dense Layers for Deep Learning with TensorFlow","permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-tensorflow"}},"content":"The most basic neural network architecture in deep learning is the dense neural networks consisting of dense layers (a.k.a. fully-connected layers).\\n\\n\x3c!--truncate--\x3e\\n\\nIn this layer, all the inputs and outputs are connected to all the neurons in each layer. Keras is the high-level APIs that runs on TensorFlow (and CNTK or Theano) which makes coding easier. Writing code in the low-level TensorFlow APIs is difficult and time-consuming. When I build a deep learning model, I always start with Keras so that I can quickly experiment with different architectures and parameters. Then, move onto TensorFlow to further fine tune it. When it comes to the first deep learning code, I think Dense Net with Keras is a good place to start. So, let\u2019 get started.\\n\\nDataset\\n\\nDeep learning 101 dataset is the classic MNIST, which is used for hand-written digit recognition. With the code below, you can certainly use MNIST.\\n\\nIn this example, I am using the machine learning classic Iris dataset. The dataset will be imported from a csv file. This gives you an idea on how to import csv into the deep learning model, rather than porting example data from the build-in package.\\n\\nDeep learning on Iris certainly feels like cracking a nut with a sledge hammer. However, you can apply the knowledge and the same code to more appropriate datasets once you understand how it works.\\n\\nThere are many ways to get a csv version of Iris. I got it from R.\\n\\n```r\\npath = \'/tmp/iris.csv\'\\ndata(iris)\\nwrite.csv(iris, path,row.names=FALSE)\\n```\\n\\nSteps\\n\\n(1) Import required modules\\n\\n```python\\nimport numpy as np\\nnp.random.seed(21)\\nimport pandas as pd\\nimport keras\\nfrom keras.models import Sequential\\nfrom keras.layers import Dense, Activation, Dropout\\nfrom keras import regularizers\\nfrom keras.optimizers import SGD\\nfrom sklearn.model_selection import train_test_split\\n```\\n\\n(2) Preprocessing\\n\\nBoth Keras and TensorFlow takes numpy arrays as features and classes. When the prediction is categorical, the outcome needs to be one-hot encoded (see one-hot encoding explanation from the Kaggle\u2019s website). For one-hot encoding, the class needs to be indexes (starting from 0). Once they are transformed, you can use keras.utils.to_categorical() for conversion.\\n\\nIt uses sklearn.model_selection.train_test_split to create training and test dataset.\\n\\n```python\\niris = pd.read_csv(\'/tmp/iris.csv\')\\nprint(iris.head(5))\\nprint(iris.Species.unique())\\niris[\'Species\'] = np.where(iris[\'Species\']==\'setosa\', 0,\\n               np.where(iris[\'Species\']==\'versicolor\', 1,\\n                       np.where(iris[\'Species\']==\'virginica\', 2, 3)))\\nprint(iris[0:3])\\nprint(iris.Species.unique())\\n\\n# convert df to features and targets\\nfeature_cols = [\'Sepal.Length\', \'Sepal.Width\', \'Petal.Length\', \'Petal.Width\']\\nfeatures = iris[feature_cols].values\\ntarget = iris.Species.values\\n\\n# Normalise features\\nfeatures_norm = (features-np.min(features, axis=0)) / \\\\\\n(np.max(features, axis=0)-np.min(features, axis=0))\\nprint(\'Checking normalised features: \\\\n{}\'.format(features_norm[0:3]))\\n\\n# Train Test Split\\nX_train, X_test, y_train, y_test = train_test_split(features_norm, target, test_size=0.2)\\nprint(X_train.shape, y_train.shape)\\nprint(X_test.shape, y_test.shape)\\nprint(X_train[3])\\nprint(y_train[3])\\n\\n# Convert y into one hot encoded variables\\nn_classes = 3\\ny_train_e = keras.utils.to_categorical(y_train, n_classes)\\ny_test_e = keras.utils.to_categorical(y_test, n_classes)\\nprint(y_train_e[0:3])\\n```\\n\\n(3) Design Networks\\n\\nI am using the sequential model with 2 fully-connected layers. ReLU is more popular in many deep neural networks, but I am using Tanh for activation because it actually performed better. You almost never use Sigmoid because it is slow to train. Softmax is used for the output layer.\\n\\nAdding the 3rd layer degrades the performance. This makes sense as the data set is fairly simple. I am using Dropout to reduce over-fitting. L2 regularizer can be used. But, it did not perform well in this case and I commented out the line.\\n\\n```python\\nmodel = Sequential()\\n# for l2 regularizer. this doesn\'t perform as good as dropouts\\n# model.add(Dense(32, activation=\'relu\', input_dim=4, kernel_regularizer=regularizers.l2(0.01)))\\n# tanh is better than relu in this case\\nmodel.add(Dense(32, activation=\'tanh\', input_shape=(4,)))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(32, activation=\'tanh\'))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(3, activation=\'softmax\'))\\n```\\n\\n(4) Model Compilation\\n\\nYou need to define the loss function, optimizer and evaluation metrics. Cross-entropy is the gold standard for the cost function. You will almost never use quadratic. On the other hand, there are many options for optimisers. In this example, I have Adam as well as SGD with learning rate of 0.01. Both works fine.\\n\\n```python\\n# Compared to mean_squared_error, cross entropy does faster learning,\\n# model.compile(loss=\'mean_squared_error\', optimizer=SGD(lr=0.01), metrics=[\'accuracy\'])\\n# We can use SGD with a specific learning rate for optimizer\\n# model.compile(loss=\'categorical_crossentropy\', optimizer=SGD(lr=0.01), metrics=[\'accuracy\'])\\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\\n```\\n\\n(5) Execution\\n\\nThe testing accuracy goes up to 96.7% after 120 epochs. With this dataset, a regular machine learning algorithm like random forest or logistic regression can achieve the similar results. The first rule of deep learning is that if the simpler machine learning algorithm can achieve the same outcome, use machine learning and look for a more complicated problem. Here, the purpose is to learn the actual programming process so that you can apply it to more complex problems.\\n\\n```python\\nepoch_n = 1000\\nmodel.fit(X_train, y_train_e, batch_size=60, epochs=epoch_n, verbose=1,\\\\\\n validation_data=(X_test, y_test_e))\\n```\\n\\nNext Step\\n\\n(1) Try using MNIST dataset on this code.\\n\\nMNIST is included in Keras and you can imported it as keras.datasets.mnist. It\u2019s already split into training and test datasets. In preprocessing, you need to flatten the data (from 28 x 28 to 784) and convert y into one-hot encoded values. Here is the code to process the data.\\n\\n(2) Replicate the same code with low-level TensorFlow code.\\n\\nTenorFlow is much more complicated than Keras. The way to code is quite unique. It will be difficult at first, but it will be worthwhile.\\n\\nFor the actual code example, go to Introduction to Dense Net with TensorFlow."},{"id":"data-science/deep-learning//introduction-to-dense-net-with-tensorflow","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-tensorflow","source":"@site/data-science/deep-learning/2018-05-07-dense-layers-tensorflow.md","title":"Introduction to Dense Layers for Deep Learning with TensorFlow","description":"TensorFlow offers both high- and low-level APIs for Deep Learning. Coding in TensorFlow is slightly different from other machine learning frameworks.","date":"2018-05-07T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Deep Learning","permalink":"/mydatahack-old-blog/data-science/tags/deep-learning"},{"label":"Dense Net","permalink":"/mydatahack-old-blog/data-science/tags/dense-net"},{"label":"Iris","permalink":"/mydatahack-old-blog/data-science/tags/iris"},{"label":"TensorFlow","permalink":"/mydatahack-old-blog/data-science/tags/tensor-flow"}],"readingTime":3.51,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/deep-learning//introduction-to-dense-net-with-tensorflow","title":"Introduction to Dense Layers for Deep Learning with TensorFlow","tags":["Data Science","Deep Learning","Dense Net","Iris","TensorFlow"]},"unlisted":false,"prevItem":{"title":"Introduction to Dense Layers for Deep Learning with Keras","permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-keras/"},"nextItem":{"title":"How To Create Your Own Personal Data Science Computing Environment In AWS","permalink":"/mydatahack-old-blog/data-science/data-science/infra/how-to-create-your-personal-data-science-computing-environment-in-aws"}},"content":"TensorFlow offers both high- and low-level APIs for Deep Learning. Coding in TensorFlow is slightly different from other machine learning frameworks.\\n\\n\x3c!--truncate--\x3e\\n\\nYou first need to define the variables and architectures. This is because the entire code is executed outside of Python with C++ and the python code itself is just a bunch of definitions.\\n\\nThe aim of this post is to replicate the previous Keras code into TensorFlow. Before writing code in TensorFlow, it is better to use high-level APIs like Keras to build the model (read Introduction to Dense Net with Keras for a preparation).\\n\\nSteps\\n\\n(1) Import Modules\\n\\n```python\\nimport tensorflow as tf\\ntf.set_random_seed(42)\\ntf.reset_default_graph()\\nimport pandas as pd\\nimport numpy as np\\nnp.random.seed(42)\\nfrom sklearn.model_selection import train_test_split\\nimport keras\\n```\\n\\n(2) Data Preparation\\n\\nAs in the previous post, we are importing the Iris dataset from a csv file. This step is the same as before.\\n\\nYou can also check the official TensorFlow documents about deep learning on Iris dataset here. The way the dataset is preprocessed is quite different from what I did and will be an interesting read.\\n\\n```python\\niris = pd.read_csv(\'/tmp/iris.csv\')\\nprint(iris.head(5))\\nprint(iris.Species.unique())\\niris[\'Species\'] = np.where(iris[\'Species\']==\'setosa\', 0,\\n               np.where(iris[\'Species\']==\'versicolor\', 1,\\n                       np.where(iris[\'Species\']==\'virginica\', 2, 3)))\\nprint(iris[0:3])\\nprint(iris.Species.unique())\\n\\n# convert df to features and targets\\nfeature_cols = [\'Sepal.Length\', \'Sepal.Width\', \'Petal.Length\', \'Petal.Width\']\\nfeatures = iris[feature_cols].values\\ntarget = iris.Species.values\\n\\n# Train Test Split\\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)\\nprint(X_train.shape, y_train.shape)\\nprint(X_test.shape, y_test.shape)\\n\\nprint(X_train[3])\\nprint(y_train[3])\\n\\n# Convert y into one hot encoded variables\\nn_classes = 3\\ny_train = keras.utils.to_categorical(y_train, n_classes)\\ny_test = keras.utils.to_categorical(y_test, n_classes)\\n\\nprint(y_train[0:3])\\n```\\n\\n(3) Defining Variables and Models\\n\\nBefore running the code, we need to define variables and models. The model is the same as the one defined in the previous post with Keras.\\n\\nEven for more complicated models (e.g. with added convolutional layers), you can use the same steps.\\n\\nSet hyperparameters\\nSet layers\\nDefine placeholders\\nDefine layers\\nDefine architecture\\nDefine variable dictionary\\nBuild Model\\nDefine loss & optimizer\\nDefine evaluation metrics\\nHere is the code from the steps above.\\n\\n```python\\n# (1) Set hyperparameters\\nlr = 0.01\\nepochs = 1000\\nbatch_size = 20\\nweight_initializer = tf.contrib.layers.xavier_initializer()\\n\\n# (2) Set layers\\nn_input = 4\\nn_dense = 10\\nn_classes = 3\\n\\n# (3) Define placeholders\\nx = tf.placeholder(tf.float32, [None, n_input])\\ny = tf.placeholder(tf.float32, [None, n_classes])\\n\\n# (4) Define layers\\ndef dense(x, w, b):\\n    z = tf.add(tf.matmul(x, w), b)\\n    a = tf.nn.relu(z)\\n    return a\\n\\n# (5) Define architecture\\ndef network(x, weights, biases):\\n    dense1 = dense(x, weights[\'w1\'], biases[\'b\'])\\n    out_layer_z = tf.add(tf.matmul(dense1, weights[\'w_out\']), biases[\'b_out\'])\\n    return out_layer_z\\n\\n# (6) Define variable dictionary\\nbias_dict = {\\n    \'b\': tf.Variable(tf.zeros([n_dense])),\\n    \'b_out\': tf.Variable(tf.zeros([n_classes]))\\n}\\n\\nweight_dict = {\\n    \'w1\': tf.get_variable(\'w1\', [n_input, n_dense], initializer = weight_initializer),\\n    \'w_out\': tf.get_variable(\'w_out\', [n_dense, n_classes], initializer = weight_initializer)\\n}\\n\\n# (7) Build Model\\npredictions = network(x, weights=weight_dict, biases=bias_dict)\\nprint(predictions)\\n\\n# (8) Define loss & optimizer\\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(cost)\\n\\n# (9) Define evaluation metrics\\ncorrect_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\\naccuracy_pct = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\\n```\\n\\n(4) Initialise and Run\\n\\nOnce everything is set up, initialise variables and execute the code in session! After 1000 epochs, you will see the test accuracy of 96%.\\n\\n```python\\n# (10) Initialization\\ninitializer_op = tf.global_variables_initializer()\\n\\n# (11) Train the network in a session\\nwith tf.Session() as session:\\n    session.run(initializer_op)\\n\\n    print(\\"Training for\\", epochs, \\"epochs.\\")\\n\\n    # Go through epochs\\n    for epoch in range(epochs):\\n        # monitoring each epochs on training cost and accuracy\\n        avg_cost = 0.0\\n        avg_acc = 0.0\\n\\n        # loop over all batches of the epoch:\\n        n_batches = int(120 / batch_size)\\n        for i in range(n_batches):\\n\\n            # Get the random int for batch\\n            random_indices = np.random.randint(120, size=batch_size) # 120 is the no of training set records\\n\\n            feed = {\\n                x: X_train[random_indices],\\n                y: y_train[random_indices]\\n            }\\n\\n            # feed batch data to run optimization and fetching cost and accuracy:\\n            _, batch_cost, batch_acc = session.run([optimizer, cost, accuracy_pct],\\n                                                   feed_dict=feed)\\n\\n            # accumulate mean loss and accuracy over epoch:\\n            avg_cost += batch_cost / n_batches\\n            avg_acc += batch_acc / n_batches\\n\\n        # Training cost and accuracy at end of each epoch of training:\\n        print(\\"Epoch \\", \'%03d\' % (epoch+1),\\n              \\": cost = \\", \'{:.3f}\'.format(avg_cost),\\n              \\", accuracy = \\", \'{:.2f}\'.format(avg_acc), \\"%\\",\\n              sep=\'\')\\n\\n    print(\\"Training Complete. Testing Model.\\\\n\\")\\n\\n    test_cost = cost.eval({x: X_test, y: y_test})\\n    test_acc = accuracy_pct.eval({x: X_test, y: y_test})\\n\\n    print(\\"Test Cost:\\", \'{:.3f}\'.format(test_cost))\\n    print(\\"Test Accuracy: \\", \'{:.2f}\'.format(test_acc), \\"%\\", sep=\'\')\\n```"},{"id":"data-science/infra//how-to-create-your-personal-data-science-computing-environment-in-aws","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/infra/how-to-create-your-personal-data-science-computing-environment-in-aws","source":"@site/data-science/infra/2018-01-27-personal-aws-env.md","title":"How To Create Your Own Personal Data Science Computing Environment In AWS","description":"Running a training algorithm is such a time-consuming task when you are building a machine learning application. If you are developing it with your computer, you cannot do anything else for a long period of time (hours and maybe days) on that machine. Especially when we do parallel processing using all the CPU cores, your CPU will be peaking at 100%.","date":"2018-01-27T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Tools and Infrastructure","permalink":"/mydatahack-old-blog/data-science/tags/tools-and-infrastructure"},{"label":"AWS","permalink":"/mydatahack-old-blog/data-science/tags/aws"},{"label":"Cloud Computing","permalink":"/mydatahack-old-blog/data-science/tags/cloud-computing"},{"label":"Data Science Development","permalink":"/mydatahack-old-blog/data-science/tags/data-science-development"},{"label":"EC2","permalink":"/mydatahack-old-blog/data-science/tags/ec-2"},{"label":"Infrastructure","permalink":"/mydatahack-old-blog/data-science/tags/infrastructure"},{"label":"RDS","permalink":"/mydatahack-old-blog/data-science/tags/rds"}],"readingTime":6.575,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/infra//how-to-create-your-personal-data-science-computing-environment-in-aws","title":"How To Create Your Own Personal Data Science Computing Environment In AWS","tags":["Data Science","Tools and Infrastructure","AWS","Cloud Computing","Data Science Development","EC2","Infrastructure","RDS"]},"unlisted":false,"prevItem":{"title":"Introduction to Dense Layers for Deep Learning with TensorFlow","permalink":"/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-tensorflow"},"nextItem":{"title":"Predict Internet Popularity By Optimising Neural Networks With Python","permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python"}},"content":"Running a training algorithm is such a time-consuming task when you are building a machine learning application.\x3c!-- truncate --\x3e If you are developing it with your computer, you cannot do anything else for a long period of time (hours and maybe days) on that machine. Especially when we do parallel processing using all the CPU cores, your CPU will be peaking at 100%.\\n\\nSo, here is what we can do. We create our personal data science computing environments in the cloud and do training there. We can simply let the script run as long as it needs. Running it in the cloud computer has lesser chance of interruptions. In the cloud, you can quickly provision resources and terminate them as you require without the need of managing physical devices. It gives you flexibility. You can start the script before going to bed and it will be finished by the time you come back from work on the next day. You can also set up your own database to get data and write the result back to a table. Sounds good, doesn\u2019t it?\\n\\nAmazone Web Services (AWS) offers easy to use cloud computing services. The computing and database resources are charged by hour. AWS is a mature cloud computing service provider and you can get all the benefits of cloud computing. When you sign up, you have access to heaps of free services for the first 12 months (including Linux and database services) as part of free tier.\\n\\nBefore going into this post, you should need basic knowledge of AWS. There are heaps of AWS courses out there. I recommend AWS Cloud Practitioner Essentials, which is the official training material from AWS. It will give you the enough foundation knowledge to get it started.\\n\\nArchitecture\\n\\nWe are going to using Linux EC2 instance for program execution and Postgres RDS for Database. We install Anaconda in EC2 instance. EC2 can read and write to Postgres. If your application produces images, you can write it to S3 (not the scope of this post, but it is very easy to do).\\n\\nWe are going to have two subnets across two availability zones. Our local machine needs to have access to both EC2 and Postgres RDS instances.\\n\\n![AWS DS Env](./img/aws-ds-env.png)\\n\\n**Steps**\\n\\nThis is going to be an epic!\\n\\n(1) Create AWS account\\n\\nFirst of all, you need to create an AWS account (if you don\u2019t have it already). AWS offers a free tier service for 12 months so that you can experiment and gain some practical experience.\\n\\n(2) Create Admin User\\n\\nThe best practice is not to use root account credential (the user who created the account). Use IAM to create an admin group attached with the AdministratorAccess policy. Then, create a user attached to the group.\\n\\nTo understand IAM identities and how to create admin user, go to How To Create Admin User In AWS.\\n\\nLog back into AWS management console with the admin user credential.\\n\\n(3) Create and Configure VPC and Subnets\\n\\nAccording to the plan, you need to create and configure VPC and Subnets. Then, attach Internet Gateway to the VPC.\\n\\nFor detailed steps, refer to this blog entry: How To Create and Configure VPC and Subnets In AWS.\\n\\n(4) Create Network ACLs for both subnets and Security Groups for EC2 and RDS instances\\n\\nGo to VPC Dashoard and create Network ACLs for subnets and Security Groups for instances according to the plan. Problems with connecting to resources are usually resolved by fixing NACLs or Security Group.\\n\\nI have the detailed set up examples for this use case: How To Configure Network Access Control Lists (NACLs) and Security Groups in AWS.\\n\\n(5) Launch Linux EC2 Instance In Subnet A\\n\\nYou need to launch the instance with correct role in to the correct subnet. Create Elastic IP and attach it to the instance. Once you have the instance, make sure you can SSH to it. Here is the detailed steps: How To Launch an EC2 Instance From AMI in AWS.\\n\\n(6) Attach an EBC volume to EC2 Instance\\n\\nThis step is optional, but fun. You can get 30GB of free EBC for the first 12 months. So, why not? Here is the detailed steps: How To Attach EBS Volume to EC2 Linux Instance In AWS.\\n\\n(7) Launch Postgres RDS instance in Subnet B.\\n\\nWhen you launch an RDS instance, you cannot choose a subnet. Instead, you have to choose Availability Zone. According to our diagram, Subnet B sits in AZ2. So, you have to launch it into AZ2. Then, Subnet B becomes where the database sits. Using subnet group doesn\u2019t really work for this use case.\\n\\nApart from that, launching RDS is very easy. Here is the detailed steps: How To Launch a RDS Instance In a Specific Subnet.\\n\\n(8) Install Anaconda to EC2 instance\\n\\nWe are almost there. Let\u2019s install Anaconda to EC2 instance. Linux has Python 2.7 pre-installed. However, upgrading their Python version is not a good idea. Linux has some dependency on Python and it may break it.\\n\\nInstead, we create a special folder /anaconda/ and install it there. When we call python, we create a variable called $python3 with the path to Python 3 in Anaconda and use it.\\n\\nYou can obtain the installation path from Anaconda website here. The example url below will be quickly outdated.\\n\\n```bash\\nsudo mkdir anaconda\\ncd anaconda\\nsudo wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh\\nsudo bash Anaconda3-5.0.1-Linux-x86_64.sh\\n```\\n\\nMake sure to change the default installation path to /anaconda/anaconda3/. Then, export a variable python3 as below. Type $python3 to see if it works.\\n\\n```bash\\nexport python3=/anaconda/anaconda3/bin/python\\n$python3\\n```\\n\\n(9) Install psycopg2\\n\\nMake sure to use the pip path for the anaconda. If you just use pip, it will install it to OS\u2019s Python. pycopg2 enables Python to connect to Postgres.\\n\\n```bash\\nsudo /anaconda/anaconda3/bin/pip install --upgrade pip\\nsudo /anaconda/anaconda3/bin/pip install psycopg2\\n```\\n\\n(10) Test to see if the EC2 can connect to the Postgres RDS instance with the script below.\\n\\nTest to see if the EC2 can connect to the Postgres RDS instance with the script below.\\n\\nI usually copy and paste the script directly into vi editor. Make sure to run the script with $python3 to use the correct Python. You also need to chmod to excute the script.\\n\\n```python\\nimport psycopg2\\ndbname=\'\'\\nuser=\'\'\\nhost=\'\'\\npassword=\'\'\\n\\nconn = psycopg2.connect(\\"dbname={} user={} host={} password={}\\".format(dbname, user, host, password))\\nprint(conn)\\ncur = conn.cursor()\\ncur.execute(\'Select NOW();\')\\nrecord = cur.fetchall()\\nprint(record)\\n```\\n\\nNow you have your own AWS environments to run a heavy training algorithm or do whatever you want. Once you finish computing, you can stop instances. While they are not running, you won\u2019t be charged. When you need them again, you just restart them.\\n\\nEpic!\\n\\nNext Frontier\\n\\nInfrastracture as Code\\n\\nI think the most important philosophy of AWS (or any cloud computing platform) is Infrastracture as Code. The entire infrastructure and resources can be coded. Bringing up environments becomes running code. It sounds cool, right? This is the next frontier you should explore.\\n\\nResources like EC2 and RDS can be a piece of code. You can even install software and configure it while launching them. In this example, we launched EC2 and installed Anaconda with psycopg2. The whole step can be coded and repeated again and again by running the script (called Bootstrap). You can check out how it can be done here: How To Launch EC2 With Bootstrapping in AWS.\\n\\nThe same goes with RDS. You can launch it with a piece of code. Check it out how to do it here: How To Launch Postgres RDS With AWS Command Line Interface (CLI).\\n\\nYou can use a tool like CloudFormation to code up the entire infrastructure including VPC, subnet, routing and securities. Learning how to bootstrap EC2 or RDS is the first step toward coding the entire AWS environment. It is also very satisfying to create and terminate resources with a piece of code.\\n\\nGood Times!"},{"id":"data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python","source":"@site/data-science/machine-learning/2018-01-12-internet-popularity-python.md","title":"Predict Internet Popularity By Optimising Neural Networks With Python","description":"In the previous post, we used grid search to find the best hyper parameter for the neural network model with R\u2019s caret package. Here, let\u2019s use Python and scikit-learn package to optimise a neural network model.","date":"2018-01-12T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Machine Learning","permalink":"/mydatahack-old-blog/data-science/tags/machine-learning"},{"label":"caret","permalink":"/mydatahack-old-blog/data-science/tags/caret"},{"label":"Hyper Parameter Search","permalink":"/mydatahack-old-blog/data-science/tags/hyper-parameter-search"},{"label":"Neural Networks","permalink":"/mydatahack-old-blog/data-science/tags/neural-networks"},{"label":"nnet","permalink":"/mydatahack-old-blog/data-science/tags/nnet"},{"label":"Python","permalink":"/mydatahack-old-blog/data-science/tags/python"}],"readingTime":7.225,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python","title":"Predict Internet Popularity By Optimising Neural Networks With Python","tags":["Data Science","Machine Learning","caret","Hyper Parameter Search","Neural Networks","nnet","Python"]},"unlisted":false,"prevItem":{"title":"How To Create Your Own Personal Data Science Computing Environment In AWS","permalink":"/mydatahack-old-blog/data-science/data-science/infra/how-to-create-your-personal-data-science-computing-environment-in-aws"},"nextItem":{"title":"Predict Internet Popularity By Optimising Neural Networks With R","permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r"}},"content":"In the previous post, we used grid search to find the best hyper parameter for the neural network model with R\u2019s caret package. Here, let\u2019s use Python and scikit-learn package to optimise a neural network model.\\n\\nJust like the caret package, scikit-learn has a pre-built function for hyper parameter search. As for dataset, we will use Online News Popularity Data Set from the UCI Machine Learning repository, which is the same dataset used in the previous post. It was originally used in this publication.\\n\\nThe nnet package from previous post is a single hidden layer back-propagation network. Therefore, there is only one size parameter for the layer to tune. Here, we will use MLPClassifier that implements a multi-layer perceptron algorithm. Having a multiple hidden layers enables us to specify the number of layers as well as the number of neurons for each layer.\\n\\nAs for parameter optimisation, we will use GridSearchCV with 10-fold cross validation. Optimising multi-layer neural networks can be lengthy because you can try different layer numbers and neuron size in each layer. Here, let\u2019s use 2 layers and try to optimise the neuron size per layer for simplicity. Generally speaking, stating small works fine with neural networks in terms of both layer and neuron sizes.\\n\\nWe will also search for the best alpha parameter for L2 regularisation (you can read more about regularisation here). In short, regularisation prevents over-fitting by penalising it. With no regularisation term, you will get a great accuracy on training set, but not so great on test. Neural networks are often called as a black box method, which makes the method fancy and magical. In realisty, they are a variation of nonlinear statistical models. So, regularisation terms work just like logistic regression. Scikit-learn has a great documentation on MPL here for more detail.\\n\\nAs in the previous post, we are dealing with the prediction of binary classifier (\u2018popular\u2019 or \u2018unpopular\u2019) based on the attributes comes with online news paper articles (see details here).\\n\\nOK, let\u2019s code!\\n\\nSummary Steps\\n\\nGet data and prep it (by selecting the right columns, splitting them to training and test and normalising the data).\\nDo hyperparameter search.\\nModel & predict with the best hyper parameter.\\nCalculate performance metrics and draw ROC curve.\\nCode\\n\\n(1) First of all, we have to load the data (after downloading data from here) and do data prep. I normalised it after splitting into training and test set, but you can do it prior to the split. Note that you have to use the scaler function fitted with training data for the test data set to keep consistency if you are doing this way.\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# (1) Get data\\nshares = pd.read_csv(\\"/tmp/OnlineNewsPopularity.csv\\")\\n\\n# (2) Check data\\nshares.head(5)\\nshares.shape\\n\\n# (3) Prepare data (train_test split and normalise)\\nX = shares.iloc[:,2:59]\\ny = shares.iloc[:,60]\\n\\ny = np.where(y >= 1400, \'Popular\', \'Unpopular\')\\n\\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=42)\\n\\nscaler = StandardScaler()\\n# Fit only to the training data\\nscaler.fit(train_x)\\n# Now apply the transformations to the data:\\ntrain_x = scaler.transform(train_x)\\ntest_x = scaler.transform(test_x)\\n```\\n\\n(2) I created a grid_search function so that I can reuse it for other models. It takes a scikit-learn model, feature matrix, label array and parameter grids and return the best parameters.\\n\\n```python\\nfrom sklearn.model_selection import GridSearchCV\\n\\ndef grid_search(estimator, X, y, param_grid):\\n    \'\'\'Takes a sklearn model, feature matrix, label array and parameter grid dictionary.\\n    Returns the dictionary of best parameters\'\'\'\\n    clf = GridSearchCV(estimator, param_grid = param_grid, n_jobs=-1, cv=10, scoring=\'f1_weighted\')\\n    clf.fit(X, y)\\n    print(\'Best F1 Score is: {}\'.format(clf.best_score_))\\n    print(\'Best Parameter is: {}\'.format(clf.best_params_))\\n    return clf.best_params_\\n```\\n\\n(3) See the range of the alpha and neuron size for each layer that I choose. For some reason, neuron size (5, 2) appear in many places (including the official documentation). So, I chose my parameter grid range around them. The bigger the neuron size is, the more complex the models become. Generally speaking, a smaller neuron size works for most of the time. But, of course, you have to experiment it a lot to find the best size. You can read this post for more details. It is a good starting point to know about layer and neuron size.\\n\\nI picked lbfgs for a solver. You can try to use different ones. Again, you have to experiment it to see which one works best for your use case.\\n\\nAs I did with R code here, you can do parallel computing by adding the n_jobs=-1. As the grid search is an iterative CPU intense process, it is always faster to do parallel.\\n\\n```python\\nfrom sklearn.neural_network import MLPClassifier\\n\\nalpha = [1e-5,3e-5,1e-4,3e-4,1e-3,3e-3, 1e-2,3e-2]\\nhidden_layer_sizes = [(3,1), (5,2), (9,4)]\\nparam_grid = {\'alpha\':alpha, \'hidden_layer_sizes\':hidden_layer_sizes}\\nestimator = MLPClassifier(solver=\'lbfgs\',random_state=1)\\nbest_param = grid_search(estimator, train_x, train_y, param_grid)\\n```\\n\\nBest F1 Score is: `0.6631095339771439`\\nBest Parameter is: `{\u2018alpha\u2019: 3e-05, \u2018hidden_layer_sizes\u2019: (5, 2)}`\\n\\n(4) Now, we got the best hyperparameter set, let\u2019s model the neural net and do prediction.\\n\\n```python\\nclf = MLPClassifier(solver=\'lbfgs\', alpha=best_param[\'alpha\'],\\\\\\n        hidden_layer_sizes=best_param[\'hidden_layer_sizes\'], random_state=1)\\nclf.fit(train_x, train_y)\\npred_train = clf.predict(train_x)\\npred_test = clf.predict(test_x)\\n\\npred_train_prob = clf.predict_proba(train_x)[:, 1]\\npred_test_prob = clf.predict_proba(test_x)[:, 1]\\n```\\n\\n(5) I created two functions to get all the evaluation metrics. The calculate_auc function also produces ROC. I used pandas magic to create a data frame for the easy summary of performance metrics. These metrics are the same one use in the publication.\\n\\n```python\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import precision_score\\nfrom sklearn.metrics import recall_score\\nfrom sklearn.metrics import f1_score\\nfrom sklearn.metrics import auc, roc_curve\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\n\\ndef evaluation(train_y, pred_train, test_y, pred_test, model_name):\\n\\n    train_acc = accuracy_score(train_y, pred_train)\\n    test_acc = accuracy_score(test_y, pred_test)\\n\\n    train_precision = precision_score(train_y, pred_train, pos_label=\'Unpopular\')\\n    test_precision = precision_score(test_y, pred_test, pos_label=\'Unpopular\')\\n\\n    train_recall = recall_score(train_y, pred_train, pos_label=\'Unpopular\')\\n    test_recall = recall_score(test_y, pred_test, pos_label=\'Unpopular\')\\n\\n    train_f1 = f1_score(train_y, pred_train, pos_label=\'Unpopular\')\\n    test_f1 = f1_score(test_y, pred_test, pos_label=\'Unpopular\')\\n\\n    train_df = pd.DataFrame({\'Accuracy\':[train_acc], \'Precision\':[train_precision],\\\\\\n                             \'Recall\':[train_recall], \'F1\':[train_f1]})\\n    test_df = pd.DataFrame({\'Accuracy\':[test_acc], \'Precision\':[test_precision],\\\\\\n                             \'Recall\':[test_recall], \'F1\':[test_f1]})\\n    return train_df, test_df\\n\\ntraindf1, testdf1 = evaluation(train_y, pred_train, test_y, pred_test, \'Neural Network\')\\n\\ndef calculate_auc(train_y, pred_train_prob, test_y, pred_test_prob, model_name):\\n    fpr_train, tpr_train, thresholds_train = roc_curve(train_y, pred_train_prob, pos_label=\'Unpopular\')\\n    fpr_test, tpr_test, thresholds_test = roc_curve(test_y, pred_test_prob, pos_label=\'Unpopular\')\\n    train_auc = auc(fpr_train, tpr_train)\\n    test_auc = auc(fpr_test, tpr_test)\\n\\n    # Draw ROC curve\\n    plt.figure()\\n    lw = 2\\n    plt.plot(fpr_test, tpr_test, color=\'darkorange\',lw=lw, label=\'ROC curve (area = %0.2f)\' % test_auc)\\n    plt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\\n    plt.xlim([0.0, 1.0])\\n    plt.ylim([0.0, 1.05])\\n    plt.xlabel(\'False Positive Rate\')\\n    plt.ylabel(\'True Positive Rate\')\\n    plt.title(\'Test Data ROC Curve With Neural Network\')\\n    plt.legend(loc=\\"lower right\\")\\n    plt.show()\\n\\n    train_df = pd.DataFrame({\'AUC\':[train_auc]})\\n    test_df = pd.DataFrame({\'AUC\':[test_auc]})\\n    return train_df, test_df\\n\\ntraindf2, testdf2 = calculate_auc(train_y, pred_train_prob, test_y, pred_test_prob, \'Neural Network\')\\n\\ntrain_df = pd.concat([traindf1, traindf2], axis=1)\\ntest_df = pd.concat([testdf1, testdf2], axis=1)\\n\\nprint(\\"Training Data Performance Metrics\\\\n\\")\\nprint(round(train_df, 2))\\nprint(\\"\\\\nTest Data Performance Metrics\\\\n\\")\\nprint(round(test_df, 2))\\n```\\n\\nHere is the output.\\n\\n![ROC Curve](./img/roc-for-neural-net-with-python.webp)\\n\\nComparing it With Default MLP Classifier Model\\n\\nIf you don\u2019t set any parameter, the default alpha is 0.0001 and hidden_layer_sizes is 100 neurons in a single layer. The default model is over-fitting, which happens a lot for neural networks. You can see the power of simple parameter tuning!\\n\\n```python\\nclf = MLPClassifier(solver=\'lbfgs\', random_state=1) # default alpha=1e-5\\nclf.fit(train_x, train_y)\\npred_train = clf.predict(train_x)\\npred_test = clf.predict(test_x)\\ntrain_acc = accuracy_score(train_y, pred_train)\\ntest_acc = accuracy_score(test_y, pred_test)\\nprint(\'Neural Network Model Train Accuracy: {}\'.format(train_acc))\\nprint(\'Neural Network Model Test Accuracy: {}\'.format(test_acc))\\n```\\n\\nNeural Network Model Train Accuracy: 0.77\\nNeural Network Model Test Accuracy: 0.62\\n\\nLet\u2019s Benchmark!\\n\\nBelow is the performance metric table from the paper (Fernandes et al. 2015). Our neural net model was comparable to the second best method (AdaBoost) in terms of accuracy and AUC. The model is still over-fitting slightly and has lower recall. So, I think there are more room for optimisation. All in all, pretty good outcome considering how simple the whole modelling process was.\\n\\n![Benchmark](./img/performance-benchmark-python.webp)\\n\\nYour Turn\\n\\nIn the publication, they also used grid search to find the best hyper parameter set in a few different methods. In the paper, SCV was optimised with C \u2208 {2^0, 2^1, 2^2, 2^3, 2^4, 2^5, 2^6} , RF and AdaBoost was with number of trees \u2208 {10, 20, 50, 100, 200, 400} and KNN was number of neighbors \u2208 {1, 3, 5, 10, 20}. Now that we have a custom grid_search function, you can try finding the best hyper parameters for all these methods in a streamlined manner.\\n\\n```python\\nfrom sklearn.svm import SVC\\nsvc_model = SVC(kernel=\'rbf\', random_state=23)\\nsvc_param = {\'C\':[2**0, 2**1, 2**2, 2**3, 2**4, 2**5, 2**6]}\\nxx, yy, zz = grid_search(svc_model, train_x, train_y, svc_param)\\n\\nfrom sklearn.ensemble import RandomForestClassifier\\n# n_estimator is the number of trees\\nrf_param = {\'n_estimators\':[10, 20, 50, 100, 200, 400]}\\nrf = RandomForestClassifier(n_jobs=-1, random_state=42)\\nxx, yy, zz = grid_search(rf, train_x, train_y, rf_param)\\n\\nfrom sklearn.neighbors import KNeighborsClassifier\\nknn_param = {\'n_neighbors\':[10, 20, 50, 100, 200, 400]}\\nknn = KNeighborsClassifier(n_jobs=-1)\\nxx, yy, zz = grid_search(knn, train_x, train_y, knn_param)\\n```"},{"id":"data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r","source":"@site/data-science/machine-learning/2018-01-05-internet-popularity-r.md","title":"Predict Internet Popularity By Optimising Neural Networks With R","description":"Writing code to do machine learning is easy. What makes it difficult is the optimisation. By large, there are two ways to optimise your model.","date":"2018-01-05T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Machine Learning","permalink":"/mydatahack-old-blog/data-science/tags/machine-learning"},{"label":"caret","permalink":"/mydatahack-old-blog/data-science/tags/caret"},{"label":"Hyper Parameter Search","permalink":"/mydatahack-old-blog/data-science/tags/hyper-parameter-search"},{"label":"Neural Networks","permalink":"/mydatahack-old-blog/data-science/tags/neural-networks"},{"label":"nnet","permalink":"/mydatahack-old-blog/data-science/tags/nnet"},{"label":"R","permalink":"/mydatahack-old-blog/data-science/tags/r"}],"readingTime":9.83,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r","title":"Predict Internet Popularity By Optimising Neural Networks With R","tags":["Data Science","Machine Learning","caret","Hyper Parameter Search","Neural Networks","nnet","R"]},"unlisted":false,"prevItem":{"title":"Predict Internet Popularity By Optimising Neural Networks With Python","permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python"},"nextItem":{"title":"How To Save Machine Learning Models In R","permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/how-to-save-machine-learning-models-in-r"}},"content":"Writing code to do machine learning is easy. What makes it difficult is the optimisation. By large, there are two ways to optimise your model.\x3c!--truncate --\x3e\\n\\n- Feature selection & transformation\\n- Model parameter optimisation\\n\\nBoth are hard-core topics and neither can be covered in this post. Feature selection and transformation often require subject matter expertise on top of statistical computation.\\n\\nIn this blog, I want to focus on parameter optimisation by using the power of the caret package. The caret package makes it easy to do hyper parameter search. The model parameter optimisation is often called hyperparameter optimisation. We search and optimise over all possible parameter values for the model and pick the ones perform the best (the best hyperparameter set).\\n\\nHere are some of the commonly used for hyperparameter search strategies.\\n\\nGrid Search (Not as efficient, but easy)\\nRandom Search (Efficient in certain scenarios)\\nBayesian Optimisation (Efficient in general)\\nThe default search method for caret is grid search which will be used here. Another question is which parameters to search. This depends on the method or package you use.\\n\\n### Neural Networks Hyperparameter Search\\n\\nNeural networks has many different flavours. I chose the nnet package because it uses the often-called vanilla neural net (a.k.a the single layer back-propagation network or single layer perceptron) and is simpler to tune hyperparameters compared to other more complicated neural networks. In this method, you don\u2019t need to determine the number of layers as it has only one. What you need to do is to choose the size of the neurons in the hidden layer.\\n\\nThe bigger neuron size you use, the more complicated the model become. Therefore, more neurons tend to become over-fitting. Of course, this is dependent on the dataset. It is only determined by experimentation. However, smaller size is a good rule of thumb.\\n\\nNeural networks sound complicated and magical. But, if you deconstruct them, they are basically non-linear statistical model. Hence, we should try to optimise the regularisation parameter. `nnet` has a parameter called decay for regularisation. In short, regularisation prevent over-fitting by penalising over-fitted training models.\\n\\nWe will set hyperparameter ranges for size and decay for `nnet` optimisation.\\n\\n**Sampling**\\n\\nAnother thing you have to consider is sampling. Again, there are a few different ways like Random Sub Sampling, Stratified Sampling or Cross Validation. In this example, I will use the classic k-fold cross validation method (where k = 10). A sampling method and parameters can be specified in the trControl parameter in the train function in caret.\\n\\nK-fold Cross Validation split the training data into k pieces. Then, it uses k minus 1 folds to estimate a model and test it on the remaining 1 fold (called validation set) for each candidate parameter values. With grid search, it will calculate the average accuracy per parameter set to produce the best set.\\n\\nThe drawback of grid search is that you are assuming a possible range for the hyperparameter set. Therefore, you cannot reach it when it exists outside of your pre-defined set. But, it\u2019s a good place to start.\\n\\n**Dataset**\\n\\nI am using the data set Online News Popularity Data Set from the UCI Machine Learning repository. The dataset was used in the publication here. Each row is an online news article containing a bunch of attributes as well as the numbers of social media shares. There are 58 features and what you predict is a binary classifier of the article, \u2018popular\u2019 or \u2018unpopular\u2019. The classification category was made by setting the threshold of 1400 shares. This number was chosen in the paper because it produced a balanced popular/unpopular class distribution (Fernandes et al. 2015). The goal is to predict if the article is popular or not by using the available features.\\n\\nIn the paper, they used Python\u2019s `scikit-learn` to do Random Forest, Adaptive Boosting, Support Vector Machine, K_Nearest Neighbour and Na\xefve Bayes. We can use their results as a benchmark to test our neural networks model performance.\\n\\n**Code**\\n\\n(1) Install and import required libraries.\\n\\n```R\\nlibrary(caret)\\nlibrary(nnet)\\nlibrary(e1071)\\nlibrary(NeuralNetTools)\\nlibrary(doParallel)\\nlibrary(ROCR)\\n```\\n\\n(2) Download csv file here. Load the data and create a target field which contains a binary classifier(popular and unpopular). I used 1400 as the threshold as in Fernandes et al. 2015. I removed non-predictors according to the article.\\n\\n```R\\nshares <- read.csv(\\"/tmp/OnlineNewsPopularity.csv\\")\\nshares$Popularity <- as.factor(ifelse(shares$shares >= 1400, \\"popular\\", \\"unpopular\\"))\\nshares2 <- shares[, -c(1, 2, 61)\\n```\\n\\n(3) Split the data set into training and testing with seed for repeatability.\\n\\n```R\\nset.seed(1000)\\ninTrain <- createDataPartition(shares2$Popularity, p=0.7, list=F)\\ntrain <- shares2[inTrain, ]\\ntest <- shares2[-inTrain, ]]\\n```\\n\\n(4) Now, it\u2019s time for training and predicting! You can set the sampling methods with the trainControl function and pass it as the trControl parameter in the train function. I defined the tuneGrid as 3 size and 3 decay options. The caret package will conduct the accuracy search to find the best combo. The final values used for the model were size = 1 and decay = 0.1 (you can see this by typing model_nnet in the console).\\n\\nThe caret package supports parallel processing. R by default uses only a single UPC. Parallel processing will enable you to do computation with all the CPUs in your computer. It is easy to set it up. For Windows, you need to use the doParallel package. Once you register the cores with the registerDoParallel function, caret will execute it automatically.\\n\\nYou can visualise the model by the plotnet function from NeuralNetTools.\\n\\n```R\\nset.seed(1000)\\n# Set up Parallel Computing\\ncl <- makeCluster(4)\\nregisterDoParallel(cl)\\n\\n# Train with caret\\nnumFolds <- trainControl(method = \'cv\', number = 10, classProbs = TRUE, verboseIter = TRUE,\\n                         preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))\\n\\nmodel_nnet<- train(Popularity~., data=train, method=\'nnet\', trace=F,\\n                   preProcess = c(\'center\', \'scale\'), trControl = numFolds,\\n                   tuneGrid=expand.grid(.size=c(1,5,10), .decay=c(0,0.001,0.1)))\\n\\n# Plot model and predict\\nplotnet(model_nnet)\\ntrain_pred <- predict(model_nnet, newdata=train, type=\\"raw\\")\\ntest_pred <- predict(model_nnet, newdata=test, type=\\"raw\\")\\nHere are some notes for errors and warnings when you are training the model.\\n\\nThe linout parameter cannot be True. You will get the error message, \u2018entropy fit only for logistic units\u2019. The default value for linout is FALSE.\\n```\\n\\nIn trainControl(), summaryFunction=twoClassSummary will result in a warning: The metric \u201cAccuracy\u201d was not in the result set. ROC will be used instead.\u201d This parameter can be omitted. It will use Accuracy by default for categorical prediction.\\n\\n(5) In R, you can use confusionMatrix() to evaluate the model. It will give you the performance metrics such as accuracy, balanced accuracy, sensitivity, specificity and so on. I created a custom function for evaluation because accuracy, precision, recall, F1 and AUC were used in the paper and I wanted to get the same metrics for benchmarking.\\n\\n```R\\nevaluate <- function(pred, actual){\\n  tab <- table(pred, actual$Popularity)\\n  tp <- tab[1,1]\\n  tn <- tab[2,2]\\n  fp <- tab[2,1]\\n  fn <- tab[1,2]\\n  accuracy <- (tp + tn)/(tp + tn + fp + fn)\\n  precision <- tp/(tp+fp)\\n  recall <- tp/(tp + fn)\\n  F1 <- 2*(precision*recall)/(precision+recall)\\n\\n  actual_label <- ifelse(actual$Popularity==\'popular\', 1, 0)\\n  pred_label <- ifelse(pred==\'popular\', 1, 0)\\n  pred_OCR= ROCR::prediction(pred_label, actual_label)\\n  perf = performance(pred_OCR, \'tpr\', \'fpr\')\\n  plot(perf)\\n  AUC <- as.numeric(performance(pred_OCR, \\"auc\\")@y.values)\\n  print(\\"Prediction Evaluation Outcome\\")\\n  eval <- as.data.frame(cbind(round(accuracy, 2), round(precision, 2), round(recall, 2), round(F1, 2), round(AUC, 2)))\\n  colnames(eval) = c(\'accuracy\',\'precision\',\'recall\',\'F1\',\'AUC\')\\n  print(eval)\\n}\\n\\nevaluate(train_pred, train)\\nevaluate(test_pred, test)\\n\\n# Confusion Matrix from caret package for evaluation\\n# Only for accuracy\\nconfusionMatrix(train_pred, train$Popularity)$overall[1]\\nconfusionMatrix(test_pred, test$Popularity)$overall[1]\\n# confusion matrix and all the metrics\\nconfusionMatrix(train_pred, train$Popularity)\\nconfusionMatrix(test_pred, test$Popularity)\\n```\\n\\n- Output\\n\\n![nnet model](./img/nnet-model.webp)\\n\\n![evaluation](./img/eval1.webp)\\n\\n### Comparing It With Default NN Model\\n\\nWhat if we used the model without any optimisation? Check out the outcome. It is pretty much as good as random picks. Now, you see the power of the simple caret optimisation!\\n\\n```R\\nmodel_default <- nnet(Popularity~., data=train, size=1)\\nplotnet(model_default)\\n\\ntrain_pred_default <- predict(model_default, newdata=train, type=\\"class\\")\\ntest_pred_default <- predict(model_default, newdata=test, type=\\"class\\")\\n\\nevaluate(train_pred_default, train)\\nevaluate(test_pred_default, test)\\n```\\n\\n![evaluation 2](./img/eval2.webp)\\n\\n### Should We Normalise the Dataset?\\n\\nIt really depends on the data. The nnet package generally performs fine without normalisation unless you have features in completely different scale. Normalisation is usually recommended for Neural Networks for better accuracy and faster training speed.\\n\\nWell, let\u2019s test it. nnet takes factor and numeric values as features. In the first example, all field types are numeric. Here, I converted categorical variables into factors and normalised other numeric fields.\\n\\n```R\\n# Normalising data\\nindex <- c(12:17, 30:37, 59)\\ncolnames(shares2[,index]) # checking the selected columns by index\\ntmp_df1 <- as.data.frame(scale(shares2[, -index]))\\ntmp_df2 <- lapply(shares2[, index], as.factor)\\nshares_norm <- cbind(tmp_df2, tmp_df1)\\n\\n# create training and testing set. Use the partition created above\\nset.seed(1000)\\ninTrain <- createDataPartition(shares2$Popularity, p=0.7, list=F)\\ntrain_norm <- shares_norm[inTrain, ]\\ntest_norm <- shares_norm[-inTrain, ]\\n\\n# train & predict\\nset.seed(1000)\\ncl <- makeCluster(4)\\nregisterDoParallel(cl)\\nnumFolds <- trainControl(method = \'cv\', number = 10, classProbs = TRUE, verboseIter = TRUE,\\n                         preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))\\n\\nmodel_nnet_norm <- train(Popularity~., data=train_norm, method=\'nnet\', trace=F,\\n                         preProcess = c(\'center\', \'scale\'), trControl = numFolds,\\n                         tuneGrid=expand.grid(.size=c(1,5,10), .decay=c(0,0.001,0.1)))\\n\\nmodel_nnet_norm # The final values used for the model were size = 1 and decay = 0.\\n\\ntrain_pred_norm <- predict(model_nnet_norm, newdata=train_norm, type=\\"raw\\")\\ntest_pred_norm <- predict(model_nnet_norm, newdata=test_norm, type=\\"raw\\")\\n\\nevaluate(train_pred_norm, train_norm)\\nevaluate(test_pred_norm, test_norm)\\n```\\n\\nTesting accuracy is slightly worse. But, overall it is similar. I think it is still good to test on the normalised dataset when you are modelling with neural networks. Normalised data usually give us a different model. In this case, the final values used for the model were size = 1 and decay = 0.\\n\\n![evaluation 3](./img/eval3.webp)\\n\\nLet\u2019s Benchmark!\\n\\nHere is the performance metric table from the paper (Fernandes et al. 2015). Our model has the same accuracy as AdaBoost and SVM. It preformed slightly worse then RF. We beat them in Precision (0.7). However, I don\u2019t think our model is superior because we have quite low AUC compared to all the models in the paper. Overall, I am happy with what I got. You can really see what simple optimisation can make difference!\\n\\n![benchmark](./img/performance-benchmark.webp)\\n\\n### Further Grid Search\\n\\nMy hyperparameter range for the size was (1, 5, 10) and 1 became the best hyperparameter. OK, 5 neuron was probably too over-fitting. But, Isn\u2019t 1 neuron too simple? Surely, having a few extra neurons would lead to better performance? I decided to investigate it further by setting size range to (1, 2, 3, 4).\\n\\n```R\\nset.seed(1000)\\ncl <- makeCluster(4)\\nregisterDoParallel(cl)\\nnumFolds <- trainControl(method = \'cv\', number = 10, classProbs = TRUE, verboseIter = TRUE,\\n                         preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))\\n\\nmodel_nnet<- train(Popularity~., data=train, method=\'nnet\', trace=F,\\n                   preProcess = c(\'center\', \'scale\'), trControl = numFolds,\\n                   tuneGrid=expand.grid(.size=c(1,2,3,4), .decay=c(0.001,0.01,0.1)))\\nmodel_nnet\\nplotnet(model_nnet)\\nplotnet(model2)\\ntrain_pred <- predict(model_nnet, newdata=train, type=\\"raw\\")\\ntest_pred <- predict(model_nnet, newdata=test, type=\\"raw\\")\\n\\nevaluate(train_pred, train)\\nevaluate(test_pred, test)\\n```\\n\\nThe best hyperparameters turned out to be size = 4 and decay = 0.1. With these parameters, I didn\u2019t see any improvement on test dataset prediction. I even observed the model was overfitting. Generally speaking, complex neural networks tend to overfit. This exercise somehow reminded me of Occam\u2019s razor. By paraphrasing it, the simplest is the best given everything is the same. I think Occam\u2019s razor is often sited in machine learning as the reminder that simpler is often better.\\n\\nPerformance for size = 4 and decay = 0.1 is below:\\n\\n![evaluation 4](./img/eval4.webp)\\n\\n### Your Turn\\n\\nCan we further optimise Neural Network model here by using different search or sampling methods?\\nR has other neural network packages like neuralnet or RSNNS. Do they perform better than nnet?\\nCheck all the models supported by the caret package. We can basically do all the methods used in the paper. See if we can use these methods and benchmark against the results?\\n\\n```R\\nlibrary(caret)\\nnames(getModelInfo())\\n```\\n\\nFun!\\n\\n### Reference\\n\\nFernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 \u2013 Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal (https://repositorium.sdum.uminho.pt/bitstream/1822/39169/1/main.pdf)"},{"id":"data-science/machine-learning/how-to-save-machine-learning-models-in-r","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/how-to-save-machine-learning-models-in-r","source":"@site/data-science/machine-learning/2018-01-05-save-r-model.md","title":"How To Save Machine Learning Models In R","description":"Once you finish training the model and are happy with it, you may need to consider saving the model. Otherwise, you will loose the model once you close the session. The model you create in R session is not persistent, only existing in the memory temporarily. Most of the time, training is a time-consuming process. Without saving the model, you have to run the training algorithm again and again. This is especially not good to happen in production.","date":"2018-01-05T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Machine Learning","permalink":"/mydatahack-old-blog/data-science/tags/machine-learning"},{"label":"Save Models","permalink":"/mydatahack-old-blog/data-science/tags/save-models"},{"label":"R","permalink":"/mydatahack-old-blog/data-science/tags/r"}],"readingTime":2.47,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/machine-learning/how-to-save-machine-learning-models-in-r","title":"How To Save Machine Learning Models In R","tags":["Data Science","Machine Learning","Save Models","R"]},"unlisted":false,"prevItem":{"title":"Predict Internet Popularity By Optimising Neural Networks With R","permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r"},"nextItem":{"title":"How To Deploy Spark Applications In AWS With EMR and Data Pipeline","permalink":"/mydatahack-old-blog/data-science/data-science/infra/how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline"}},"content":"Once you finish training the model and are happy with it, you may need to consider saving the model. \x3c!--truncate--\x3eOtherwise, you will loose the model once you close the session. The model you create in R session is not persistent, only existing in the memory temporarily. Most of the time, training is a time-consuming process. Without saving the model, you have to run the training algorithm again and again. This is especially not good to happen in production.\\n\\nIn production, it is ideal to have a trained model saved and your code are only loading and using it to predict the outcome on the new dataset.\\n\\nThere are two ways to save and load models in R. Let\u2019s have a look at them.\\n\\n### save() and load()\\n\\nYou can save named object to a file and load it in the later session. With save(), you have to load it with the same name. Here is the example of saving the optimised neural networks model created in the previous post. The model name is model_nnet.\\n\\nLet\u2019s save the model. If you want to deploy it, you can push .rda file with your code to production.\\n\\n```R\\nsave(model_nnet, file = \\"/tmp/model_nnet.rda\\")\\n```\\n\\nOnce you successfully save it, close the current R session. Then, you can load it back in the new session. It\u2019s ready for use.\\n\\n```R\\nload(file = \\"/tmp/model_nnet.rda\\")\\n```\\n\\n### saveRDS() and readRDS()\\n\\nsaveRDS() does not save the object name. Therefore, when you load it back, you can name it whatever you want (see below). While you can save many objects into a file with save(), saveRDS() only saves one object at a time as it is a lower-level function. saveRDS() also serialise an R object, which some people say better. But, most of the time, it really doesn\u2019t matter. The model saved with save() and saveRDS() is the same. You will get the same predictive outcome.\\n\\n```R\\nsaveRDS(model_nnet, file = \\"/tmp/model_nnet2.rda\\")\\n```\\n\\nAs mentioned above, you can load it with whatever name you want (in this case, loaded as model2).\\n\\n```R\\nmodel2 <- readRDS(\\"/tmp/model_nnet2.rda\\")\\n```\\n\\nCheck to see if both models are loaded.\\n\\n```\\n> ls()\\n> [1] \\"model_nnet\\" \\"model2\\"\\n> For sanity check, you can plot them.\\n\\nplotnet(model_nnet)\\nplotnet(model2)\\n```\\n\\nCommon Warning Handling\\n\\nWhen you try to save the model with either save() or saveRDS(), you may get the warnings such as \u2018display list redraw incomplete\u2019 and \u2018invalid graphics state\u2019 as below.\\n\\nWarning messages:\\n\\n```\\n1: In doTryCatch(return(expr), name, parentenv, handler) :\\ndisplay list redraw incomplete\\n2: In doTryCatch(return(expr), name, parentenv, handler) :\\ninvalid graphics state\\n3: In doTryCatch(return(expr), name, parentenv, handler) :\\ninvalid graphics state\\n```\\n\\nWhen you have these warnings, you need to reset your graphic device by running dev.off(). For example, if you have a plot in the R Studio viewer, you will probably get them. Once you reset it with dev.off(), you can save the model without warning.\\n\\nYep, now you are an expert in saving R models!"},{"id":"data-science/infra//how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/infra/how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline","source":"@site/data-science/infra/2018-01-04-spark-emr-aws.md","title":"How To Deploy Spark Applications In AWS With EMR and Data Pipeline","description":"Once you create an awesome data science application, it is time for you to deploy it. There are many ways to productionise them. The focus here is deploying Spark applications by using the AWS big data infrastructure. From my experience with the AWS stack and Spark development, I will discuss some high level architectural view and use cases as well as development process flow.","date":"2018-01-04T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"Tools and Infrastructure","permalink":"/mydatahack-old-blog/data-science/tags/tools-and-infrastructure"},{"label":"AWS","permalink":"/mydatahack-old-blog/data-science/tags/aws"},{"label":"Data Pipeline","permalink":"/mydatahack-old-blog/data-science/tags/data-pipeline"},{"label":"EMR","permalink":"/mydatahack-old-blog/data-science/tags/emr"},{"label":"Spark","permalink":"/mydatahack-old-blog/data-science/tags/spark"}],"readingTime":5.09,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/infra//how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline","title":"How To Deploy Spark Applications In AWS With EMR and Data Pipeline","tags":["Data Science","Tools and Infrastructure","AWS","Data Pipeline","EMR","Spark"]},"unlisted":false,"prevItem":{"title":"How To Save Machine Learning Models In R","permalink":"/mydatahack-old-blog/data-science/data-science/machine-learning/how-to-save-machine-learning-models-in-r"},"nextItem":{"title":"How To Do Sentiment Analysis On Your Favourite Book With R","permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-do-sentiment-analysis-on-your-favourite-book-with-r"}},"content":"Once you create an awesome data science application, it is time for you to deploy it. There are many ways to productionise them. \x3c!-- truncate --\x3eThe focus here is deploying Spark applications by using the AWS big data infrastructure. From my experience with the AWS stack and Spark development, I will discuss some high level architectural view and use cases as well as development process flow.\\n\\nAWS offers a solid ecosystem to support Big Data processing and analytics, including EMR, S3, Redshift, DynamoDB and Data Pipeline. If you have a Spark application that runs on EMR daily, Data Pipleline enables you to execute it in the serverless manner.\\n\\nThe serverless architecture doesn\u2019t strictly mean there is no server. When the code is running, you of course need a server to run it. The main difference from the traditional way is that you store your codes and models in a repository, launch the server only during execution and close it as soon as it finishes. In this architecture, you only pay for the cost for the length of code execution. The architecture is often used for real-time data streaming or integration. AWS Lambda and Kinesis are good examples.\\n\\nWhat is good about Data Pipeline?\\n\\nData Pipleline is a great tool to use the serverless architecture for batch jobs that run on schedule. You can design the pipeline job to control resources, workflow, execution dependency, scheduling and error handling without the hustle of provisioning and managing servers and the cost of keeping them running all the time.\\n\\nAnother advantage is that you can create a job with parameters (e.g. DB connection URLs, credentials, target schema/table). Data Pipeline jobs are basically JSON files. You can easily export and edit it. With parameters, you can easily promote jobs from the development environment to the production as it is a matter of importing the JSON file from dev to prod (which of course can be coded up for automation).\\n\\nI also found that debugging and updating Spark codes or models became simpler. We can simply update the repo without touching the pipeline.\\n\\nThe cost of running Data Pipeline jobs is affordable. For a low frequency job (once a day), it costs 60 cent per month as of today. See the link for the pricing information. Note that the cost of running EMR will be charged at hourly rate on top of the running cost of pipelines.\\n\\nLet\u2019s have a look at the use cases.\\n\\nUse Case 1\\n\\nSourcing the data from different databases (application database, data lake and data warehouse) and joining them prior to running the algorithm.\\nThe output needs to be presented in a BI tool.\\nPrerequisite\\n\\nSave your Sqoop code (as .sh), Spark code and model to Bitbucet or GitHub (or S3, which is less preferable option).\\n\\nSolution\\n\\nWithin the Data Pipeline, you can create a job to do below:\\n\\nLaunch a ERM cluster with Sqoop and Spark. Source the Sqoop code to EMR and execute it to move the data to S3.\\nSource the Spark code and model into EMR from a repo (e.g. Bitbucket, GitHub, S3). Execute the code, which transform the data and create output according to the pre-developed model.\\nMove the output of the Spark application to S3 and execute copy command to Redshift.\\nBI tools to fetch the output from Redshift for presentation.\\nSqoop is a command line tool to transfer data between Hadoop and relational databases. EMR uses Hadoop for file management. So, it is the best tool to move the data from relational databases through Hadoop in EMR to S3. It is fast and easy to learn. I learned everything about Sqoop from a cookbook which you can download for free here.\\n\\nBelow image not working with docusaurus compilation \ud83d\ude22\\n\\n\x3c!-- ![data-pipeline](\'./img/data-pipeline.png) --\x3e\\n\\nUse Case 2\\n\\nIngesting data into Data Lake with an ETL tool.\\nTransforming data with ETL or ELT within the Redshift.\\nAll the data required for the Spark application is in the data warehousing layer.\\nIn the enterprise environment, it is common to have an ETL tool that manage the data ingestion and transformation. Accessing the application databases directly for analytics is not the best architectural practice, either. The first use case is suitable when you need to do data ingestion in an ad-hoc manner or cannot wait for ETL development for the sake of speedy delivery.\\n\\nPrerequisite\\n\\nThe same as Use Case 1.\\n\\nSolution\\n\\nThe figure shows Informatica as an ETL tool. There are heaps of options out there and any tool that suits your use case is fine. I compared DataStage, Informatica and Talend in the past and found Informatica best suited for the particular situation I was in. I especially liked the Redshift connector and I wrote a small review.\\n\\nThe workflow has two parts, managed by an ETL tool and Data Pipeline.\\n\\nETL Tool manages below:\\n\\nETL tool does data ingestion from source systems.\\nDo ETL or ELT within Redshift for transformation.\\nUnload any transformed data into S3.\\nData Pipeline manages below:\\n\\nLaunch a cluster with Spark, source codes & models from a repo and execute them. The output is moved to S3.\\nCopy data from S3 to Redshift (you can execute copy commands in the Spark code or Data Pipeline).\\nThen, you can source the output into a BI tool for presentation.\\n\\n![informatica](img/informatica.png)\\n\\nDevelopment Process Workflow\\n\\nFinally, let\u2019s have a look at development process workflow. Prior to Spark application deployment, we still need to develop and test the application in an EMR cluster. In this workflow, we only launch the cluster after prototyping on the local machine with a smaller dataset. This will save money as running an EMR cluster is expensive.\\n\\nOnce the code and models are developed, we can close the EMR cluster and move onto the serverless execution in batch. Codes and models can be source from S3 in the Data Pipeline. It is a standard practice to version control them in a git type repository. Sourcing them from a repo in Data Pipeline makes more sense.\\n\\n![Data Science workflow with EMR](img/ds-workflow-with-emr.png)\\n\\nLet us know your experience with data science application deployment!"},{"id":"data-science/visualisation/how-to-do-sentiment-analysis-on-your-favourite-book-with-r","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-do-sentiment-analysis-on-your-favourite-book-with-r","source":"@site/data-science/visualisation/2018-01-03-sentiment-analysis.md","title":"How To Do Sentiment Analysis On Your Favourite Book With R","description":"I love dissecting and analysing my favourite books by reading them again and again, discussing them with my like-minded friends, getting to know the authors and reading other people\u2019s writings about them.  My obsession with books lead me to thinking, how can I visualise them in interesting ways? Making Word Cloud is an easy and fun way to visualise your favourite book as in the previous post. In this post, we will go down deeper into the world of text analytics by using sentiment analysis. I will show you how to split the text by sentence, conduct sentence-wise sentiment analysis and create an interactive plot that shows how sentiment changes as the story progresses.","date":"2018-01-03T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"R","permalink":"/mydatahack-old-blog/data-science/tags/r"},{"label":"Sentiment Analysis","permalink":"/mydatahack-old-blog/data-science/tags/sentiment-analysis"},{"label":"Text Analytics Visualisation","permalink":"/mydatahack-old-blog/data-science/tags/text-analytics-visualisation"}],"readingTime":7.485,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/visualisation/how-to-do-sentiment-analysis-on-your-favourite-book-with-r","title":"How To Do Sentiment Analysis On Your Favourite Book With R","tags":["Data Science","R","Sentiment Analysis","Text Analytics Visualisation"]},"unlisted":false,"prevItem":{"title":"How To Deploy Spark Applications In AWS With EMR and Data Pipeline","permalink":"/mydatahack-old-blog/data-science/data-science/infra/how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline"},"nextItem":{"title":"How To Create a Word Cloud For Your Favourite Book With R","permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-create-a-word-cloud-for-your-favourite-book-with-r"}},"content":"I love dissecting and analysing my favourite books by reading them again and again, discussing them with my like-minded friends, getting to know the authors and reading other people\u2019s writings about them. \x3c!--truncate--\x3e My obsession with books lead me to thinking, how can I visualise them in interesting ways? Making Word Cloud is an easy and fun way to visualise your favourite book as in the previous post. In this post, we will go down deeper into the world of text analytics by using sentiment analysis. I will show you how to split the text by sentence, conduct sentence-wise sentiment analysis and create an interactive plot that shows how sentiment changes as the story progresses.\\n\\nThere are many different ways to do sentiment analysis. The easiest way is to split the sentence by word and score each word by looking up word sentiment dictionary. The tidytext packages in R has a build in function to do a basic sentiment analysis. The package documentation from CRAN shows sentiment analysis on Jane Austin text. This one with Harry Potter is also fun to read. For more advanced Natural Language Processing (NLP), you can use Stanford CoreNLP, which is very powerful, but cumbersome to use and can be slow at a time.\\n\\nIn this post, I am using sentimentr. The package is fast and super easy to use. It does sentence-wise sentiment analysis by using dictionary lookup and valence shifter. With valence shifter, it updates the normal word sentiment dictionary look up score with the score by looking at polarised word (such as really or hardly). It reflects the sentiment more accurately than just scoring individual words. It also has the get_sentense function to splice the text into sentences by using regular expressions.\\n\\nThe `sentimentr` package really makes sentiment analysis pop and accessible. I really love using it.\\n\\nSentence analysis on literature is hard because of slang, unconventional style and expressions, unambiguous sentence separation, sarcasms, jokes and so on. Despite the outcome is not perfect, you\u2019ll have a lot of fun!\\n\\nAs the continuation from the previous post, I am using one of my favourite book, On The Road by the greatest Beat hero Jack Kerouac.\\n\\nOk, let\u2019s code!\\n\\n### Summary Steps\\n\\n1. Download a pdf file and convert it into text.\\n2. Create a data frame with row per sentence and do clean up.\\n3. Further split the sentence by get_sentence() and create a new data frame\\n4. Get sentiment per sentence.\\n5. Plot the sentiment by `plotly`.\\n6. Check the results and have fun!\\n\\n### Steps\\n\\n(1) Get the pdf file of On The Road from `freeditorial.com` and use `pdftools` to convert it to text. This step is the same as in the previous post.\\n\\n```R\\nlibrary(pdftools)\\ndownload.file(\\"https://freeditorial.com/en/books/on-the-road/downloadbookepub/pdf\\",\\n\\"/tmp/on_the_road.pdf\\", mode = \\"wb\\")\\ntxt <- pdf_text(\'/tmp/on_the_road.pdf\')\\n```\\n\\n(2) Create the first data frame with row by sentence. For sentiment analysis, cleaning up the text has to be a little bit more diligent than just making a word cloud. You just need to look at your text file to determine what is needed. The regular expression is the way to go to clean up the text data. Use online resource for detailed examples as well as a quick reference. Make sure to test your regex with a short example!\\n\\n```R\\nlines <- \'\'\\nfor (i in 2:length(txt)) {\\ntmp <- gsub(\'\\\\r\\\\n\', \' \', txt[i]) # \\\\r\\\\n for Windows, \\\\n for Linux\\nlines <- paste(lines, tmp, sep=\' \')\\n}\\n\\nvec <- strsplit(lines, \'\\\\\\\\.\')\\ndf <- data.frame(vec)\\n\\ndf <- as.data.frame(df[-c(nrow(df), nrow(df)-1), ]) # Remove Last 2 lines\\ncolnames(df)[1] = \'line\' # Rename Columns\\n\\ndf$line <- gsub(\\"\xab\\", \\"\\", gsub(\\"\xbb\\", \\"\\", df$line))\\ndf$line <- gsub(\'^\\\\\\\\s+PART\\\\\\\\s+[A-Z]+\\\\\\\\s+\', \'\', df$line)\\ndf$line <- as.character(trimws(df$line, \'both\'))\\ndf$line <- gsub(\'^[1-5]\\\\\\\\s{2,}\', \'\',  df$line)\\ndf$line <- gsub(\'- -, \', \'\',df$line)\\n```\\n\\n(3) Further split the sentences by the power of get_sentences function. This step is necessary to get a sentiment score per sentence according to the `sentimentr` package. The sentiment function in `sentimentr` may returns more than one sentiment score per sentence if you do not do this.\\n\\n```R\\nlibrary(sentimentr)\\nsentence <- c()\\nfor (line in df$line) {\\ntmp <- get_sentences(line)\\nfor(i in 1:length(tmp[[1]])) {\\nsentence_tmp <- tmp[[1]][i]\\nsentence <- c(sentence, sentence_tmp)\\n}\\n}\\n\\ndf_sentr <- data.frame(sentence)\\ndf_sentr$sentence <- as.character(df_sentr$sentence)\\n(4) Once your data frame is ready, do the scoring with sentiment(). Add negative, positive and neutral indicator for visualisation.\\n\\nsentiment <- sentiment(df_sentr$sentence)\\n\\ndf_sentr$sentiment <- as.numeric(sentiment$sentiment)\\ndf_sentr$pntag <- ifelse(sentiment$sentiment == 0, \'Neutral\',\\nifelse(sentiment$sentiment > 0, \'Positive\',\\n                                ifelse(sentiment$sentiment < 0, \'Negative\', \'NA\')))\\n```\\n\\n(5) You can plot it with base R plot function. I like using Plotly because it becomes interactive. The x-axis shows the progress of the story and the y-axis shows the sentiment. Positive, negative and neutral sentiments are colour-coded. Once you hover the mouse over the dot, you can see the sentence. The `magrittr` package enables the magic of the pipe (%>%) operation.\\n\\n- base R plot\\n\\n```R\\nplot(df_sentr$sentiment, type=\'l\', pch=3)\\n```\\n\\n- plotly- more fun\\n\\n```R\\nax <- list(\\ntitle = \\"Sentence\\",\\nzeroline = FALSE,\\nshowline = FALSE,\\nshowticklabels = FALSE\\n)\\n\\nlibrary(plotly)\\nlibrary(magrittr)\\nplot_ly(data = df_sentr, x = ~sentence, y = ~sentiment, color = ~pntag,\\ntype = \'scatter\', mode = \'markers\') %>% layout(xaxis = ax)\\nplot_ly(data = df_sentr, y = ~sentiment, color = ~pntag,\\ntype = \'scatter\', mode = \'markers\') %>% layout(xaxis = ax)\\n```\\n\\nOutput Example\\n\\n![Output](./img/sentiment-graph.webp)\\n\\n(6) Let\u2019s check what is the most positive and the most negative and calculate the average sentiment score. I also created csv files with sentiment score above 1 and below -1.\\n\\n- Check max and min sentiments\\n\\n```R\\ndf_sentr$sentence[which.max(df_sentr$sentiment)]\\ndf_sentr$sentence[which.min(df_sentr$sentiment)]\\n```\\n\\n- Get average\\n\\n```R\\nmean(df_sentr$sentiment)\\n```\\n\\n- Check positive and negative sentences\\n\\n```R\\ncheck_pos <- subset(df_sentr, df_sentr$sentiment >= 1.0)\\ncheck_pos <- check_pos[order(check_pos$sentiment, decreasing = T),]\\nwrite.csv(check_pos, \\"positive.csv\\")\\ncheck_neg <- subset(df_sentr, df_sentr$sentiment <= -1.0)\\ncheck_neg <- check_neg[order(check_neg$sentiment, decreasing = F),]\\nwrite.csv(check_neg, \\"negative.csv\\")\\n```\\n\\n## Discussion\\n\\nThe average sentiment score was 0.024. All the highs and lows must have cancelled each other out, coming out slightly positive just like life in general.\\n\\nThe most positive moment in the story according to sentimentr is absolutely gold.\\n\\n- I think Marylou was very, very wise leaving you, Dean, said Galatea\\n\\n- Yeah, that\u2019s pretty much the best decision! On a more serious note, this is the prime example of the effect of using valence shifter in the sentiment algorithm. It has two positive polarised words (very, very) on a positive word (wise). Very twice really amplified the positive sentiment on this sentence.\\n\\n- Not sure about the most negative one. But, this is also the fun part, getting misplaced sentiment by an algorithm.\\n\\n- He showed me rooming houses where he stayed, railroad hotels, poolhalls, diners, sidings where he jumped off the engine for grapes, Chinese restaurants where he ate, park\\n\\nLooking at the results, I think it performed well overall despite questionable sores here and there. To be fair, On The Road is a difficult one to run sentiment algorithm because it is written in a stream-of-consciousness style with many unique expressions and unclear sentence break points.\\n\\nHere are the top 6 positive sentences. No.2 is the classic Dean Moriarty haunted by his own madness. No.4 and 5 are questionable. No.6 is absolutely yes, I think.\\n\\n1. I think Marylou was very, very wise leaving you, Dean, said Galatea\\n2. Hmm, ah, yes, excellent, splendid, harrumph, egad!\\n3. It certainly was pleasant, said Hingham, looking away\\n4. Doll, Dorothy and Roy Johnson, a girl from Buffalo, Wyoming, who was Dorothy\u2019s friend, Stan, Tim Gray, Babe, me, Ed Dunkel, Tom Snark, and several others, thirteen in all\\n5. There\u2019s one last thing I want to know \u2013 But, dear Sal, you\u2019re listening, you\u2019re sitting there, we\u2019ll ask Sal\\n6. Absolutely, now, yes?\\n\\nHere are the bottom 6 negative sentences. Yeah, it certainly shows some low moments in the book despite of some questionable entries. No. 5 is another classic Dean Moriarty.\\n\\n1. He showed me rooming houses where he stayed, railroad hotels, poolhalls, diners, sidings where he jumped off the engine for grapes, Chinese restaurants where he ate, park benches where he met girls, and certain places where he\u2019d done nothing but sit and wait around\\n2. There was something paralyzed about his movements, and he did nothing about leaving the doorway, but just stood in it, muttering, Stan, and Don\u2019t go, and looking after us anxiously as we rounded the comer\\n3. Then the third day began having a terrible series of waking nightmares, and the were so absolutely horrible and grisly and green that I lay there doubled up with my hands around my knees, saying, \u2018Oh, oh, oh, ah, oh\\n4. Offisah, I heard Dean say in the most unctuous and ridiculous tones, offisah, I was only buttoning my flah\\n5. And the one to my left here, older, more sure of himself but sad\\n6. But this foolish gang was bending onward\\n\\nWow, what a fun I had!\\n\\nNow, you are ready for sentiment analysis on your favourite book!"},{"id":"data-science/visualisation/how-to-create-a-word-cloud-for-your-favourite-book-with-r","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-create-a-word-cloud-for-your-favourite-book-with-r","source":"@site/data-science/visualisation/2018-01-02-on-the-road-word-cloud.md","title":"How To Create a Word Cloud For Your Favourite Book With R","description":"Making a word cloud is fun and easy. It is a way of looking at text data and gain a different perspective. For example, if you have a bunch of customer feedback about your product, you can quickly create a word cloud to get some ideas. When I work with text data, it is often the first step I do to quickly show business users something or to simply get the feeling before I get into more advanced analysis such as sentiment analysis or machine learning.","date":"2018-01-02T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"R","permalink":"/mydatahack-old-blog/data-science/tags/r"},{"label":"Text Analytics","permalink":"/mydatahack-old-blog/data-science/tags/text-analytics"},{"label":"Word Cloud","permalink":"/mydatahack-old-blog/data-science/tags/word-cloud"},{"label":"Visualisation","permalink":"/mydatahack-old-blog/data-science/tags/visualisation"}],"readingTime":6.83,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/visualisation/how-to-create-a-word-cloud-for-your-favourite-book-with-r","title":"How To Create a Word Cloud For Your Favourite Book With R","tags":["Data Science","R","Text Analytics","Word Cloud","Visualisation"]},"unlisted":false,"prevItem":{"title":"How To Do Sentiment Analysis On Your Favourite Book With R","permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-do-sentiment-analysis-on-your-favourite-book-with-r"},"nextItem":{"title":"How To Customise ShinyApp With Bootstrap, Javascript And Plotly","permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-customise-shinyapp-with-bootstrap-css-javascript-and-plotly"}},"content":"Making a word cloud is fun and easy. It is a way of looking at text data and gain a different perspective. \x3c!--truncate --\x3eFor example, if you have a bunch of customer feedback about your product, you can quickly create a word cloud to get some ideas. When I work with text data, it is often the first step I do to quickly show business users something or to simply get the feeling before I get into more advanced analysis such as sentiment analysis or machine learning.\\n\\nCreating a word cloud for your favourite book is even more fun if you are a book lover. It is another way to get to know your book and gives you a new creative perspective. In this post, I am building a word cloud from On The Road by Jack Kerouac. This is one of my favourite books. It is beautiful, ragged and free.\\n\\nThe simplest way of building word cloud is counting individual words that appear in the document. The more frequent a word appears on the book, the bigger the size of the word becomes in the cloud. To accomplish this task, you first need to download your favourite book and read it into the memory, process it into an appropriate format, and visualise it. The process is relatively simple. In R, the tm package pretty much handles processing and formatting of text data and the wordcloud package handles the creation of word cloud.\\n\\nThere are heaps of instruction available online, too. Here is the super simple introduction to word cloud with R from R-bloggers. This post will go further into the customisation of the word cloud and the creation of a document term matrix.\\n\\nOK, enough intro. Let\u2019s get to coding.\\n\\n### Step Summary\\n\\n- Obtain free pdf copy of On The Road online.\\n- Convert pdf into text data.\\n- Split it by a new line, convert it into data frame and do clean up.\\n- Create corpus and do pre-processing with tm.\\n- Build word cloud.\\n\\n### Steps\\n\\n(1) I used freeditorial.com to get a copy of On The Road. You can also check out Project Gutenberg, which offers heaps of free books for downloading. If you are into Jane Austin, R has a package (janeaustenr) that contains her complete works and prepped for text analytics. The harrypotter package offers the full text of the first seven Harry Potter books.\\n\\nAfter downloading the pdf file, I used pdftools to convert it into text. It converts each page into a vector element. It is a great package to read pdf with R.\\n\\n```R\\nlibrary(pdftools)\\ndownload.file(\\"https://freeditorial.com/en/books/on-the-road/downloadbookepub/pdf\\",\\n              \\"/tmp/on_the_road.pdf\\", mode = \\"wb\\")\\ntxt <- pdf_text(\'/tmp/on_the_road.pdf\')\\n(2) Split the text by new line and create a data frame. Once you have a data frame, it becomes easier to do clean up. I removed the title page, chapter title and the last two lines which are not the part of the novel. I also removed some special characters. I kept the chapter number because this can be dealt with tm later. For this part, you really need to look at the pdf you download and decide what to remove. When you use strisplit, make sure to add carrage return with new line for Windows like \\\\r\\\\n. For Linux and Unix, \\\\n is sufficient.\\n\\nvec <- character()\\n\\nfor(i in 2:length(txt)){\\n    tmp <- strsplit(txt[i], \'\\\\n\') #\\\\r\\\\n for Windows\\n    for(line in tmp){\\n        vec <- c(vec, line)\\n    }\\n}\\n\\nlines <- data.frame(vec)\\nlen <- length(lines$vec)\\nlines <- subset(lines, !grepl(\'PART\', vec))\\nlines$vec <- as.character(trimws(lines$vec, \'both\'))\\nlines <- as.data.frame(lines[-c(len, len-1),]) # Remove the last two lines (not part of the novel)\\ncolnames(lines) <- c(\'vec\')\\nlines$vec <- gsub(\\"\xc2\xab\\", \\"\\", gsub(\\"\xc2\xbb\\", \\"\\", lines$vec))\\n(3) Create a corpus and do pre-processing (remove punctuation, numbers and stopwords and converting all words to lower case). Stemming is important for machine learning like spam identification. For a word cloud, it generates truncated words and looks wired. So, I always omit it.\\n\\nlibrary(tm)\\nlibrary(SnowballC)\\nlibrary(wordcloud)\\n\\ncorpus <- Corpus(VectorSource(lines$vec))\\ncorpus <- tm_map(corpus, PlainTextDocument)\\ncorpus <- tm_map(corpus, removePunctuation)\\ncorpus <- tm_map(corpus, removeNumbers)\\ncorpus <- tm_map(corpus, content_transformer(tolower))\\ncorpus <- tm_map(corpus, removeWords, stopwords(\'english\'))\\n# corpus <- tm_map(corpus, stemDocument) - skip this one!\\ncorpus <- tm_map(corpus, stripWhitespace)\\n```\\n\\n(4) To create a word cloud, you can simply pass corpus to the wordcloud function. For colour, I created my own colour vector and randomiser so that colour changes randomly every time I run it. You can use RColorBrewer (see Step 5), but I found none of them really gives me the colour combos that I want. I set min.freq = 80 and max.words = 100. 100 words per word cloud is a good place to start. I usually determine min.freq by looking at the document term matrix (see step 6). The example below creates a png file, which is better than generating the cloud in the viewer. By default, the file is saved by the path of the current R session (check it with the getwd function).\\n\\n```R\\ncolors <- c(\\"blue\\", \\"blue3\\", \\"blue4\\", \\"blueviolet\\", \\"brown1\\", \\"brown2\\",\\n            \\"brown3\\", \\"cyan3\\", \\"cyan4\\", \\"darkgoldenrod3\\", \\"darkgoldenrod4\\",\\n            \\"chocolate2\\", \\"chocolate3\\", \\"chocolate1\\", \\"chartreuse3\\",\\n            \\"chartreuse3\\", \\"coral2\\", \\"coral3\\", \\"coral4\\",\\"cornflowerblue\\",\\n            \\"darkmagenta\\", \\"darkorchid2\\", \\"darkorchid3\\", \\"darkorchid4\\",\\n            \\"deeppink1\\", \\"deeppink2\\", \\"deeppink3\\",\\"deepskyblue2\\", \\"deepskyblue3\\",\\n            \\"deepskyblue4\\", \\"dodgerblue4\\", \\"dodgerblue3\\", \\"dodgerblue2\\",\\n            \\"firebrick1\\", \\"firebrick2\\", \\"firebrick3\\", \\"green3\\", \\"green4\\",\\n            \\"hotpink\\", \\"hotpink3\\", \\"hotpink4\\", \\"lightseagreen\\", \\"lightslateblue\\",\\n            \\"purple\\", \\"purple1\\", \\"purple2\\", \\"purple3\\", \\"purple4\\", \\"red\\", \\"red1\\",\\n            \\"red2\\", \\"red3\\", \\"red4\\", \\"turquoise4\\", \\"turquoise2\\", \\"violetred1\\",\\n            \\"violetred2\\", \\"violetred3\\", \\"violetred4\\")\\n\\npng(\\"wordcloud_on_the_road.png\\", width=1280,height=800)\\n\\ncolor_vector1 <- sample(colors, 20)\\nwordcloud(corpus, max.words = 100, min.freq = 80, random.order = FALSE,\\n          color = color_vector1, rot.per = 0.15, scale = c(10,1.5))\\ndev.off()\\nprint(\\"Wordcloud png file has been generated.\\")\\n(5) Here is the example of using RColorBrewer for a word cloud.\\n\\npng(\\"wordcloud_on_the_road2.png\\", width=1280,height=800)\\ncolor_vector1 <- sample(colors, 20)\\nwordcloud(corpus, max.words = 100, min.freq = 80, random.order = FALSE,\\n          color = brewer.pal(12, \\"Set3\\"), rot.per = 0.15, scale = c(10,1.5))\\ndev.off()\\nprint(\\"Wordcloud png file with RColorBrewer has been generated.\\")\\n(6) A document term matrix has all the words in the text as columns and each line as rows. If a word appears in the row, it puts 1. To create a word count, you can simply aggregate it by columns. The wordcloud function takes column names and column sum as argument instead of corpus as below.\\n\\npng(\\"wordcloud_on_the_road2.png\\", width=1280,height=800)\\ncolor_vector1 <- sample(colors, 20)\\nwordcloud(corpus, max.words = 100, min.freq = 80, random.order = FALSE,\\n          color = brewer.pal(12, \\"Set3\\"), rot.per = 0.15, scale = c(10,1.5))\\ndev.off()\\nprint(\\"Wordcloud png file with RColorBrewer has been generated.\\")\\n```\\n\\nYou can sort the aggregated data frame to check if the word cloud looks right according to word frequencies.\\n\\n![word count example](./img/dtm_df_sum.webp)\\n\\nI also use it to determin the min.freq parameter. 89 words exist for more than 80 frequencies. Hence, I used 80 as min.freq. Each text is different and it is best to check it so that you don\u2019t miss out on words.\\n\\nCreating DTM is the first step for machine learning on text data (but make sure to include stemming, which is the step I skipped for the word cloud).\\n\\n```R\\nfrequencies <- DocumentTermMatrix(corpus)\\ndtm_df <- as.data.frame(as.matrix(frequencies))\\ndtm_df_sum <- as.data.frame(apply(dtm_df, 2, sum))\\ncolnames(dtm_df_sum) <- c(\\"Frequency_Count\\")\\nwords <- rownames(dtm_df_sum)\\nrownames(dtm_df_sum) <- NULL\\ndtm_df_sum <- cbind(dtm_df_sum, words)\\ndtm_df_sum <- dtm_df_sum[order(dtm_df_sum$Frequency_Count, decreasing=T),]\\n\\nnrow(subset(dtm_df_sum, Frequency_Count >= 100))\\nnrow(subset(dtm_df_sum, Frequency_Count >= 80))\\nnrow(subset(dtm_df_sum, Frequency_Count >= 50))\\n```\\n\\nHere are the word cloud I generated. Most of the books, you will see the characters\u2019 name appearing as the most frequently used word. As expected, you can see dean as the biggest word as the story is centred on the madman, Dean Moriarty. The word cloud sort of makes sense to me. A lot of talking in the book, hence said is the second most frequently used word. Dean always goes back or comes back, hence back comes third. They are always on the move traveling across America, hence the words like went, going, see, around, road, car, way, people, girls, everybody and miles are frequently used words. This is so cool!\\n\\nCustom Colour\\n\\n![word cloud 1](./img/word-cloud-1.webp)\\n\\nRColorBrewer (Set3)\\n\\n![word cloud 2](./img/word-cloud-2.webp)\\n\\nI prefer using my own colour selections than any of the RColorBrewer palette as the palette sometimes give me the colours that are hard to read!\\n\\nNow it\u2019s your turn to create a word cloud for your favourite book!\\n\\nThe next step is learning how to do sentiment analysis on your favourite book!"},{"id":"data-science/visualisation/how-to-customise-shinyapp-with-bootstrap-css-javascript-and-plotly","metadata":{"permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-customise-shinyapp-with-bootstrap-css-javascript-and-plotly","source":"@site/data-science/visualisation/2017-12-22-customise-shiny-app.md","title":"How To Customise ShinyApp With Bootstrap, Javascript And Plotly","description":"What is the easiest way to make a data science product? My answer is to use Shiny. You can code both front-end and server-side in R to create beautiful interactive web applications.  You don\u2019t even need to know any HTML. Shiny server is free, too. You sign up and get it started right away! Check out the shiny gallery for inspiration. To learn Shiny, there are heaps of online tutorials like this one.","date":"2017-12-22T00:00:00.000Z","tags":[{"label":"Data Science","permalink":"/mydatahack-old-blog/data-science/tags/data-science"},{"label":"App","permalink":"/mydatahack-old-blog/data-science/tags/app"},{"label":"R","permalink":"/mydatahack-old-blog/data-science/tags/r"},{"label":"Shiny","permalink":"/mydatahack-old-blog/data-science/tags/shiny"},{"label":"Visualisation","permalink":"/mydatahack-old-blog/data-science/tags/visualisation"}],"readingTime":7.15,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"data-science/visualisation/how-to-customise-shinyapp-with-bootstrap-css-javascript-and-plotly","title":"How To Customise ShinyApp With Bootstrap, Javascript And Plotly","tags":["Data Science","App","R","Shiny","Visualisation"]},"unlisted":false,"prevItem":{"title":"How To Create a Word Cloud For Your Favourite Book With R","permalink":"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-create-a-word-cloud-for-your-favourite-book-with-r"}},"content":"What is the easiest way to make a data science product? My answer is to use Shiny. You can code both front-end and server-side in R to create beautiful interactive web applications. \x3c!--truncate --\x3e You don\u2019t even need to know any HTML. Shiny server is free, too. You sign up and get it started right away! Check out the shiny gallery for inspiration. To learn Shiny, there are heaps of online tutorials like this one.\\n\\nIn my opinion, this is the area where the hurdle is higher for Python. Python doesn\u2019t really have a specific framework to create data visualisation apps that run on a web server. Sure, you can use Django or Flask, but you really need to study web development before making anything. Most of us are interested in analysing data and delivering insights, not becoming a full-stack web developer. Shiny really enables us to focus on data analysis and insights creation rather than worrying about the web technologies.\\n\\nHaving said that, Shiny has its own limitations. It is not for creating a complex web application as you are coding the server-side with R. Certainly, this is not for an e-commerce platform. It is most suited for a one page web app with R logic behind it. If you are only coding the front-end with R, you will soon hit the wall in terms of how the page looks.\\n\\nTo customise Shiny App UI, the best approach is to use your own HTML, CSS and Javascripts. In this hack, I will show you how to customise ShinyApp UI with minimal effort. I will also show you how to make plotly work with custom HTML and import your own images, too.\\n\\nPlotly API enables you to make your R visualisation interactive. R graphs produced by plot() function, for example, is just an image without interactivity. Plotly uses Javascript to make it interactive in both web browsers and R Studio viewer.\\n\\nTo use plotly, you can simply pass the data and parameters to the plot_ly() function. See the difference it makes even with such a simple scatter graph.\\n\\n```R\\nlibrary(plotly)\\ndata(mtcars)\\n\\n# (1) Basic scatter graph with R plot function\\nplot(mtcars$mpg, mtcars$hp, col=\'red\', pch=16)\\n# (2) With plotly\\nplot_ly(mtcars, x = ~mpg, y = ~hp)\\n```\\n\\nWhat makes Plotly even more powerful is that you can use it on top of ggplot. Plotly has a function called ggplotly which takes a ggplot function as an argument. This means you can create a complex visualisation with ggplot first and then simply pass the function to ggplotly() for interactivity. It\u2019s the ggplot on steroids. Check out how easily this can be done.\\n\\n```R\\nlibrary(ggplot)\\nplot <- ggplot(data=mtcars, aes(x=mpg, y=hp)) + geom_point(color=\'purple\')\\n# (1) Scatter plot with ggplot\\nplot\\n# (2) Scatter plot with ggplot and plotly\\nggplotly(plot)\\n```\\n\\nAs an example, I created the [diamond price predictor](https://portf.shinyapps.io/diamondplotly/) app that predicts diamond price based on carat, cut, color and clarity by using diamond dataset that comes with base R. You probably need to press \u2018Reload\u2019 as the free version of Shiny server goes to sleep if it\u2019s not in use. It also uses plotly for plot interactivity. There are a few tricks to customise your UI with HTML, CSS and make Plotly work on the customised UI.\\n\\nThe summary of the steps are:\\n\\nCreate the app with ui.R and server.R first.\\nRun it in the browser (Run App and Open in the browser) from R Studio and get HTML code by viewing the page source from the browser\\nCreate a new folder structure that allows you to import your own images, CSS and Javascript.\\nAdd images, CSS and Javascript files in the respective folder.\\nEdit the html file and save it as index.html in the app folder.\\nDeploy\\n\\n**Steps**\\n\\n(1) Create the app with ui.R, server.R, and global.R. I recommend to use global.R for package import. It makes package management easy. The basic UI look like this.\\n\\n`ui.R`\\n\\n```R\\nshinyUI(fluidPage(\\n  titlePanel(\\"Diamond Price\\"),\\n  sidebarLayout(\\n    sidebarPanel(\\n      numericInput(\'carat\', \'Enter Carat\', min=0.1, max=5.0, step=0.1, value=1.0),\\n      selectInput(\'cut\', \'Choose Cut\', c(\'Ideal\'=\'Ideal\', \\"Premium\\"=\\"Premium\\", \\"Very Good\\"=\\"Very Good\\", \\"Good\\"=\\"Good\\", \\"Fair\\"=\\"Fair\\"),selected=\'Premium\'),\\n      selectInput(\'color\', \'Choose Color\', c(\\"J\\"=\\"J\\", \\"I\\"=\\"I\\", \\"H\\"=\\"H\\", \\"G\\"=\\"G\\", \\"F\\"=\\"F\\", \\"E\\"=\\"E\\", \\"D\\"=\\"D\\"),selected=\'H\'),\\n      selectInput(\'clarity\', \'Choose Clarity\', c(\\"IF\\"=\\"IF\\", \\"vvS1\\"=\\"vvS1\\", \\"vvs2\\"=\\"vvs2\\", \\"vs1\\"=\\"vs1\\", \\"vs2\\"=\\"vs2\\", \\"SI1\\"=\\"SI1\\", \\"SI2\\"=\\"SI2\\", \\"I1\\"=\\"I1\\"),selected=\'VS1\')\\n    ),\\n    mainPanel(\\n      h2(\\"Predicted Diamond Price\\"),\\n      textOutput(\\"predicted\\"),\\n      h2(\\"Plot Output\\"),\\n      plotlyOutput(\'plot\')\\n    )\\n  )\\n))\\n```\\n\\n`server.R`\\n\\n```R\\nshinyServer(function(input, output){\\n\\n  data(diamonds)\\n\\n  output$plot <- renderPlotly({\\n\\n    carat_val = input$carat\\n    cut_val = input$cut\\n    color_val = input$color\\n    clarity_val = input$clarity\\n\\n    df <- subset(diamonds, cut==cut_val & color==color_val & clarity==clarity_val)\\n\\n    model <- loess(price ~ carat, df)\\n    predicted_price <- predict(model, newdata=data.frame(carat=carat_val))[[1]]\\n\\n    output$predicted <- renderText({paste(\\"$\\",round(predicted_price))})\\n\\n    plot <- ggplot(data=df, aes(x=carat, y=price, alpha=0.5)) + geom_point(color=\\"red\\") + geom_smooth(method=\\"loess\\", color=\'green\', alpha=0.5, size=0.7, se=F) +\\n      geom_point(x=carat_val, y=predicted_price, shape=3, color=\'blue\', fill=\'blue\', size=2.5, alpha=0.5) + ylab(\\"$\\")\\n\\n    ggplotly(plot)\\n  })\\n})\\n```\\n\\nglobal.R\\n\\n```R\\nlibrary(shiny)\\n\\nif(require(UsingR)){\\n  print(\'Loading UsingR\')\\n} else {\\n  install.packages(\\"UsingR\\")\\n  if(require(UsingR)){\\n    print(\'Loading UsingR\')\\n  } else{\\n    \'Failed to install UsingR\'\\n  }\\n}\\n\\nif(require(ggplot2)){\\n  print(\'Loading ggplot2\')\\n} else {\\n  install.packages(\\"ggplot2\\")\\n  if(require(ggplot2)){\\n    print(\'Loading ggplot2\')\\n  } else{\\n    \'Failed to install ggplot2\'\\n  }\\n}\\n\\nif(require(plotly)){\\n  print(\'Loading plotly\')\\n} else {\\n  install.packages(\'plotly\')\\n  if(require(plotly)){\\n    print(\'Loading plotly\')\\n  } else {\\n    \'Failed to install plotly\'\\n  }\\n}\\n```\\n\\n(2) Run the app and view the source code in the browser and get the source code and save it as index.html.\\n\\n(3) Create a new folder structure. The app folder name becomes your app name when you deploy it to the server. In this case, I have an app folder called `diamondploty`. Within the main folder, create the www folder. Inside it, create img, plotly and style folder.\\n\\n```bash\\n- www\\n  - img\\n  - plotly\\n  - style\\n```\\n\\n(4) I have two header images for dynamic content. When it is viewed on mobile screen, the header image changes (you can try to scale down the browser width to see if the header image changes).\\n\\nFor CSS, I grabbed a customised bootstrap theme from . You can choose whichever theme you want to use and download it. Save the file in the style folder. This CSS will replace the default CSS in Shiny.\\n\\nTo make Plotly work with custom HTML, you need to add 3 Javascript files and 1 css file in the plotly folder. If you are using ui.R, you don\u2019t need to import them. But, once you start using custom HTML, these files are required.\\n\\n```text\\nhtmlwidgets.js\\nplogly.js\\nplotly-htmlwidgets.css\\nplotly-lastes.min\\n```\\n\\nYou will find these files from the R library folder. Check your library path with .libPath(). You can either manually copy these files or use copy command from cmd or Unix equivalent. Make sure to copy them instead of moving them (mistake I have made so many times)!\\n\\n```bash\\ncopy <app folder>\\\\www\\\\plotly <R lib path>\\\\htmlwidgets\\\\www\\\\htmlwidgets.js\\ncopy <app folder>\\\\www\\\\plotly <R lib path>\\\\plotly\\\\htmlwidgets\\\\plotly.js\\ncopy <app folder>\\\\www\\\\plotly <R lib path>\\\\plotly\\\\htmlwidgets\\\\lib\\\\plotly-latest.min.js\\ncopy <app folder>\\\\www\\\\plotly <R lib path>\\\\plotly\\\\htmlwidgets\\\\lib\\\\plotly-htmlwidgets.css\\n```\\n\\nYou can remove ui.R and keep both global.R and server.R. In the end, the app folder should look like this.\\n\\n```\\n--\\n  - global.R\\n  - server.R\\n- www\\n  - index.html\\n  - img\\n    - diamong.jpg\\n    - diamond_small.jpg\\n  - plotly\\n    - htmlwidgets.js\\n    - plotly-htmlwidgets.css\\n    - plotly-latest.min.js\\n    - plotly.js\\n  - style\\n    - bootstrap.css\\n```\\n\\n(5) Move the html file that you made in Step 2 into www folder. First of all, you need to import files in the plotly folder in the head section as below. In the same way, you can import your own custom Javascript.\\n\\n```html\\n<script src=\\"plotly/htmlwidgets.js\\"><\/script>\\n<link href=\\"plotly/plotly-htmlwidgets.css\\" rel=\\"stylesheet\\" />\\n<script src=\\"plotly/plotly-latest.min.js\\"><\/script>\\n<script src=\\"plotly/plotly.js\\"><\/script>\\n```\\n\\nTo import your custom css file, replace the default css bootstrap link element with your own.\\n\\n```html\\n\x3c!-- <link href=\\"shared/bootstrap/css/bootstrap.min.css\\" rel=\\"stylesheet\\" /> --\x3e\\n<link href=\\"style/bootstrap.css\\" rel=\\"stylesheet\\" />\\n```\\n\\nIn the head section, you can also add your own style and java script.\\n\\nShiny UI uses bootsrap. The default class for the page div is set to \u201ccontainer-fluid\u201d. This is the reason why you see the app wide. I changed this class to \u201ccontainer\u201d so that I can have the app uses better looking width.\\n\\n```html\\n<div class=\\"container\\" style=\\"height:100%;\\"></div>\\n```\\n\\nFor the rest, I updated HTML as I see fit. For example, I am making buttons more exciting by adding btn class to utilise bootstrap. I also added links for terms definition. I also have my own custom CSS and Javascript in the head section.\\n\\n(6) Once you are happy with the look, you can deploy it. Deployment is the easiest part of Shiny. The rsconnect package will take care of everything. You can get the authentication credentials from the website. You can either run code below or hit Publish on R Studio. You can check deployment instruction for further details.\\n\\n```R\\ninstall.packages(\'rsconnect\')\\nlibrary(rsconnect)\\nrsconnect::setAccountInfo(name=<>;, token=<>;, secret=<>;)\\ndeployApp()\\n```\\n\\nNow you have your own shiny Shiny App.\\n\\nEpic!"}]}}')}}]);