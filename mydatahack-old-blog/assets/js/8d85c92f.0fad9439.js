"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[8242],{83117:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>_,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var r=n(74848),o=n(28453);const s={sidebar_position:30},a="How To Get Twitter Data With Python",i={id:"data-ingestion/twitter-py",title:"How To Get Twitter Data With Python",description:"In this post, we will discuss how to use Python to grab publicly available Twitter post data (from any user you specify) and convert it into a tabular format so that we can analyse the data through Excel or insert them into a relational database. Python has a package that is a wrapper around the Twitter API (python-twitter). The package is easy to use and works fine. In this post, I used the generic requests package to make API calls to the endpoint for timeline.",source:"@site/docs/data-ingestion/30.twitter-py.md",sourceDirName:"data-ingestion",slug:"/data-ingestion/twitter-py",permalink:"/mydatahack-old-blog/mydatahack-old-blog/data-ingestion/twitter-py",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:30,frontMatter:{sidebar_position:30},sidebar:"tutorialSidebar",previous:{title:"How To Get Facebook Data With Python",permalink:"/mydatahack-old-blog/mydatahack-old-blog/data-ingestion/facebook-py"},next:{title:"Infrastructure",permalink:"/mydatahack-old-blog/mydatahack-old-blog/category/infrastructure"}},_={},l=[];function c(e){const t={code:"code",h1:"h1",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.h1,{id:"how-to-get-twitter-data-with-python",children:"How To Get Twitter Data With Python"}),"\n",(0,r.jsx)(t.p,{children:"In this post, we will discuss how to use Python to grab publicly available Twitter post data (from any user you specify) and convert it into a tabular format so that we can analyse the data through Excel or insert them into a relational database. Python has a package that is a wrapper around the Twitter API (python-twitter). The package is easy to use and works fine. In this post, I used the generic requests package to make API calls to the endpoint for timeline."}),"\n",(0,r.jsx)(t.p,{children:"Twitter API"}),"\n",(0,r.jsx)(t.p,{children:"Twitter API is a REST-based API and use both OAuth1.0 and OAuth2.0 for authentication depending on the application you are making. We use OAuth1.0 here."}),"\n",(0,r.jsx)(t.p,{children:"To obtain the API credentials, you need your Twitter account and go to the Twitter Apps page. Then, click \u2018Create New App\u2019 and enter Name, Description and Website. Callback URL can be left empty. You can see Consumer Key (API Key) and Consumer Secret (API Secret) once you create an app. You will need to generate access token and secret through the UI by clicking the button."}),"\n",(0,r.jsx)(t.p,{children:"How the code works"}),"\n",(0,r.jsx)(t.p,{children:"It takes 9 arguments, including the API credentials, Twitter user (whom you want to get the data from), number of records to ingest, directory path (without file name) to create json response files, dir_path/filename for csv files. The program creates one timeline table for tweet and another table with the latest user information."}),"\n",(0,r.jsx)(t.p,{children:"The code runs in both Python 2.7 and 3. The major challenge for backward compatibility is the way Python 2.7 handles Unicode (it is quite different from Python 3). Therefore, you need to uncomment or replace a few lines as indicated in the code."}),"\n",(0,r.jsx)(t.p,{children:"Example call"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-bash",children:"python twitter_timeline_scraper.py <api_key> <pi_secret> <access_token> <access_token_secret>\n<screen_name (e.g. CocaCola)> <record_no (e.g. 3000)> <json_path (e.g. /tmp/twitter/raw/)>\n<timeline_file_path (e.g. /tmp/twitter/timeline.csv)> <user_file_path (e.g. /tmp/twitter/user.csv)>\n"})}),"\n",(0,r.jsx)(t.p,{children:"Key Points"}),"\n",(0,r.jsx)(t.p,{children:"For the user_timeline data, 200 is the maximum number of the record you can retrieve at a time. To obtain more than 200 records, we need to use the max_id parameter to specify which record is the first for each iteration."}),"\n",(0,r.jsx)(t.p,{children:"The possibly_sensitive field may be missing. Therefore, use the key error exception to assign no value in case it is missing."}),"\n",(0,r.jsx)(t.p,{children:"Enjoy!"}),"\n",(0,r.jsx)(t.p,{children:"Python Code"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-python",children:'import requests_oauthlib\nimport requests\nimport os\nimport json\nimport numpy as np\nimport sys\n\n\'\'\'\n# for Python 2.7\nreload(sys)\nsys.setdefaultencoding(\'utf8\')\n\'\'\'\n\nclass TwitterScraper:\n    \'\'\'This class is used to scrape Twitter Timeline\'\'\'\n\n    def __init__(self, api_key, api_secret, access_token, access_token_secret):\n\n        self.api_key = api_key\n        self.api_secret = api_secret\n        self.access_token = access_token\n        self.access_token_secret = access_token_secret\n\n    @staticmethod\n    def export_with_url(auth, url, json_path, json_file_name):\n        \'\'\'Take url string and auth object and return json object & generate json files\'\'\'\n        r = requests.get(url, auth=auth)\n        if r.status_code == 200:\n            print("Connection Successful. Status Code: {}".format(r.status_code))\n\n            # Convert to json object\n            data = json.loads(r.text)\n            # Write Json file\n            f = open(json_path + json_file_name, "w")\n            f.write(json.dumps(data, indent=4))\n            # Return json object\n            return data\n        else:\n            print("Connection Unsuccessful. Status Code: {}".format(r.status_code))\n            quit()\n\n    @staticmethod\n    def get_max_id(json_data):\n        \'\'\'Return max_id from twitter timeline data\'\'\'\n        lst = []\n        for i in range(len(json_data)):\n            id = json_data[i]["id"]\n            lst.append(id)\n        array = np.array(lst)\n        max_id = min(array)\n        print ("The oldest Id in the record set is: {}".format(str(max_id)))\n        return max_id\n\n    def get_data(self, screen_name, record_no, json_path):\n        \'\'\'Return a list of json object depending on the no of records\'\'\'\n        counter = 0\n        max_id = None\n        data_list = []\n        auth = requests_oauthlib.OAuth1(self.api_key, self.api_secret,\\\n        self.access_token, self.access_token_secret)\n\n        if int(record_no) <= 200:\n            url = \'https://api.twitter.com/1.1/statuses/\\\n            user_timeline.json?screen_name={}&count={}\'\\\n            .format(screen_name, record_no)\n            data = self.export_with_url(auth, url, json_path, "timeline.json")\n            data_list.append(data)\n            return data_list\n\n        else:\n            while(counter < int(record_no)):\n                file_name = "timeline_{}.json".format(counter)\n                if max_id is None:\n                    url = \'https://api.twitter.com/1.1/statuses/\\\n                    user_timeline.json?screen_name={}&count=200\'.format(screen_name)\n                    data = self.export_with_url(auth, url, json_path, file_name)\n                    data_list.append(data)\n                    max_id = self.get_max_id(data) - 1\n                    counter += 200\n\n                else:\n                    url = \'https://api.twitter.com/1.1/statuses/\\\n                    user_timeline.json?screen_name={}&count=200&max_id={}\'\\\n                    .format(screen_name, max_id)\n                    data = self.export_with_url(auth, url, json_path, file_name)\n                    data_list.append(data)\n                    max_id = self.get_max_id(data) - 1\n                    counter += 200\n            return data_list\n\n\n    def create_timeline_table(self, data_list, file_path):\n        \'\'\'Generate timeline csv file\'\'\'\n\n        # timeline = open(file_path, "w") # for Python 2.7\n        timeline = open(file_path, \'w\', encoding=\'utf-8\')\n\n        header = \'"contributors","truncated","text","is_quote_status","in_reply_to_status_id",\\\n        "id","favorite_count","source","retweeted","coordinates",\\\n        "in_reply_to_screen_name","in_reply_to_user_id","retweet_count","id_str","favorited",\\\n        "geo","in_reply_to_user_id_str","possibly_sensitive","lang","created_at",\\\n        "in_reply_to_status_id_str","place"\'\n\n        timeline.write(header + \'\\n\')\n\n        for data in data_list:\n            for i in range(len(data)):\n                row_string = ""\n                row = data[i]\n                contributors = row["contributors"] or \'\'\n                truncated = str(row["truncated"])\n                # for Python 2.7\n                # text = row["text"].encode(\'latin1\', \'ignore\').replace(\'\\n\', \' \')\n                text = row["text"].replace(\'\\n\', \' \')\n                is_quote_status = str(row["is_quote_status"])\n                in_reply_to_status_id = row["in_reply_to_status_id"] or \'\'\n                id = str(row["id"])\n                favorite_count = str(row["favorite_count"])\n                source = row["source"].encode(\'latin1\', \'ignore\') or \'\'\n                retweeted = str(row["retweeted"])\n                coordinates = str(row["coordinates"])\n                in_reply_to_screen_name = row["in_reply_to_screen_name"] or \'\'\n                in_reply_to_user_id = str(row["in_reply_to_user_id"]) or \'\'\n                retweet_count = str(row["retweet_count"])\n                id_str = row["id_str"]\n                favorited = str(row["favorited"])\n                geo = row["geo"] or \'\'\n                in_reply_to_user_id_str = row["in_reply_to_user_id_str"] or \'\'\n                try:\n                    possibly_sensitive = str(row["possibly_sensitive"])\n                except KeyError:\n                    possibly_sensitive = \'\'\n                lang = row["lang"]\n                created_at = row["created_at"]\n                in_reply_to_status_id_str = row["in_reply_to_status_id_str"] or \'\'\n                place = row["place"] or \'\'\n\n                row_string = \'"{0}","{1}","{2}","{3}","{4}","{5}","{6}","{7}","{8}","{9}",\\\n                {10},"{11}","{12}","{13}","{14}","{15}","{16}","{17}","{18}","{19}","{20}","{21}"\\n\'\\\n                .format(contributors,truncated,text,is_quote_status,in_reply_to_status_id,\\\n                id,favorite_count,source,retweeted,coordinates,in_reply_to_screen_name,\\\n                in_reply_to_user_id,retweet_count,\\\n                id_str,favorited,geo,in_reply_to_user_id_str,possibly_sensitive,\\\n                lang,created_at,in_reply_to_status_id_str,place)\n\n                timeline.write(row_string)\n\n        timeline.close()\n        print("Generated timeline.csv")\n\n    def create_user_table(self, data_list, file_path):\n        \'\'\'Create user table from user info in the first timeline\'\'\'\n        data = data_list[0]\n        # user_csv = open(file_path, "w") # for Python 2.7\n        user_csv = open(file_path, \'w\', encoding=\'utf-8\')\n        user = data[0]["user"]\n        user_string = \'\'\n        follow_request_sent = str(user["follow_request_sent"])\n        has_extended_profile = str(user["has_extended_profile"])\n        profile_use_background_image = str(user["profile_use_background_image"])\n        default_profile_image = str(user["default_profile_image"])\n        id = str(user["id"])\n        profile_background_image_url_https = user["profile_background_image_url_https"]\n        verified = str(user["verified"])\n        translator_type = user["translator_type"] or \'\'\n        profile_text_color = user["profile_text_color"]\n        profile_image_url_https = user["profile_image_url_https"]\n        profile_sidebar_fill_color = user["profile_sidebar_fill_color"]\n        followers_count = str(user["followers_count"])\n        profile_sidebar_border_color = user["profile_sidebar_border_color"]\n        id_str = user["id_str"]\n        profile_background_color = user["profile_background_color"]\n        listed_count = str(user["listed_count"])\n        is_translation_enabled = str(user["is_translation_enabled"])\n        utc_offset = str(user["utc_offset"])\n        statuses_count = str(user["statuses_count"])\n        description = user["description"]\n        friends_count = str(user["friends_count"])\n        location = user["location"]\n        profile_link_color = user["profile_link_color"]\n        profile_image_url = user["profile_image_url"]\n        following = str(user["following"])\n        geo_enabled = str(user["geo_enabled"])\n        profile_banner_url = user["profile_banner_url"]\n        profile_background_image_url = user["profile_background_image_url"]\n        screen_name = user["screen_name"]\n        lang = user["lang"]\n        profile_background_tile = str(user["profile_background_tile"])\n        favourites_count = str(user["favourites_count"])\n        name = user["name"]\n        notifications = str(user["notifications"])\n        url = user["url"]\n        created_at = user["created_at"]\n        contributors_enabled = str(user["contributors_enabled"])\n        time_zone = user["time_zone"]\n        protected = str(user["protected"])\n        default_profile = str(user["default_profile"])\n        is_translator = str(user["is_translator"])\n\n        user_string = \'"{0}","{1}","{2}","{3}","{4}","{5}","{6}","{7}","{8}","{9}","{10}",\\\n        "{11}","{12}","{13}","{14}","{15}","{16}","{17}","{18}","{19}","{20}",\\\n        "{21}","{22}","{23}","{24}","{25}","{26}","{27}","{28}","{29}","{30}",\\\n        "{31}","{32}","{33}","{34}","{35}","{36}","{37}","{38}","{39}","{40}"\\n\'\\\n        .format(follow_request_sent,has_extended_profile,profile_use_background_image,default_profile_image,\\\n        id,profile_background_image_url_https,verified,translator_type,profile_text_color,profile_image_url_https,\\\n        profile_sidebar_fill_color,followers_count,profile_sidebar_border_color,id_str,profile_background_color,\\\n        listed_count,is_translation_enabled,utc_offset,statuses_count,description,friends_count,location,\\\n        profile_link_color,profile_image_url,following,geo_enabled,profile_banner_url,profile_background_image_url,\\\n        screen_name,lang,profile_background_tile,favourites_count,name,notifications,url,created_at,\\\n        contributors_enabled,time_zone,protected,default_profile,is_translator)\n\n        header = \'"follow_request_sent","has_extended_profile","profile_use_background_image",\\\n        "default_profile_image","id","profile_background_image_url_https","verified","translator_type",\\\n        "profile_text_color","profile_image_url_https","profile_sidebar_fill_color","followers_count",\\\n        "profile_sidebar_border_color","id_str","profile_background_color","listed_count",\\\n        "is_translation_enabled","utc_offset","statuses_count","description","friends_count","location",\\\n        "profile_link_color","profile_image_url","following","geo_enabled","profile_banner_url",\\\n        "profile_background_image_url","screen_name","lang","profile_background_tile","favourites_count",\\\n        "name","notifications","url","created_at","contributors_enabled","time_zone","protected",\\\n        "default_profile","is_translator"\'\n        user_csv.write(header + \'\\n\')\n\n        # print user_string\n        user_csv.write(user_string)\n        user_csv.close()\n        print ("Generated user.csv")\n\nif __name__ == "__main__":\n\n    # Get variable from arg input\n    api_key = sys.argv[1]\n    api_secret = sys.argv[2]\n    access_token = sys.argv[3]\n    access_token_secret = sys.argv[4]\n    screen_name = sys.argv[5]\n    record_no = sys.argv[6]\n    json_path = sys.argv[7]\n    timeline_file_path = sys.argv[8]\n    user_file_path = sys.argv[9]\n\n    twitter = TwitterScraper(api_key, api_secret, access_token, access_token_secret)\n    data_list = twitter.get_data(screen_name, record_no, json_path)\n    twitter.create_timeline_table(data_list, timeline_file_path)\n    twitter.create_user_table(data_list, user_file_path)\n'})}),"\n",(0,r.jsx)(t.p,{children:"(2017-11-05)"})]})}function u(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>a,x:()=>i});var r=n(96540);const o={},s=r.createContext(o);function a(e){const t=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),r.createElement(s.Provider,{value:t},e.children)}}}]);