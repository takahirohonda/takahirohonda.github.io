"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[9108],{46385:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var a=t(74848),i=t(28453);const r={slug:"data-science/deep-learning//introduction-to-dense-net-with-keras/",title:"Introduction to Dense Layers for Deep Learning with Keras",tags:["Data Science","Deep Learning","Dense Net","Iris","Keras"]},s=void 0,o={permalink:"/data-science/data-science/deep-learning/introduction-to-dense-net-with-keras/",source:"@site/data-science/deep-learning/2018-05-07-dense-layer-keras.md",title:"Introduction to Dense Layers for Deep Learning with Keras",description:"The most basic neural network architecture in deep learning is the dense neural networks consisting of dense layers (a.k.a. fully-connected layers).",date:"2018-05-07T00:00:00.000Z",tags:[{label:"Data Science",permalink:"/data-science/tags/data-science"},{label:"Deep Learning",permalink:"/data-science/tags/deep-learning"},{label:"Dense Net",permalink:"/data-science/tags/dense-net"},{label:"Iris",permalink:"/data-science/tags/iris"},{label:"Keras",permalink:"/data-science/tags/keras"}],readingTime:4.355,hasTruncateMarker:!0,authors:[],frontMatter:{slug:"data-science/deep-learning//introduction-to-dense-net-with-keras/",title:"Introduction to Dense Layers for Deep Learning with Keras",tags:["Data Science","Deep Learning","Dense Net","Iris","Keras"]},unlisted:!1,prevItem:{title:"Building AlexNet with TensorFlow and Running it with AWS SageMaker",permalink:"/data-science/data-science/deep-learning/building-alexnet-with-tensorflow-and-running-it-with-aws-sagemaker"},nextItem:{title:"Introduction to Dense Layers for Deep Learning with TensorFlow",permalink:"/data-science/data-science/deep-learning/introduction-to-dense-net-with-tensorflow"}},l={authorsImageUrls:[]},c=[];function d(e){const n={code:"code",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:"The most basic neural network architecture in deep learning is the dense neural networks consisting of dense layers (a.k.a. fully-connected layers)."}),"\n",(0,a.jsx)(n.p,{children:"In this layer, all the inputs and outputs are connected to all the neurons in each layer. Keras is the high-level APIs that runs on TensorFlow (and CNTK or Theano) which makes coding easier. Writing code in the low-level TensorFlow APIs is difficult and time-consuming. When I build a deep learning model, I always start with Keras so that I can quickly experiment with different architectures and parameters. Then, move onto TensorFlow to further fine tune it. When it comes to the first deep learning code, I think Dense Net with Keras is a good place to start. So, let\u2019 get started."}),"\n",(0,a.jsx)(n.p,{children:"Dataset"}),"\n",(0,a.jsx)(n.p,{children:"Deep learning 101 dataset is the classic MNIST, which is used for hand-written digit recognition. With the code below, you can certainly use MNIST."}),"\n",(0,a.jsx)(n.p,{children:"In this example, I am using the machine learning classic Iris dataset. The dataset will be imported from a csv file. This gives you an idea on how to import csv into the deep learning model, rather than porting example data from the build-in package."}),"\n",(0,a.jsx)(n.p,{children:"Deep learning on Iris certainly feels like cracking a nut with a sledge hammer. However, you can apply the knowledge and the same code to more appropriate datasets once you understand how it works."}),"\n",(0,a.jsx)(n.p,{children:"There are many ways to get a csv version of Iris. I got it from R."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-r",children:"path = '/tmp/iris.csv'\ndata(iris)\nwrite.csv(iris, path,row.names=FALSE)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Steps"}),"\n",(0,a.jsx)(n.p,{children:"(1) Import required modules"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import numpy as np\nnp.random.seed(21)\nimport pandas as pd\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras import regularizers\nfrom keras.optimizers import SGD\nfrom sklearn.model_selection import train_test_split\n"})}),"\n",(0,a.jsx)(n.p,{children:"(2) Preprocessing"}),"\n",(0,a.jsx)(n.p,{children:"Both Keras and TensorFlow takes numpy arrays as features and classes. When the prediction is categorical, the outcome needs to be one-hot encoded (see one-hot encoding explanation from the Kaggle\u2019s website). For one-hot encoding, the class needs to be indexes (starting from 0). Once they are transformed, you can use keras.utils.to_categorical() for conversion."}),"\n",(0,a.jsx)(n.p,{children:"It uses sklearn.model_selection.train_test_split to create training and test dataset."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"iris = pd.read_csv('/tmp/iris.csv')\nprint(iris.head(5))\nprint(iris.Species.unique())\niris['Species'] = np.where(iris['Species']=='setosa', 0,\n               np.where(iris['Species']=='versicolor', 1,\n                       np.where(iris['Species']=='virginica', 2, 3)))\nprint(iris[0:3])\nprint(iris.Species.unique())\n\n# convert df to features and targets\nfeature_cols = ['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']\nfeatures = iris[feature_cols].values\ntarget = iris.Species.values\n\n# Normalise features\nfeatures_norm = (features-np.min(features, axis=0)) / \\\n(np.max(features, axis=0)-np.min(features, axis=0))\nprint('Checking normalised features: \\n{}'.format(features_norm[0:3]))\n\n# Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(features_norm, target, test_size=0.2)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)\nprint(X_train[3])\nprint(y_train[3])\n\n# Convert y into one hot encoded variables\nn_classes = 3\ny_train_e = keras.utils.to_categorical(y_train, n_classes)\ny_test_e = keras.utils.to_categorical(y_test, n_classes)\nprint(y_train_e[0:3])\n"})}),"\n",(0,a.jsx)(n.p,{children:"(3) Design Networks"}),"\n",(0,a.jsx)(n.p,{children:"I am using the sequential model with 2 fully-connected layers. ReLU is more popular in many deep neural networks, but I am using Tanh for activation because it actually performed better. You almost never use Sigmoid because it is slow to train. Softmax is used for the output layer."}),"\n",(0,a.jsx)(n.p,{children:"Adding the 3rd layer degrades the performance. This makes sense as the data set is fairly simple. I am using Dropout to reduce over-fitting. L2 regularizer can be used. But, it did not perform well in this case and I commented out the line."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"model = Sequential()\n# for l2 regularizer. this doesn't perform as good as dropouts\n# model.add(Dense(32, activation='relu', input_dim=4, kernel_regularizer=regularizers.l2(0.01)))\n# tanh is better than relu in this case\nmodel.add(Dense(32, activation='tanh', input_shape=(4,)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))\n"})}),"\n",(0,a.jsx)(n.p,{children:"(4) Model Compilation"}),"\n",(0,a.jsx)(n.p,{children:"You need to define the loss function, optimizer and evaluation metrics. Cross-entropy is the gold standard for the cost function. You will almost never use quadratic. On the other hand, there are many options for optimisers. In this example, I have Adam as well as SGD with learning rate of 0.01. Both works fine."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Compared to mean_squared_error, cross entropy does faster learning,\n# model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])\n# We can use SGD with a specific learning rate for optimizer\n# model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"})}),"\n",(0,a.jsx)(n.p,{children:"(5) Execution"}),"\n",(0,a.jsx)(n.p,{children:"The testing accuracy goes up to 96.7% after 120 epochs. With this dataset, a regular machine learning algorithm like random forest or logistic regression can achieve the similar results. The first rule of deep learning is that if the simpler machine learning algorithm can achieve the same outcome, use machine learning and look for a more complicated problem. Here, the purpose is to learn the actual programming process so that you can apply it to more complex problems."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"epoch_n = 1000\nmodel.fit(X_train, y_train_e, batch_size=60, epochs=epoch_n, verbose=1,\\\n validation_data=(X_test, y_test_e))\n"})}),"\n",(0,a.jsx)(n.p,{children:"Next Step"}),"\n",(0,a.jsx)(n.p,{children:"(1) Try using MNIST dataset on this code."}),"\n",(0,a.jsx)(n.p,{children:"MNIST is included in Keras and you can imported it as keras.datasets.mnist. It\u2019s already split into training and test datasets. In preprocessing, you need to flatten the data (from 28 x 28 to 784) and convert y into one-hot encoded values. Here is the code to process the data."}),"\n",(0,a.jsx)(n.p,{children:"(2) Replicate the same code with low-level TensorFlow code."}),"\n",(0,a.jsx)(n.p,{children:"TenorFlow is much more complicated than Keras. The way to code is quite unique. It will be difficult at first, but it will be worthwhile."}),"\n",(0,a.jsx)(n.p,{children:"For the actual code example, go to Introduction to Dense Net with TensorFlow."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var a=t(96540);const i={},r=a.createContext(i);function s(e){const n=a.useContext(r);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);