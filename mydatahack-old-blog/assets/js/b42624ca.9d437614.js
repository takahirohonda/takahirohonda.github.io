"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[311],{65375:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var n=t(74848),o=t(28453);const i={slug:"data-science/infra//how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline",title:"How To Deploy Spark Applications In AWS With EMR and Data Pipeline",tags:["Data Science","Tools and Infrastructure","AWS","Data Pipeline","EMR","Spark"]},s=void 0,r={permalink:"/mydatahack-old-blog/data-science/data-science/infra/how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline",source:"@site/data-science/infra/2018-01-04-spark-emr-aws.md",title:"How To Deploy Spark Applications In AWS With EMR and Data Pipeline",description:"Once you create an awesome data science application, it is time for you to deploy it. There are many ways to productionise them. The focus here is deploying Spark applications by using the AWS big data infrastructure. From my experience with the AWS stack and Spark development, I will discuss some high level architectural view and use cases as well as development process flow.",date:"2018-01-04T00:00:00.000Z",tags:[{label:"Data Science",permalink:"/mydatahack-old-blog/data-science/tags/data-science"},{label:"Tools and Infrastructure",permalink:"/mydatahack-old-blog/data-science/tags/tools-and-infrastructure"},{label:"AWS",permalink:"/mydatahack-old-blog/data-science/tags/aws"},{label:"Data Pipeline",permalink:"/mydatahack-old-blog/data-science/tags/data-pipeline"},{label:"EMR",permalink:"/mydatahack-old-blog/data-science/tags/emr"},{label:"Spark",permalink:"/mydatahack-old-blog/data-science/tags/spark"}],readingTime:5.09,hasTruncateMarker:!0,authors:[],frontMatter:{slug:"data-science/infra//how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline",title:"How To Deploy Spark Applications In AWS With EMR and Data Pipeline",tags:["Data Science","Tools and Infrastructure","AWS","Data Pipeline","EMR","Spark"]},unlisted:!1,prevItem:{title:"How To Save Machine Learning Models In R",permalink:"/mydatahack-old-blog/data-science/data-science/machine-learning/how-to-save-machine-learning-models-in-r"},nextItem:{title:"How To Do Sentiment Analysis On Your Favourite Book With R",permalink:"/mydatahack-old-blog/data-science/data-science/visualisation/how-to-do-sentiment-analysis-on-your-favourite-book-with-r"}},c={authorsImageUrls:[]},l=[];function d(e){const a={img:"img",p:"p",...(0,o.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.p,{children:"Once you create an awesome data science application, it is time for you to deploy it. There are many ways to productionise them. The focus here is deploying Spark applications by using the AWS big data infrastructure. From my experience with the AWS stack and Spark development, I will discuss some high level architectural view and use cases as well as development process flow."}),"\n",(0,n.jsx)(a.p,{children:"AWS offers a solid ecosystem to support Big Data processing and analytics, including EMR, S3, Redshift, DynamoDB and Data Pipeline. If you have a Spark application that runs on EMR daily, Data Pipleline enables you to execute it in the serverless manner."}),"\n",(0,n.jsx)(a.p,{children:"The serverless architecture doesn\u2019t strictly mean there is no server. When the code is running, you of course need a server to run it. The main difference from the traditional way is that you store your codes and models in a repository, launch the server only during execution and close it as soon as it finishes. In this architecture, you only pay for the cost for the length of code execution. The architecture is often used for real-time data streaming or integration. AWS Lambda and Kinesis are good examples."}),"\n",(0,n.jsx)(a.p,{children:"What is good about Data Pipeline?"}),"\n",(0,n.jsx)(a.p,{children:"Data Pipleline is a great tool to use the serverless architecture for batch jobs that run on schedule. You can design the pipeline job to control resources, workflow, execution dependency, scheduling and error handling without the hustle of provisioning and managing servers and the cost of keeping them running all the time."}),"\n",(0,n.jsx)(a.p,{children:"Another advantage is that you can create a job with parameters (e.g. DB connection URLs, credentials, target schema/table). Data Pipeline jobs are basically JSON files. You can easily export and edit it. With parameters, you can easily promote jobs from the development environment to the production as it is a matter of importing the JSON file from dev to prod (which of course can be coded up for automation)."}),"\n",(0,n.jsx)(a.p,{children:"I also found that debugging and updating Spark codes or models became simpler. We can simply update the repo without touching the pipeline."}),"\n",(0,n.jsx)(a.p,{children:"The cost of running Data Pipeline jobs is affordable. For a low frequency job (once a day), it costs 60 cent per month as of today. See the link for the pricing information. Note that the cost of running EMR will be charged at hourly rate on top of the running cost of pipelines."}),"\n",(0,n.jsx)(a.p,{children:"Let\u2019s have a look at the use cases."}),"\n",(0,n.jsx)(a.p,{children:"Use Case 1"}),"\n",(0,n.jsx)(a.p,{children:"Sourcing the data from different databases (application database, data lake and data warehouse) and joining them prior to running the algorithm.\nThe output needs to be presented in a BI tool.\nPrerequisite"}),"\n",(0,n.jsx)(a.p,{children:"Save your Sqoop code (as .sh), Spark code and model to Bitbucet or GitHub (or S3, which is less preferable option)."}),"\n",(0,n.jsx)(a.p,{children:"Solution"}),"\n",(0,n.jsx)(a.p,{children:"Within the Data Pipeline, you can create a job to do below:"}),"\n",(0,n.jsx)(a.p,{children:"Launch a ERM cluster with Sqoop and Spark. Source the Sqoop code to EMR and execute it to move the data to S3.\nSource the Spark code and model into EMR from a repo (e.g. Bitbucket, GitHub, S3). Execute the code, which transform the data and create output according to the pre-developed model.\nMove the output of the Spark application to S3 and execute copy command to Redshift.\nBI tools to fetch the output from Redshift for presentation.\nSqoop is a command line tool to transfer data between Hadoop and relational databases. EMR uses Hadoop for file management. So, it is the best tool to move the data from relational databases through Hadoop in EMR to S3. It is fast and easy to learn. I learned everything about Sqoop from a cookbook which you can download for free here."}),"\n",(0,n.jsx)(a.p,{children:"Below image not working with docusaurus compilation \ud83d\ude22"}),"\n",(0,n.jsx)(a.p,{children:"Use Case 2"}),"\n",(0,n.jsx)(a.p,{children:"Ingesting data into Data Lake with an ETL tool.\nTransforming data with ETL or ELT within the Redshift.\nAll the data required for the Spark application is in the data warehousing layer.\nIn the enterprise environment, it is common to have an ETL tool that manage the data ingestion and transformation. Accessing the application databases directly for analytics is not the best architectural practice, either. The first use case is suitable when you need to do data ingestion in an ad-hoc manner or cannot wait for ETL development for the sake of speedy delivery."}),"\n",(0,n.jsx)(a.p,{children:"Prerequisite"}),"\n",(0,n.jsx)(a.p,{children:"The same as Use Case 1."}),"\n",(0,n.jsx)(a.p,{children:"Solution"}),"\n",(0,n.jsx)(a.p,{children:"The figure shows Informatica as an ETL tool. There are heaps of options out there and any tool that suits your use case is fine. I compared DataStage, Informatica and Talend in the past and found Informatica best suited for the particular situation I was in. I especially liked the Redshift connector and I wrote a small review."}),"\n",(0,n.jsx)(a.p,{children:"The workflow has two parts, managed by an ETL tool and Data Pipeline."}),"\n",(0,n.jsx)(a.p,{children:"ETL Tool manages below:"}),"\n",(0,n.jsx)(a.p,{children:"ETL tool does data ingestion from source systems.\nDo ETL or ELT within Redshift for transformation.\nUnload any transformed data into S3.\nData Pipeline manages below:"}),"\n",(0,n.jsx)(a.p,{children:"Launch a cluster with Spark, source codes & models from a repo and execute them. The output is moved to S3.\nCopy data from S3 to Redshift (you can execute copy commands in the Spark code or Data Pipeline).\nThen, you can source the output into a BI tool for presentation."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"informatica",src:t(31032).A+"",width:"1264",height:"604"})}),"\n",(0,n.jsx)(a.p,{children:"Development Process Workflow"}),"\n",(0,n.jsx)(a.p,{children:"Finally, let\u2019s have a look at development process workflow. Prior to Spark application deployment, we still need to develop and test the application in an EMR cluster. In this workflow, we only launch the cluster after prototyping on the local machine with a smaller dataset. This will save money as running an EMR cluster is expensive."}),"\n",(0,n.jsx)(a.p,{children:"Once the code and models are developed, we can close the EMR cluster and move onto the serverless execution in batch. Codes and models can be source from S3 in the Data Pipeline. It is a standard practice to version control them in a git type repository. Sourcing them from a repo in Data Pipeline makes more sense."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Data Science workflow with EMR",src:t(63198).A+"",width:"1110",height:"650"})}),"\n",(0,n.jsx)(a.p,{children:"Let us know your experience with data science application deployment!"})]})}function h(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},63198:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/ds-workflow-with-emr-eab8a50459ebe8992e73c227abff7f0f.png"},31032:(e,a,t)=>{t.d(a,{A:()=>n});const n=t.p+"assets/images/informatica-8a801a27dd2eb6358318312536b15810.png"},28453:(e,a,t)=>{t.d(a,{R:()=>s,x:()=>r});var n=t(96540);const o={},i=n.createContext(o);function s(e){const a=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function r(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),n.createElement(i.Provider,{value:a},e.children)}}}]);