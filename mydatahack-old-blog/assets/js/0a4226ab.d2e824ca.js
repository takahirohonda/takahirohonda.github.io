"use strict";(self.webpackChunkmydatahack_blog_site=self.webpackChunkmydatahack_blog_site||[]).push([[6769],{65784:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var a=t(74848),s=t(28453);const o={sidebar_position:27},i="Data Engineering in S3 and Redshift with Python",r={id:"data-ingestion/s3-redshift-py",title:"Data Engineering in S3 and Redshift with Python",description:"AWS offers a nice solution to data warehousing with their columnar database, Redshift, and an object storage, S3. Python and AWS SDK make it easy for us to move data in the ecosystem.",source:"@site/docs/data-ingestion/27.s3-redshift-py.md",sourceDirName:"data-ingestion",slug:"/data-ingestion/s3-redshift-py",permalink:"/mydatahack-old-blog/docs/data-ingestion/s3-redshift-py",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:27,frontMatter:{sidebar_position:27},sidebar:"tutorialSidebar",previous:{title:"How To Get Survey Response Data From Qualtrics With Python",permalink:"/mydatahack-old-blog/docs/data-ingestion/qualtrics-python"},next:{title:"How To Get Data From Google Analytics With Python",permalink:"/mydatahack-old-blog/docs/data-ingestion/ga-py"}},c={},l=[];function d(e){const n={code:"code",h1:"h1",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"data-engineering-in-s3-and-redshift-with-python",children:"Data Engineering in S3 and Redshift with Python"}),"\n",(0,a.jsx)(n.p,{children:"AWS offers a nice solution to data warehousing with their columnar database, Redshift, and an object storage, S3. Python and AWS SDK make it easy for us to move data in the ecosystem."}),"\n",(0,a.jsx)(n.p,{children:"In this post, I will present code examples for the scenarios below:"}),"\n",(0,a.jsx)(n.p,{children:"Uploading data from S3 to Redshift\nUnloading data from Redshift to S3\nUploading data to S3 from a server or local computer\nThe best way to load data to Redshift is to go via S3 by calling a copy command because of its ease and speed. You can upload data into Redshift from both flat files and json files."}),"\n",(0,a.jsx)(n.p,{children:"You can also unload data from Redshift to S3 by calling an unload command. Boto3 (AWS SDK for Python) enables you to upload file into S3 from a server or local computer."}),"\n",(0,a.jsx)(n.p,{children:"Preparation"}),"\n",(0,a.jsx)(n.p,{children:"I usually encourage people to use Python 3. When it comes to AWS, I highly recommend to use Python 2.7. It will make your life much easier. For example, if you want to deploy a Python script in an EC2 instance or EMR through Data Pipeline to leverage their serverless architecture, it is faster and easier to run code in 2.7. The code examples are all written 2.7, but they all work with 3.x, too."}),"\n",(0,a.jsx)(n.p,{children:"You need to install boto3 and psycopg2 (which enables you to connect to Redshift)."}),"\n",(0,a.jsx)(n.p,{children:"pip install boto3\npip install psycopg2\nFinally, you need to install the AWS Command Line Interface (see Installing the AWS Command Line Interface) and configure it (see Configuring the AWS CLI) in the server you are running your program or the local machine. This is not necessary if you are running the code through Data Pipeline. This is pre-installed in the EC2 instance. Boto3 leverages the credentials stored in AWS CLI. Once AWS CLI is configured, you do not need to enter any AWS credentials in the code to move data to and from S3."}),"\n",(0,a.jsx)(n.p,{children:"Let\u2019s get down to the business!"}),"\n",(0,a.jsx)(n.p,{children:"Code Examples"}),"\n",(0,a.jsx)(n.p,{children:"Example 1: Upload a file into Redshift from S3"}),"\n",(0,a.jsx)(n.p,{children:"There are many options you can specify. In this case, the data is a pipe separated flat file. You can upload json, csv and so on. For further reference on Redshift copy command, you can start from here."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"schema = sys.argv[2]\ndbname = sys.argv[3]\nport = sys.argv[4]\nuser = sys.argv[5]\npassword = sys.argv[6]\nhost_url = sys.argv[7]\nfile_path = sys.argv[8]\naws_access_key_id = sys.argv[9]\naws_secret_access_key = sys.argv[10]\n\ndef main():\n    '''This method will unload redshift table into S3'''\n    conn_string = \"dbname='{}' port='{}' user='{}' password='{}' host='{}'\"\\\n        .format(dbname,port,user,password,host_url)\n    sql=\"\"\"copy {}.{} from '{}'\\\n        credentials \\\n        'aws_access_key_id={};aws_secret_access_key={}' \\\n        DELIMITER '|' ACCEPTINVCHARS EMPTYASNULL ESCAPE COMPUPDATE OFF;commit;\"\"\"\\\n        .format(schema, table, file_path, aws_access_key_id, aws_secret_access_key)\n\n    try:\n        con = psycopg2.connect(conn_string)\n        print(\"Connection Successful!\")\n    except:\n        print(\"Unable to connect to Redshift\")\n\n    cur = con.cursor()\n    try:\n        cur.execute(sql)\n        print(\"Copy Command executed successfully\")\n    except:\n        print(\"Failed to execute copy command\")\n    con.close()\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example 2: Unload data from Redshift into S3"}),"\n",(0,a.jsx)(n.p,{children:"In this example, the data is unloaded as gzip format with manifest file. This is the recommended file format for unloading according to AWS. Unloading also has many options and you can create a different file formats according to your requirements. For further information, you can start from here."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"schema_name = sys.argv[2]\ndbname = sys.argv[3]\nport = sys.argv[4]\nuser = sys.argv[5]\npassword = sys.argv[6]\nhost_url = sys.argv[7]\ns3_bucket_name = sys.argv[8]\naws_access_key_id = sys.argv[9]\naws_secret_access_key = sys.argv[10]\n\ndef main():\n    '''This method will unload redshift table into S3'''\n    conn_string = \"dbname='{}' port='{}' user='{}' password='{}' host='{}'\"\\\n        .format(dbname,port,user,password,host_url)\n    sql=\"\"\"UNLOAD ('select * from %s.%s') TO 's3://%s/%s/%s.csv' \\\n        credentials 'aws_access_key_id=%s;aws_secret_access_key=%s' \\\n        MANIFEST GZIP ALLOWOVERWRITE;Commit;\"\"\" \\\n        % (schema,table,s3_bucket_nameschema,table,aws_access_key_id,\\\n        aws_secret_access_key)\n\n    con = psycopg2.connect(conn_string)\n    cur = con.cursor()\n    cur.execute(sql)\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example 3: Upload files into S3 with Boto3"}),"\n",(0,a.jsx)(n.p,{children:"You need to have AWS CLI configured to make this code work. Whatever the credentials you configure is the environment for the file to be uploaded."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import boto3\nimport os\n\ndef upload_files(local_path, s3_path, bucket_name):\n    '''Search all the files from specified directory and push to S3'''\n    s3 = boto3.resource('s3')\n\n    for (root, dirs, files) in os.walk(local_path):\n        for filename in files:\n            print(\"File: {}\".format(filename))\n            s3_filename = s3_path + filename\n            print('Uploading to %s...' % (s3_filename))\n            s3.meta.client.upload_file(local_path + filename, bucket_name, s3_filename)\n            print('Done')\n\nupload_files(<local path e.g. /tmp/data/>, <S3_path e.g. /test/>,\\\n <bucket name e.g. datalake.bucket.data>)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Example 4: Upload files into S3 by calling AWS command in Python"}),"\n",(0,a.jsx)(n.p,{children:"All files in the specified local directory will be recursively copied to S3 by using aws cli."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\nlocal_dir = <local_dir>\ns3_dir = <s3_bucket_name+folder_dir>\nos.system('aws s3 cp {} {} --recur'.format(local_dir, s3_dir))\n"})}),"\n",(0,a.jsx)(n.p,{children:"If you are interested in connecting to S3 and downloading files, check out this post: Comprehensive Guide to Download Files From S3 with Python"}),"\n",(0,a.jsx)(n.p,{children:"(2017-11-11)"})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var a=t(96540);const s={},o=a.createContext(s);function i(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);