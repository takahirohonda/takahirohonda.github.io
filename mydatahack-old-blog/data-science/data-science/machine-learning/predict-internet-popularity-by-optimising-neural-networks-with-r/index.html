<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-data-science" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.3.2">
<title data-rh="true">Predict Internet Popularity By Optimising Neural Networks With R | MyDatahack</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://takahirohonda.github.io/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Predict Internet Popularity By Optimising Neural Networks With R | MyDatahack"><meta data-rh="true" name="description" content="Writing code to do machine learning is easy. What makes it difficult is the optimisation. By large, there are two ways to optimise your model."><meta data-rh="true" property="og:description" content="Writing code to do machine learning is easy. What makes it difficult is the optimisation. By large, there are two ways to optimise your model."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2018-01-05T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="Data Science,Machine Learning,caret,Hyper Parameter Search,Neural Networks,nnet,R"><link data-rh="true" rel="icon" href="/mydatahack-old-blog/img/icon-circle.png"><link data-rh="true" rel="canonical" href="https://takahirohonda.github.io/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r"><link data-rh="true" rel="alternate" href="https://takahirohonda.github.io/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r" hreflang="en"><link data-rh="true" rel="alternate" href="https://takahirohonda.github.io/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://takahirohonda.github.io/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r","mainEntityOfPage":"https://takahirohonda.github.io/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r","url":"https://takahirohonda.github.io/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r","headline":"Predict Internet Popularity By Optimising Neural Networks With R","name":"Predict Internet Popularity By Optimising Neural Networks With R","description":"Writing code to do machine learning is easy. What makes it difficult is the optimisation. By large, there are two ways to optimise your model.","datePublished":"2018-01-05T00:00:00.000Z","author":[],"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://takahirohonda.github.io/mydatahack-old-blog/data-science","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/mydatahack-old-blog/blog/rss.xml" title="MyDatahack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/mydatahack-old-blog/blog/atom.xml" title="MyDatahack Atom Feed">



<link rel="alternate" type="application/rss+xml" href="/mydatahack-old-blog/web-technologies/rss.xml" title="MyDatahack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/mydatahack-old-blog/web-technologies/atom.xml" title="MyDatahack Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/mydatahack-old-blog/data-engineering/rss.xml" title="MyDatahack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/mydatahack-old-blog/data-engineering/atom.xml" title="MyDatahack Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/mydatahack-old-blog/data-science/rss.xml" title="MyDatahack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/mydatahack-old-blog/data-science/atom.xml" title="MyDatahack Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/mydatahack-old-blog/infrastructure/rss.xml" title="MyDatahack RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/mydatahack-old-blog/infrastructure/atom.xml" title="MyDatahack Atom Feed"><link rel="stylesheet" href="/mydatahack-old-blog/assets/css/styles.2668ea1a.css">
<script src="/mydatahack-old-blog/assets/js/runtime~main.65edbf1e.js" defer="defer"></script>
<script src="/mydatahack-old-blog/assets/js/main.ffa8a442.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/mydatahack-old-blog/"><div class="navbar__logo"><img src="/mydatahack-old-blog/img/icon-circle.png" alt="MyDatahack Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/mydatahack-old-blog/img/icon-circle.png" alt="MyDatahack Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">MyDatahack</b></a><a class="navbar__item navbar__link" href="/mydatahack-old-blog/docs/">Blogs</a><a class="navbar__item navbar__link" href="/mydatahack-old-blog/web-technologies">Web Technologies</a><a class="navbar__item navbar__link" href="/mydatahack-old-blog/data-engineering">Data Engineering</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/mydatahack-old-blog/data-science">Data Science</a><a class="navbar__item navbar__link" href="/mydatahack-old-blog/infrastructure">Infrastructure</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/takahirohonda" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/deep-learning/building-alexnet-with-keras">Building AlexNet with Keras</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/deep-learning/building-alexnet-with-tensorflow-and-running-it-with-aws-sagemaker">Building AlexNet with TensorFlow and Running it with AWS SageMaker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-keras/">Introduction to Dense Layers for Deep Learning with Keras</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/deep-learning/introduction-to-dense-net-with-tensorflow">Introduction to Dense Layers for Deep Learning with TensorFlow</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/infra/how-to-create-your-personal-data-science-computing-environment-in-aws">How To Create Your Own Personal Data Science Computing Environment In AWS</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python">Predict Internet Popularity By Optimising Neural Networks With Python</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-r">Predict Internet Popularity By Optimising Neural Networks With R</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/machine-learning/how-to-save-machine-learning-models-in-r">How To Save Machine Learning Models In R</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/infra/how-to-deploy-spark-applications-in-aws-with-emr-and-data-pipeline">How To Deploy Spark Applications In AWS With EMR and Data Pipeline</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/visualisation/how-to-do-sentiment-analysis-on-your-favourite-book-with-r">How To Do Sentiment Analysis On Your Favourite Book With R</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/visualisation/how-to-create-a-word-cloud-for-your-favourite-book-with-r">How To Create a Word Cloud For Your Favourite Book With R</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/mydatahack-old-blog/data-science/data-science/visualisation/how-to-customise-shinyapp-with-bootstrap-css-javascript-and-plotly">How To Customise ShinyApp With Bootstrap, Javascript And Plotly</a></li></ul></nav></aside><main class="col col--7"><article><header><h1 class="title_f1Hy">Predict Internet Popularity By Optimising Neural Networks With R</h1><div class="container_mt6G margin-vert--md"><time datetime="2018-01-05T00:00:00.000Z">January 5, 2018</time> · <!-- -->10 min read</div></header><div id="__blog-post-container" class="markdown"><p>Writing code to do machine learning is easy. What makes it difficult is the optimisation. By large, there are two ways to optimise your model.</p>
<ul>
<li>Feature selection &amp; transformation</li>
<li>Model parameter optimisation</li>
</ul>
<p>Both are hard-core topics and neither can be covered in this post. Feature selection and transformation often require subject matter expertise on top of statistical computation.</p>
<p>In this blog, I want to focus on parameter optimisation by using the power of the caret package. The caret package makes it easy to do hyper parameter search. The model parameter optimisation is often called hyperparameter optimisation. We search and optimise over all possible parameter values for the model and pick the ones perform the best (the best hyperparameter set).</p>
<p>Here are some of the commonly used for hyperparameter search strategies.</p>
<p>Grid Search (Not as efficient, but easy)
Random Search (Efficient in certain scenarios)
Bayesian Optimisation (Efficient in general)
The default search method for caret is grid search which will be used here. Another question is which parameters to search. This depends on the method or package you use.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="neural-networks-hyperparameter-search">Neural Networks Hyperparameter Search<a href="#neural-networks-hyperparameter-search" class="hash-link" aria-label="Direct link to Neural Networks Hyperparameter Search" title="Direct link to Neural Networks Hyperparameter Search">​</a></h3>
<p>Neural networks has many different flavours. I chose the nnet package because it uses the often-called vanilla neural net (a.k.a the single layer back-propagation network or single layer perceptron) and is simpler to tune hyperparameters compared to other more complicated neural networks. In this method, you don’t need to determine the number of layers as it has only one. What you need to do is to choose the size of the neurons in the hidden layer.</p>
<p>The bigger neuron size you use, the more complicated the model become. Therefore, more neurons tend to become over-fitting. Of course, this is dependent on the dataset. It is only determined by experimentation. However, smaller size is a good rule of thumb.</p>
<p>Neural networks sound complicated and magical. But, if you deconstruct them, they are basically non-linear statistical model. Hence, we should try to optimise the regularisation parameter. <code>nnet</code> has a parameter called decay for regularisation. In short, regularisation prevent over-fitting by penalising over-fitted training models.</p>
<p>We will set hyperparameter ranges for size and decay for <code>nnet</code> optimisation.</p>
<p><strong>Sampling</strong></p>
<p>Another thing you have to consider is sampling. Again, there are a few different ways like Random Sub Sampling, Stratified Sampling or Cross Validation. In this example, I will use the classic k-fold cross validation method (where k = 10). A sampling method and parameters can be specified in the trControl parameter in the train function in caret.</p>
<p>K-fold Cross Validation split the training data into k pieces. Then, it uses k minus 1 folds to estimate a model and test it on the remaining 1 fold (called validation set) for each candidate parameter values. With grid search, it will calculate the average accuracy per parameter set to produce the best set.</p>
<p>The drawback of grid search is that you are assuming a possible range for the hyperparameter set. Therefore, you cannot reach it when it exists outside of your pre-defined set. But, it’s a good place to start.</p>
<p><strong>Dataset</strong></p>
<p>I am using the data set Online News Popularity Data Set from the UCI Machine Learning repository. The dataset was used in the publication here. Each row is an online news article containing a bunch of attributes as well as the numbers of social media shares. There are 58 features and what you predict is a binary classifier of the article, ‘popular’ or ‘unpopular’. The classification category was made by setting the threshold of 1400 shares. This number was chosen in the paper because it produced a balanced popular/unpopular class distribution (Fernandes et al. 2015). The goal is to predict if the article is popular or not by using the available features.</p>
<p>In the paper, they used Python’s <code>scikit-learn</code> to do Random Forest, Adaptive Boosting, Support Vector Machine, K_Nearest Neighbour and Naïve Bayes. We can use their results as a benchmark to test our neural networks model performance.</p>
<p><strong>Code</strong></p>
<p>(1) Install and import required libraries.</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">library(caret)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">library(nnet)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">library(e1071)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">library(NeuralNetTools)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">library(doParallel)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">library(ROCR)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>(2) Download csv file here. Load the data and create a target field which contains a binary classifier(popular and unpopular). I used 1400 as the threshold as in Fernandes et al. 2015. I removed non-predictors according to the article.</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">shares &lt;- read.csv(&quot;/tmp/OnlineNewsPopularity.csv&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">shares$Popularity &lt;- as.factor(ifelse(shares$shares &gt;= 1400, &quot;popular&quot;, &quot;unpopular&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">shares2 &lt;- shares[, -c(1, 2, 61)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>(3) Split the data set into training and testing with seed for repeatability.</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">set.seed(1000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inTrain &lt;- createDataPartition(shares2$Popularity, p=0.7, list=F)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train &lt;- shares2[inTrain, ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">test &lt;- shares2[-inTrain, ]]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>(4) Now, it’s time for training and predicting! You can set the sampling methods with the trainControl function and pass it as the trControl parameter in the train function. I defined the tuneGrid as 3 size and 3 decay options. The caret package will conduct the accuracy search to find the best combo. The final values used for the model were size = 1 and decay = 0.1 (you can see this by typing model_nnet in the console).</p>
<p>The caret package supports parallel processing. R by default uses only a single UPC. Parallel processing will enable you to do computation with all the CPUs in your computer. It is easy to set it up. For Windows, you need to use the doParallel package. Once you register the cores with the registerDoParallel function, caret will execute it automatically.</p>
<p>You can visualise the model by the plotnet function from NeuralNetTools.</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">set.seed(1000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Set up Parallel Computing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cl &lt;- makeCluster(4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">registerDoParallel(cl)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Train with caret</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">numFolds &lt;- trainControl(method = &#x27;cv&#x27;, number = 10, classProbs = TRUE, verboseIter = TRUE,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_nnet&lt;- train(Popularity~., data=train, method=&#x27;nnet&#x27;, trace=F,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   preProcess = c(&#x27;center&#x27;, &#x27;scale&#x27;), trControl = numFolds,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   tuneGrid=expand.grid(.size=c(1,5,10), .decay=c(0,0.001,0.1)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Plot model and predict</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">plotnet(model_nnet)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_pred &lt;- predict(model_nnet, newdata=train, type=&quot;raw&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">test_pred &lt;- predict(model_nnet, newdata=test, type=&quot;raw&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">Here are some notes for errors and warnings when you are training the model.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">The linout parameter cannot be True. You will get the error message, ‘entropy fit only for logistic units’. The default value for linout is FALSE.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>In trainControl(), summaryFunction=twoClassSummary will result in a warning: The metric “Accuracy” was not in the result set. ROC will be used instead.” This parameter can be omitted. It will use Accuracy by default for categorical prediction.</p>
<p>(5) In R, you can use confusionMatrix() to evaluate the model. It will give you the performance metrics such as accuracy, balanced accuracy, sensitivity, specificity and so on. I created a custom function for evaluation because accuracy, precision, recall, F1 and AUC were used in the paper and I wanted to get the same metrics for benchmarking.</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">evaluate &lt;- function(pred, actual){</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tab &lt;- table(pred, actual$Popularity)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tp &lt;- tab[1,1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  tn &lt;- tab[2,2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  fp &lt;- tab[2,1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  fn &lt;- tab[1,2]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  accuracy &lt;- (tp + tn)/(tp + tn + fp + fn)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  precision &lt;- tp/(tp+fp)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  recall &lt;- tp/(tp + fn)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  F1 &lt;- 2*(precision*recall)/(precision+recall)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  actual_label &lt;- ifelse(actual$Popularity==&#x27;popular&#x27;, 1, 0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  pred_label &lt;- ifelse(pred==&#x27;popular&#x27;, 1, 0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  pred_OCR= ROCR::prediction(pred_label, actual_label)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  perf = performance(pred_OCR, &#x27;tpr&#x27;, &#x27;fpr&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  plot(perf)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  AUC &lt;- as.numeric(performance(pred_OCR, &quot;auc&quot;)@y.values)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  print(&quot;Prediction Evaluation Outcome&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  eval &lt;- as.data.frame(cbind(round(accuracy, 2), round(precision, 2), round(recall, 2), round(F1, 2), round(AUC, 2)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  colnames(eval) = c(&#x27;accuracy&#x27;,&#x27;precision&#x27;,&#x27;recall&#x27;,&#x27;F1&#x27;,&#x27;AUC&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  print(eval)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(train_pred, train)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(test_pred, test)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Confusion Matrix from caret package for evaluation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># Only for accuracy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">confusionMatrix(train_pred, train$Popularity)$overall[1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">confusionMatrix(test_pred, test$Popularity)$overall[1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># confusion matrix and all the metrics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">confusionMatrix(train_pred, train$Popularity)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">confusionMatrix(test_pred, test$Popularity)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ul>
<li>Output</li>
</ul>
<p><img decoding="async" loading="lazy" alt="nnet model" src="/mydatahack-old-blog/assets/images/nnet-model-82b4005ac1614715c54615b93bdbd010.webp" width="1114" height="747" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="evaluation" src="data:image/webp;base64,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" width="352" height="151" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="comparing-it-with-default-nn-model">Comparing It With Default NN Model<a href="#comparing-it-with-default-nn-model" class="hash-link" aria-label="Direct link to Comparing It With Default NN Model" title="Direct link to Comparing It With Default NN Model">​</a></h3>
<p>What if we used the model without any optimisation? Check out the outcome. It is pretty much as good as random picks. Now, you see the power of the simple caret optimisation!</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">model_default &lt;- nnet(Popularity~., data=train, size=1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">plotnet(model_default)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_pred_default &lt;- predict(model_default, newdata=train, type=&quot;class&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">test_pred_default &lt;- predict(model_default, newdata=test, type=&quot;class&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(train_pred_default, train)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(test_pred_default, test)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" alt="evaluation 2" src="data:image/webp;base64,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" width="352" height="148" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="should-we-normalise-the-dataset">Should We Normalise the Dataset?<a href="#should-we-normalise-the-dataset" class="hash-link" aria-label="Direct link to Should We Normalise the Dataset?" title="Direct link to Should We Normalise the Dataset?">​</a></h3>
<p>It really depends on the data. The nnet package generally performs fine without normalisation unless you have features in completely different scale. Normalisation is usually recommended for Neural Networks for better accuracy and faster training speed.</p>
<p>Well, let’s test it. nnet takes factor and numeric values as features. In the first example, all field types are numeric. Here, I converted categorical variables into factors and normalised other numeric fields.</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"># Normalising data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">index &lt;- c(12:17, 30:37, 59)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">colnames(shares2[,index]) # checking the selected columns by index</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tmp_df1 &lt;- as.data.frame(scale(shares2[, -index]))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">tmp_df2 &lt;- lapply(shares2[, index], as.factor)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">shares_norm &lt;- cbind(tmp_df2, tmp_df1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># create training and testing set. Use the partition created above</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">set.seed(1000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">inTrain &lt;- createDataPartition(shares2$Popularity, p=0.7, list=F)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_norm &lt;- shares_norm[inTrain, ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">test_norm &lt;- shares_norm[-inTrain, ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"># train &amp; predict</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">set.seed(1000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cl &lt;- makeCluster(4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">registerDoParallel(cl)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">numFolds &lt;- trainControl(method = &#x27;cv&#x27;, number = 10, classProbs = TRUE, verboseIter = TRUE,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_nnet_norm &lt;- train(Popularity~., data=train_norm, method=&#x27;nnet&#x27;, trace=F,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         preProcess = c(&#x27;center&#x27;, &#x27;scale&#x27;), trControl = numFolds,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         tuneGrid=expand.grid(.size=c(1,5,10), .decay=c(0,0.001,0.1)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_nnet_norm # The final values used for the model were size = 1 and decay = 0.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_pred_norm &lt;- predict(model_nnet_norm, newdata=train_norm, type=&quot;raw&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">test_pred_norm &lt;- predict(model_nnet_norm, newdata=test_norm, type=&quot;raw&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(train_pred_norm, train_norm)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(test_pred_norm, test_norm)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Testing accuracy is slightly worse. But, overall it is similar. I think it is still good to test on the normalised dataset when you are modelling with neural networks. Normalised data usually give us a different model. In this case, the final values used for the model were size = 1 and decay = 0.</p>
<p><img decoding="async" loading="lazy" alt="evaluation 3" src="data:image/webp;base64,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" width="365" height="149" class="img_ev3q"></p>
<p>Let’s Benchmark!</p>
<p>Here is the performance metric table from the paper (Fernandes et al. 2015). Our model has the same accuracy as AdaBoost and SVM. It preformed slightly worse then RF. We beat them in Precision (0.7). However, I don’t think our model is superior because we have quite low AUC compared to all the models in the paper. Overall, I am happy with what I got. You can really see what simple optimisation can make difference!</p>
<p><img decoding="async" loading="lazy" alt="benchmark" src="/mydatahack-old-blog/assets/images/performance-benchmark-ea38c24d2c5f25bf7d3e11d4a48a41bd.webp" width="1030" height="243" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="further-grid-search">Further Grid Search<a href="#further-grid-search" class="hash-link" aria-label="Direct link to Further Grid Search" title="Direct link to Further Grid Search">​</a></h3>
<p>My hyperparameter range for the size was (1, 5, 10) and 1 became the best hyperparameter. OK, 5 neuron was probably too over-fitting. But, Isn’t 1 neuron too simple? Surely, having a few extra neurons would lead to better performance? I decided to investigate it further by setting size range to (1, 2, 3, 4).</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">set.seed(1000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">cl &lt;- makeCluster(4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">registerDoParallel(cl)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">numFolds &lt;- trainControl(method = &#x27;cv&#x27;, number = 10, classProbs = TRUE, verboseIter = TRUE,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                         preProcOptions = list(thresh = 0.75, ICAcomp = 3, k = 5))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_nnet&lt;- train(Popularity~., data=train, method=&#x27;nnet&#x27;, trace=F,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   preProcess = c(&#x27;center&#x27;, &#x27;scale&#x27;), trControl = numFolds,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                   tuneGrid=expand.grid(.size=c(1,2,3,4), .decay=c(0.001,0.01,0.1)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">model_nnet</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">plotnet(model_nnet)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">plotnet(model2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">train_pred &lt;- predict(model_nnet, newdata=train, type=&quot;raw&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">test_pred &lt;- predict(model_nnet, newdata=test, type=&quot;raw&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(train_pred, train)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">evaluate(test_pred, test)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The best hyperparameters turned out to be size = 4 and decay = 0.1. With these parameters, I didn’t see any improvement on test dataset prediction. I even observed the model was overfitting. Generally speaking, complex neural networks tend to overfit. This exercise somehow reminded me of Occam’s razor. By paraphrasing it, the simplest is the best given everything is the same. I think Occam’s razor is often sited in machine learning as the reminder that simpler is often better.</p>
<p>Performance for size = 4 and decay = 0.1 is below:</p>
<p><img decoding="async" loading="lazy" alt="evaluation 4" src="data:image/webp;base64,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" width="384" height="149" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="your-turn">Your Turn<a href="#your-turn" class="hash-link" aria-label="Direct link to Your Turn" title="Direct link to Your Turn">​</a></h3>
<p>Can we further optimise Neural Network model here by using different search or sampling methods?
R has other neural network packages like neuralnet or RSNNS. Do they perform better than nnet?
Check all the models supported by the caret package. We can basically do all the methods used in the paper. See if we can use these methods and benchmark against the results?</p>
<div class="language-R language-r codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-r codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">library(caret)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">names(getModelInfo())</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>Fun!</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="reference">Reference<a href="#reference" class="hash-link" aria-label="Direct link to Reference" title="Direct link to Reference">​</a></h3>
<p>Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 – Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal (<a href="https://repositorium.sdum.uminho.pt/bitstream/1822/39169/1/main.pdf" target="_blank" rel="noopener noreferrer">https://repositorium.sdum.uminho.pt/bitstream/1822/39169/1/main.pdf</a>)</p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/mydatahack-old-blog/data-science/tags/data-science">Data Science</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/mydatahack-old-blog/data-science/tags/machine-learning">Machine Learning</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/mydatahack-old-blog/data-science/tags/caret">caret</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/mydatahack-old-blog/data-science/tags/hyper-parameter-search">Hyper Parameter Search</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/mydatahack-old-blog/data-science/tags/neural-networks">Neural Networks</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/mydatahack-old-blog/data-science/tags/nnet">nnet</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/mydatahack-old-blog/data-science/tags/r">R</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/mydatahack-old-blog/data-science/data-science/machine-learning/predict-internet-popularity-by-optimising-neural-networks-with-python"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">Predict Internet Popularity By Optimising Neural Networks With Python</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/mydatahack-old-blog/data-science/data-science/machine-learning/how-to-save-machine-learning-models-in-r"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">How To Save Machine Learning Models In R</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#neural-networks-hyperparameter-search" class="table-of-contents__link toc-highlight">Neural Networks Hyperparameter Search</a></li><li><a href="#comparing-it-with-default-nn-model" class="table-of-contents__link toc-highlight">Comparing It With Default NN Model</a></li><li><a href="#should-we-normalise-the-dataset" class="table-of-contents__link toc-highlight">Should We Normalise the Dataset?</a></li><li><a href="#further-grid-search" class="table-of-contents__link toc-highlight">Further Grid Search</a></li><li><a href="#your-turn" class="table-of-contents__link toc-highlight">Your Turn</a></li><li><a href="#reference" class="table-of-contents__link toc-highlight">Reference</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/mydatahack-old-blog/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/takahirohonda" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 MDH.</div></div></div></footer></div>
</body>
</html>